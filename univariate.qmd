# Univariate Analysis

Before we start examining the relationships between multiple variables, we have to look under the hood of our variables individually and make sure we have a good sense of what they are like. 

We want to know the frequency distribution of the response values, the measures of central tendency, and the dispersion. In other words, we want to see how many respondents chose each response value, which response values are most typical for each variable, and the extent to which the response values vary. 

Luckily for us, R has plenty of tools for generating these quantitative summaries, as well as visualizing them in the form of barplots, histograms, and other common styles for displaying data.

This tutorial will cover each of these elements, and I will highlight any differences specific to certain levels of measurement along the way.

## Categorical Variables
Let's start with categorical variables. We will start with frequency distributions by way of frequency tables and bar plots. Then, we will talk about measures of tendency and observe when they are appropriate for categorical data. 

### Frequency Distributions

#### Frequency tables

Frequency tables are the most effective way to report the frequency distributions for categorical variables. There are actually a ton of different ways to produce this kind of table in R, but we are going to use some convenient functions from the `janitor` package, so let's go ahead and load that in. 

If you are on a lab computer, try to load it with `library()` first. If you get an error, go ahead and install it with `install.packages()` and then load it in.

While we're at it, we'll also bring in `tidyverse` and load in our GSS subset.

```{r, warning = FALSE, message = FALSE}
library(janitor)
library(tidyverse)
load("our_gss.rda")
```

Let's go ahead and work with one of our `partyid` recodes from last time. We'll use `partyid_recoded`, where we retained all the party choices but collapsed the categories of degree (e.g. strong democrat + not very strong democrat = Democrat). 

```{r, eval = TRUE, echo = FALSE}
our_gss <- our_gss |>
  mutate(
    partyid_recoded=fct_collapse(partyid, 
"Democrat" = c("strong democrat", "not very strong democrat"),
"Republican" = c("strong republican","not very strong republican"),
"Independent" = c("independent, close to democrat", "independent (neither, no response)", "independent, close to republican"),
"Other Party" = c("other party")
))
```

We can use a simple command with `tabyl()` from the `janitor` package to produce our frequency tables. Let's take a look at some output, and I'll go through a couple details and mention some further specifications we can make.

All we need to do is give our data frame as the first argument, and then our particular variable column as the second argument.

```{r, echo = TRUE, eval = FALSE}
# Without pipe operator
tabyl(our_gss, partyid_recoded)
```
```{r}
# With pipe operator
our_gss |>
  tabyl(partyid_recoded)
```

Now, we have a frequency table for `partyid_recoded`!

Each row is a level of `partyid_recoded`. The `n` column refers to the sample size, so, for example, 1,565 respondents indicated that they were Independents. 

We then have two different columns for percentages. The `percent` column is the proportion of all respondents that chose each response value---while including NA values. Often, we want to get a proportion for only the sample of those who actually answered the question. This is what the `valid_percent` column indicates and is what we are often looking for. 

Let's break it down a little for clarity. 

The total number of respondents is the sum of the 'n' column. 

1565 + 1046 + 792 + 106 + 35 = 3,544 
Note that this is also the number of observations in our data frame

There are also 35 observations coded as NA, so the total number of *valid* respondents is 3,509.

Now, I'll give the calculations for the `Independent` row.

```{r}
# I'll define a few objects here for simplicity.

all_respondents <- 3544

all_minus_na <- (3544 - 35)

independents <- 1565
```

```{r}
# Percent column

independents / all_respondents

# Valid percent column

independents / all_minus_na

```

In this case, there really aren't that many NA values, so it does not change too much. In general, we will often discard NA categories in analyses we do for the course, but they give us important information about the survey and can be useful for some questions, so it's important to keep track of them nonetheless. 

Now, let's clean a couple things up with the `adorn_()` functions.

Let's add a row for totals, turn the proportions into percentages, and round the decimals. We can use the pipe operator to quickly leverage a bunch of different `adorn()` functions.

```{r}
our_gss |>
  tabyl(partyid_recoded) |>
  adorn_totals(where = "row") |>
  adorn_pct_formatting() |>
  adorn_rounding(digits = 2)
```
We don't need to supply any input for `adorn_pct_formatting()`---which changes the proportions to percentages---but a couple of these functions do require inputs.

For `adorn_totals()`, we give "row" for the `where` input, which tells the function that we want the totals column to appear as a row.

For `adorn_rounding()`, we give an input for `digits`, which is the number of digits we want after the decimal point. We'll keep 2 and thereby round to the hundredth. 

Lastly, if we want, we can also make the column names a little nicer. It may not be obvious because we've just been displaying this table rather than storing it as an object, but it's actually a data frame, so we can edit the column names directly.

Let's save it as an object so that's easier to see.

```{r}
our_table <- our_gss |>
  tabyl(partyid_recoded) |>
  adorn_totals(where = "row", na.rm = TRUE) |>
  adorn_pct_formatting() |>
  adorn_rounding(digits = 2)
```

We can then use the `colnames()` function. Let's supply our table data frame as an input to get a feel for it.

```{r}
colnames(our_table)
```
It lists out the character vector of the 4 column names, and we can actually just write over them by supplying our desired column names as a character vector. If we assign that character vector to `colnames(our_table)`, it will overwrite the column names. Make sure the labels are in the same order as they appear in the data frame.


```{r}
colnames(our_table) <- c(
  "Political Party",
  "Frequency",
  "Percent",
  "Valid Percent"
  )

our_table
```
Excellent! We'll learn how to make this even nicer a bit later in the semester, but this looks pretty good for now.

Let's look at one last visualization technique for categorical variables.

#### Bar plots

For categorical data, bar plots are often the way to go. These typically have the response values on the x axis, and the count or percentage of each response value is tracked on the Y axis. 

Much like with tables, we'll learn some fancier ways to do this later in the semester. But, in the meanwhile, base R has some great plotting functions that can be used very easily without much specification. They're not the prettiest out of the box, but they will do everything we need.

For `barplot()`, we just need the variable column that we are trying to plot. But, there's one thing to remember when you are trying to get a barplot this way---you have to provide the result of `summary(my_variable)` rather than the variable column directly. 

I'll show you a couple ways to do that, which all do the exact same thing.

```{r, eval = FALSE}
# No pipe operator; summary() nested inside barplot()

barplot(summary(our_gss$partyid_recoded))
```

```{r, eval = FALSE}
# With pipe operator
our_gss$partyid_recoded |>
  summary() |>
  barplot()
```

```{r}
# Make separate object for the summary() output

my_summary <- summary(our_gss$partyid_recoded)

barplot(my_summary)
```

Now, as I mentioned, we won't spend too much time adjusting these plots for now, but there are a couple things we can do to make this a little better to look at for now.

For one, our response labels (Democrat, Republican, Independent, etc) Actually look pretty good in the display here, but it's not uncommon for these to get cut off if we have several response categories or the response names are especially long. So, I'll show you how to reduce the font size as one potential fix for that. 

We can add the `cex.names` argument to `barplot()`. This argument takes a number, and the number should reflect an intended ratio of the default font size for our value labels on the x-axis. So, for example, I would enter '2' if I wanted the font to be twice as big. For our purposes, I want to reduce the font a bit, so we'll enter '.75' to reduce the font size by 1/4. 

```{r}
our_gss$partyid_recoded |>
  summary() |>
  barplot(cex.names = .75)
```

Lastly, we can also add some simple descriptive information for the plot, such as a title and labels for the x and y axis. We can do so within `barplot()`

```{r}
our_gss$partyid_recoded |>
  summary() |>
  barplot(
    cex.names = .75, 
    main = "Count of GSS Respondents by Political Party",
    xlab = "Political Party",
    ylab = "Count"
  )
```
### Measures of Central Tendency
Now, let's talk about measures of central tendency. Because there is no mathematically meaningful relationship between the different levels of a categorical variable, our options for reporting measures of central tendency are a little more limited.

The classic central tendency measures are the mode, the median, and the mean. For categorical variables, it is not ever meaningful to take the mean. As we saw back on our first day with R, you will get a warning message if you try to run `mean()` on a character or factor variable. 

Our options are limited to the mode or the median, depending on the distribution of responses.

#### Nominal variables
In the case nominal variables, there is no semblance of meaningful arrangement to the categories. In this case, we can only report the mode. 

Often times, we can just glean this from the frequency tables, or even just the `summary()` output. Let's take `sex` as an example.

```{r}
summary(our_gss$sex)
```

Remember that the mode is just the value with the most occurrences. So, the mode here is 'female'. 

This is likely sufficient for the variables we will work with, but it's possible for some variables to have so many factor levels that it can be a little annoying to try and spot the mode visually. So, I'll show you how to calculate it manually as well.

We can use the `summary()` output again. For a factor variable, it gives us each of the levels of the factor along with the number of respondents in each level. So, we can use a nifty base R function called `which.max()` along with `summary()`. This will return the maximum value in our summary output, which will correspond to the most frequent level of the factor.

```{r}
sex_sum <- summary(our_gss$sex)

which.max(sex_sum)
```

so, this also gives us our mode, and is a bit more precise a way of doing so---especially when we have a bunch of possible response values.

Now, let's talk about ordinal variables, where we have another consideration.

#### Ordinal variables

While ordinal variables are still categorical and thus unamenable to mathematical analysis, they do have a meaningful order. This means that it's possible for us to take the median value of an ordinal variable.

However, we can only do so when the variable is normally distributed. If there is significant skew in the distribution of responses, then we will need to report the mode for an ordinal variable.

Let's refresh our memory on distributions before we take some central tendency measures of ordinal variables in the GSS. 

##### Distribution assumptions
If the responses of an ordinal variable are normally distributed, we can take the mean. 

In the ideal form of the normal distribution, you have a mean of 0 and a standard deviation of 1, where roughly 68% of values are within 1 standard deviation in either direction of the mean, and roughly 95% of values are within 2 standard deviations in either direction.

However, in practice, our distributions are unlikely to reflect the ideal normal distribution. But, if they are roughly normal, we can work with that.

I'll display a classic normal distribution here. If you produce a barplot and find the data looks generally like this---where the majority of variables are concentrated around the middle of the distribution---then you can report the median response rather than the mode.

I'll display a few distributions here as a reminder.

This is the classic normal distribution
```{r, eval = TRUE, echo = FALSE}
hist(
  rnorm(10000),
  main = "Normal distribution; n = 10,000",
  xlab = "Variable Response Values"
)
```

Here's a left-skewed distribution. This is when there's a promintent 'tail' on most of the left-hand side of the distribution. In other words, when most of the values are concentrated on the right-hand side of the distribution (the upper end), then we have a left-skew.

```{r, eval = TRUE, echo = FALSE}
hist(
  rbeta(10000, 10, 2),
  main = "Left-skewed distribution, n = 10,000",
  xlab = "Variable Response Values"
)
```

Lastly, here's a right-skewed distribution. As you can probably guess from the description of a left-skewed distribution, a right-skew occurs when we have a long tail through most of the right-hand side of the distribution. In other words, most values are concentrated on the left-hand side. 

```{r, eval = TRUE, echo = FALSE}
hist(
  rbeta(10000, 2, 10),
  main = "Right-skewed distribution, n = 10,000",
  xlab = "Variable Response Values"
)
```

Lastly, I'll show a normal distribution with outliers. This is also a situation where we would need to take the mode of an ordinal variable. Note that the vast majority of this distribution is between -4 and 4, but we have some observations at -20, way outside the range of the distribution. For a categorical variable---even an ordinal one---the median will not be ideal in this context.

We do want to keep any outlier values if we have good reason to believe that they were measured correctly, but it's always worth double-checking outliers, as they sometimes indicate that something has gone wrong with the measurement process.

```{r, eval = TRUE, echo = FALSE}
hist(
  c(
    rnorm(300),
    1,
  -20
  ),
  main = "Normal distribution with outliers; n = 300",
  xlab = "Variable Response Values"
) 
```
So, if the distribution of your variable looks (mostly) like the normal distribution, go ahead and take the median of any ordinal variable.

Let's go ahead and practice doing that with the `age_ord` variable we created last time.

```{r, eval = TRUE, echo = FALSE}
our_gss <- our_gss |>
  mutate(
    age_ord = cut(
      age,
      breaks = c(18, 36, 51, Inf),
      include.lowest = TRUE,
      right = FALSE,
      labels = c("Younger", "Middle Age", "Older"),
      ordered_result = TRUE
    )
  )
```

In the case of `age_ord`, we'd actually want to take the mode, because it's a bit left-skewed. We can see that when we make the barplot

```{r}
our_gss$age_ord |>
  summary() |>
  barplot(
    main = "Age Distribution of GSS Respondents",
    xlab = "Age grouping"
  )
```

However, there's not a great example of a normally distributed ordinal variable in our subset, so I'll show you how to calculate the median using this variable anyhow.

For this, we can use the `quantile()` function. It has a lot of nifty uses, but we will only really use it for this purpose---calculating the median of a normally distributed ordinal variable.

In technical terms, quantiles are markers that split up a distribution into equal parts. 

For our purposes, you can think of quantiles in terms of percentiles, where, for example, being in the 90th percentile means you are performing at or above 90% of others in the sample.

We can use the `quantile()` function to get the value at the 50th percentile of the distribution, which is another way of saying the median. 

```{r}
quantile(
our_gss$age_ord, 
.5, 
type = 1,
na.rm = TRUE
)
```

For the first input, we provide the ordinal variable column for which we want to calculate the median.

Then, we give a proportion equivalent to the percentile that we want. Because want to take the value at the 50th percentile, we will enter 0.5. 

The specifics of the 'type' argument are a little above our paygrade, but we need to specify this so that `quantile()` knows its working with an ordered factor.

Lastly, we'll specify that we want NAs to be removed in the calculation, as R will throw an error unless we tell it to disregard these.

So, we can see that 'Middle Age' would be the median response---which we would report if `age_ord` weren't skewed.