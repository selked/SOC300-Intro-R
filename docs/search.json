[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to R for Quantitative Social Science Research",
    "section": "",
    "text": "Preface\nWelcome to this introductory R tutorial for SOC 300!\nHere you will find all the step-by-step instructions for completing our initial foray into R for quantitative analysis of social science data. We will begin by establishing some common ground in basic R operations and functionality. After we lay this foundation, we will progress through various data processing tasks—from importing and cleaning public data to visualizing and analyzing these data for consumption by interested stakeholders.\nYou will receive an R script file with the commands detailed here, so that you can easily run and manipulate them on your own device, but you will always be able to refer to this mini-textbook in the event that you would like to see everything in one place and refer to some more detailed documentation on various R operations.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Background",
    "section": "",
    "text": "What is R?\nBefore we start exploring some of R’s basic functionality, I’m going to set the stage a little on what R and R studio are, why we are using these tools in particular, and what we will need to know before we dig in.\nAt it’s core, R is a programming language. There’s a lot to say about this from a computer science perspective, but, for our purposes, you can just think of R as a language with a very particular structure that’s designed to tell our computers what to do.\nThere are all sorts of different programming languages out there, and they all offer certain benefits or cater to particular computing needs. Unlike some general purpose languages like C, C++, or Python, R is relatively specialized, and this is part of what makes it so useful for us. R is designed with statistical computing as a primary motivator, and now—roughly 30 years into its tenure—stands as one of the most widely adopted resources for statistical data science in the social sciences and beyond.\nThough there can be a bit of a learning curve when getting used to R, we will focus on exactly the things that we need and build ourselves up slowly. Once you get used to it, R will allow you to perform incredibly complex statistical procedures with relative ease, and it can even help us with other related tasks like visualizing and presenting our analyses.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "background.html#what-is-r-studio",
    "href": "background.html#what-is-r-studio",
    "title": "Background",
    "section": "What is R Studio?",
    "text": "What is R Studio?\nWe are going to pair R with the software R Studio, which is what’s known as an Integrated Development Environment (IDE). Programming languages can be leveraged in a number of different ways. You could run R commands entirely from a Windows command line or Mac terminal. But that would probably not be very ideal for us—not to mention sort of ethereal and frustrating for those of without any programming experience. IDEs provide user-friendly interfaces for working with programming languages, so that we can easily manage our code, quickly generate and view the output of our analyses, and generally keep track of what we are doing with R. There are lots of other IDEs out there, but R Studio is an ideal balance of ease and power, so it will serve as our IDE of choice.\nThe best way to think about R Studio’s relationship to R is by framing it as an analogy with a desktop computer. R Studio is to R as a monitor is to a computer. R is the thing that’s doing all the heavy lifting computationally, and R Studio is the thing that allows us to view and interact with R in a way that’s simple and straightforward.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "background.html#why-r",
    "href": "background.html#why-r",
    "title": "Background",
    "section": "Why R?",
    "text": "Why R?\nUltimately, R is just one of several different options we could have gone with for a course like this. STATA, SPSS, Python, and even MS Excel are used with regularity for quantitative analysis in academia and industry. However, R has a few advantages that make it well-suited for us.\nWhile software like Excel, SPSS, and STATA arguably have more accessible, user-friendly interfaces, the upper limit of their capabilities is far lower than R. On the other end of the spectrum, Python is a little overkill for our purposes. While it has some excellent resources for data science, it’s also used for a wider variety of programming tasks related to web- and software development. I once heard a computational sociologist joke that using Python for something that can be done in R is like using a nuke when all you need is a hammer. While R is not quite as expansive as Python, it’s specialization in statistical computing provides a helpful balance for us when it comes to our goals for the course.\nAnother big perk of R is that it is completely free. This is true for Python as well, but SPSS, STATA, and Excel all require the purchase of a license, and some of them can get very pricey. You have access to all of these programs as an NC State student, but you will be able to use R regardless of whether you are on an NC State computer or even enrolled in the university at all. Relatedly, R is completely open-source—you can view it’s source code, and even modify it for your own purposes. This makes R incredibly customizable, and this open-source culture has brought about a dedicated community of researchers and data scientists who regularly contribute new functional add-ons to R.\nLastly, R is quite marketable as a technical skill. Especially for those who want to go on to do research of any kind, experience with R will likely be seen as a plus. I’ve been on the lookout for various research, teaching, and industry jobs as I get ready to enter the job market, and I see calls for R as a required or preferenced skill all the time.\nIn sum, R provides us with the ideal balance of computing power and feasibility while helping keep our pockets full and giving us some skills that translate well beyond the course.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "background.html#acquiring-r-and-r-studio",
    "href": "background.html#acquiring-r-and-r-studio",
    "title": "Background",
    "section": "Acquiring R and R Studio",
    "text": "Acquiring R and R Studio\nAll of the CHASS computers (and likely most NC State computers) come with R and R Studio, so you do not need to download them, but you may find it convenient to work on R assignments using your own personal device, so I’ve provided some instructions below.\nNote that you will want to install R first,\n\nDownloading R\nYou can download R from the Comprehensive R Archive Network (CRAN).\nWhen you click that link, you will arrive at CRAN’s homepage. Navigate to the sidebar on the left, find ‘Download’ near the top, and then click ‘CRAN’. This will take you to the ‘mirrors’ page. Mirrors are just different host locations for downloading the R installation files. This allows you to maximize download speed by choosing a nearby server, so scroll down to ‘USA’ and choose one of those (I usually opt for the Durham, NC mirror).\nUnless you are very experienced with computers, you should download one of the options listed as ‘pre-compiled binary distributions’. These will typically be the first options listed. Don’t even worry about what that means if you’re not familiar. Just choose the one that reflects your operating system (there are options for Windows, Mac, and Linux) That should download an R installer, and you can follow the directions to complete a default installation.\n\n\nDownloading R Studio\nR Studio is a little more straightforward to download. Just navigate to its homepage, scroll down a little, and you will find a big button that says ‘Download R Studio for [your operating system]’. Go ahead and run the installer with the default settings.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "1  R Fundamentals",
    "section": "",
    "text": "1.1 Basic Operations\nIn this first section, we are going to start from the ground up and start to familiarize ourselves with the way R works and what it expects from us. We will begin with the most basic building blocks of R data and work our way up to the data frame—the object that will be most relevant for us. While you won’t generally need to build data frames from scratch within R for your own research, it’s a good way to familiarize yourself with the structure of data in R. While we will start here with a rather simple data frame, all the principles you learn here will scale up as we start to work with much larger and more complex data frames.\nFirst, we will start by exploring some of the basic characteristics of R.\nR can be used as a simple calculator and will process both numbers and conventional mathematical operator symbols. You can run the commands below by placing your cursor at the beginning or end of the line in your script file and pressing CTRL+Enter (Windows) or Command+Return (Mac)\n5+2\n\n[1] 7\nYou should see the result displayed in the console below.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "getting_started.html#storing-objects",
    "href": "getting_started.html#storing-objects",
    "title": "1  R Fundamentals",
    "section": "1.2 Storing Objects",
    "text": "1.2 Storing Objects\nR is especially helpful for allowing us to create and store objects that we can call and manipulate later. We can create names for these objects and then use R’s ‘assignment operator,’ the &lt;- symbol, to assign a value to our specified object name. Here, we’ll assign the previous calculation to an object that we are calling our_object.\nIf you run this command on your own device, you should see our_object populate in the upper-right Environment window. This is where you can find all of the objects that you create in your R session. We can run the object itself, as well as combine it with other operations\n\nour_object &lt;- 5+2\n\nThere are some more baroque ways around this, but it’s best to operate under the impression that object names cannot include spaces (or start with numbers). This kind of thing is common in some programming languages, so there are a couple stylistic conventions to address this. I tend to use what’s called ‘snake case,’ which involves replacing spaces with underscores. There’s also ‘camel case,’ where each word has the first letter capitalized, e.g. MyVariableName. I would settle on one that you like and be consistent with it.\n\nour_object\n\n[1] 7\n\nour_object + 3\n\n[1] 10\n\nour_object * 100\n\n[1] 700",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "getting_started.html#a-note-on-functions",
    "href": "getting_started.html#a-note-on-functions",
    "title": "1  R Fundamentals",
    "section": "1.3 A Note on Functions",
    "text": "1.3 A Note on Functions\nR is also useful for its implementation of functions, which you can think of in the sense you likely learned in your math classes. Functions are defined procedures that take some input value, transform that value according to the procedure, and then output a new value.\nR comes with a great deal of already defined functions, and we can use these to perform all sorts of helpful operations. You can call a function by indicating it’s common name and then placing it’s required inputs between parentheses, e.g. function_name(input).Note that function inputs are also often referred to as ‘arguments’. We’ll get a lot of mileage out of functions, and part of the initial learning curve of R will be related to getting used to the range of available functions and the syntax you must follow to call them.\nNow, let’s take a step back and think about some of our basic building blocks in R.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "getting_started.html#vectors-and-r-data-types",
    "href": "getting_started.html#vectors-and-r-data-types",
    "title": "1  R Fundamentals",
    "section": "1.4 Vectors and R Data Types",
    "text": "1.4 Vectors and R Data Types\nYou can think of vectors as ordered sets of values. We can use the c() function (short for ‘combine’) to create a vector made up of the values we provide. Let’s make a few different vectors—each one will have 5 separate items in it, and we separate those items with commas. Note that when we want R to process something as text (and not a named object, number, or function), we put it in quotation marks.\n\nnum_vec &lt;- c(1.2, 3.4, 5.6, 7.1, 2.8)\n\ncharacter_vec &lt;- c(\"east\", \"west\", \"south\", \"south\", \"north\") \n\nlogical_vec &lt;- c(TRUE, FALSE, TRUE, FALSE, FALSE) \n\nLet’s talk a bit about what we have here. Each of these vectors represents a data type in R, or, in other words, one of the basic ways in which R stores data. There are some more data types out there, but these are the most most relevant for us.\n\nNumeric Data: As the name suggests, this is the typical fashion in which numbers are stored in R. Numeric data encompasses both continuous values and discrete values. These are essentially numbers that can have decimal places vs. integers (whole numbers).\nCharacter Data: Character here refers to the idea of character strings. This is typically how R stores text data—as distinct strings of text. Note that, while numbers are typically processed as numeric by R, numbers can also become character data if you place them between quotation marks.\nLogical Data: In R syntax, upper-case ‘true’ and ‘false’ have fixed values and, when used without quotes, will refer to these pre-defined logical values. We probably won’t use this data type much for analyses, but we will run into them in other places. They can be useful for sorting and searching through subsets of data, and we will also use logical values to turn certain procedures on or off in some functions.\n\nMany R functions will respond differently to different data types, so it’s important to keep these in mind when you need to troubleshoot errors.\nTake the mean() function, for example. As the name implies, this function will return the arithmetic mean of a numeric vector. Let’s give it the one we just made above:\n\nmean(num_vec)\n\n[1] 4.02\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n(1.2+3.4+5.6+7.1+2.8)/5\n\n[1] 4.02\n\n\nObserve that mean() gives the same response as if we had manually calculated it. Functions can make our lives a lot easier with larger amounts of data, but always make sure you’re familiar with what’s going on under the hood of any given function.\n\n\nBut, what happens when we run the following command?\n\nmean(character_vec)\n\nWarning in mean.default(character_vec): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\n\nIt doesn’t make any sense to take the mean of the cardinal directions, so it will throw a warning message. We need a variable that can be represented numerically. As we’ll see, it’s a good habit to make sure you know the data type of your variables before you begin your analysis.\nNow that we’ve talked about some of these basic building blocks for data, let’s talk about putting them together.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "getting_started.html#data-frames",
    "href": "getting_started.html#data-frames",
    "title": "1  R Fundamentals",
    "section": "1.5 Data Frames",
    "text": "1.5 Data Frames\nFor the most part, we will be working with data frames. These are collections of data organized in rows and columns. In data science, it’s generally preferable for data to take a particular shape wherein each row indicates a single observation, and each column represents a unique variable. This is called the ‘tidy’ data format.\n\n1.5.1 Building a Data Frame\nLet’s use the vectors we created above to mock up a little data frame. We will imagine some variables that those vectors could represent. But first, let’s make a couple more vectors.\nLet’s add a vector of participant IDs associated with imaginary people in our mock data set. In accordance with tidy data, each of our rows will then represent a unique person. The column vectors will represent the variables that we are measuring for each person. Lastly, the individual cells will represent the specific values measured for each variable.\nFor reasons that will become clear in the next section, we are also going to add one more character vector.\n\np_id_vec&lt;-c(\"p1\", \"p2\", \"p3\", \"p4\", \"p5\")\n\nordinal_vec&lt;-c(\"small\", \"medium\", \"medium\", \"large\", \"medium\")\n\nNow, let’s use a function to create a data frame and store it in a new object.\nWe can use data.frame() for this. data.frame() expects that we will give it some vectors, which it will then organize into columns. We could just give it the vectors, and it would take the vector names as column names, e.g.:\n\nour_df &lt;- data.frame(p_id_vec, num_vec, character_vec, ordinal_vec, logical_vec)\n\nOr we could specify new variable names and use the = sign to associate them with the vector. We will go with this latter strategy because our current vector names do not translate well to variable names.\nWe’ll imagine building a small data frame of dog owners and rename our vectors accordingly.\n\nour_df&lt;-data.frame(\n  p_id = p_id_vec,\n  dog_size = ordinal_vec,\n  side_of_town = character_vec,\n  food_per_day = num_vec, \n  has_a_labrador = logical_vec\n)\n\n\n\n\n\n\n\nTip\n\n\n\nAs a slight tangent, note that we can use line breaks to our advantage with longer strings of code. The above command is identical to the one below, but some find the line-break strategy more intuitively readable. It’s most important that your code works, so you don’t have to organize it like that, but know that’s an option\n\nour_df &lt;- data.frame(p_id = p_id_vec, dog_size = ordinal_vec, side_of_town = character_vec, food_per_day = num_vec, has_a_labrador = logical_vec)\n\n\n\nNow our vectors make up meaningful variables in our mock data frame.\n\np_id = An ID for each participant in our survey of dog owners\ndog_size = Owner’s ranking of their dog’s size\nside_of_town = Which part of town the owners reside\nfood_per_day = The amount of food each owner feeds their dog daily (in ounces)\nhas_a_labrador = true/false indicator for whether the owner has a lab or not\n\n\n\n1.5.2 Examining our Data Frame\nTake a look at our new data frame by clicking on the object in our Environment window at the upper right, or by running the command View(our_df).\nOnce we have created a data frame, we can refer to individual variable vectors with the $ operator in R\n\nour_df$food_per_day\n\n[1] 1.2 3.4 5.6 7.1 2.8\n\nmean(our_df$food_per_day)\n\n[1] 4.02\n\n\nWe can look at some basic characteristics of our variables with the summary() function. Note that it will return different information depending on the data type of the variable\n\nsummary(our_df)\n\n     p_id             dog_size         side_of_town        food_per_day \n Length:5           Length:5           Length:5           Min.   :1.20  \n Class :character   Class :character   Class :character   1st Qu.:2.80  \n Mode  :character   Mode  :character   Mode  :character   Median :3.40  \n                                                          Mean   :4.02  \n                                                          3rd Qu.:5.60  \n                                                          Max.   :7.10  \n has_a_labrador \n Mode :logical  \n FALSE:3        \n TRUE :2        \n                \n                \n                \n\n\nLet’s think about these for a second.\nThe summary of has_a_labrador makes sense. It’s recognized as a logical vector and tells us the number of TRUEs and FALSEs\nfood_per_day works as well. We’re dealing with a continuous variable that allows for decimal places, so it makes sense to take the mean and look at the range and distribution.\nBut how about side_of_town? What that summary tells us is that this variable is a character type (or class). ‘Length’ refers to the size of the vector. So, a vector containing 5 items would be a vector of length 5. But does it make sense for us to treat the side_of_town variable as 5 totally separate strings of characters?\n\nsummary(our_df$side_of_town)\n\n   Length     Class      Mode \n        5 character character \n\n\nNot quite. When we have two entries of “south”, for example, we want those responses to be grouped together and not treated as unique entries.\n\nour_df$side_of_town\n\n[1] \"east\"  \"west\"  \"south\" \"south\" \"north\"\n\n\nFor this, we will want another key R data type.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "getting_started.html#factors",
    "href": "getting_started.html#factors",
    "title": "1  R Fundamentals",
    "section": "1.6 Factors",
    "text": "1.6 Factors\n\n1.6.1 Unorderd Factors\nFactors are often the best way to treat categorical variables (nominal or ordinal) in R. Factors are a certain kind of vector that can only contain a number of pre-defined values. Each of these pre-defined values is considered a ‘level’ of the factor. So, we want side_of_town to be a factor variable with 4 levels: east, west, south, and north.\nWe can turn this variable into a factor with R’s as.factor() function.\n\nour_df$side_of_town &lt;- as.factor(our_df$side_of_town)\n\nCheck the summary() output again and notice how the output is reported now. Instead of simply listing that the vector contained 5 character strings, we can now see the different levels and the number of people who belong to each side of town.\n\nsummary(our_df)\n\n     p_id             dog_size         side_of_town  food_per_day \n Length:5           Length:5           east :1      Min.   :1.20  \n Class :character   Class :character   north:1      1st Qu.:2.80  \n Mode  :character   Mode  :character   south:2      Median :3.40  \n                                       west :1      Mean   :4.02  \n                                                    3rd Qu.:5.60  \n                                                    Max.   :7.10  \n has_a_labrador \n Mode :logical  \n FALSE:3        \n TRUE :2        \n                \n                \n                \n\n\n\n\n1.6.2 Ordered Factors\nNow, let’s think about dog_size. This should clearly be a factor variable as well. But, unlike food_per_day, the levels of this variable have an apparent order, from small to large.\nThe factor() function allows us to turn a vector into a factor, as well as manually specify the levels. Additionally, we can activate a process in the function letting it know that we want the order to matter.\n\nour_df$dog_size &lt;- factor(\n  our_df$dog_size, \n  levels=c(\"small\", \"medium\", \"large\"),\n  ordered = TRUE \n  )\n\nTake a look back at the summary. Now, instead of 5 separate character strings, we can see the breakdown of how many people have a dog of a certain size.\n\nsummary(our_df)\n\n     p_id             dog_size side_of_town  food_per_day  has_a_labrador \n Length:5           small :1   east :1      Min.   :1.20   Mode :logical  \n Class :character   medium:3   north:1      1st Qu.:2.80   FALSE:3        \n Mode  :character   large :1   south:2      Median :3.40   TRUE :2        \n                               west :1      Mean   :4.02                  \n                                            3rd Qu.:5.60                  \n                                            Max.   :7.10                  \n\n\nNote that the str() command is also useful for quickly gleaning the various data types of variable columns within a data frame. It will show us our variable names, the data types, and then a preview of the first several values in each variable column.\nWe can also verify that dog_size has been successfully re-coded as an ordered factor.\n\nstr(our_df)\n\n'data.frame':   5 obs. of  5 variables:\n $ p_id          : chr  \"p1\" \"p2\" \"p3\" \"p4\" ...\n $ dog_size      : Ord.factor w/ 3 levels \"small\"&lt;\"medium\"&lt;..: 1 2 2 3 2\n $ side_of_town  : Factor w/ 4 levels \"east\",\"north\",..: 1 4 3 3 2\n $ food_per_day  : num  1.2 3.4 5.6 7.1 2.8\n $ has_a_labrador: logi  TRUE FALSE TRUE FALSE FALSE\n\n\nThere are cases where you will want to convert a column like p_id to a factor variable as well, but often we just need a variable like p_id to serve as a searchable index for individual observations, so we can leave it be for now.\nThis is all part of the process of data cleaning, where we make sure our data is structured in a fashion that’s amenable to analysis. This re-coding of variables is an essential component, and we’ll see plenty more tasks in this vein when we work with GSS data later on.\nAs we close this section, here is a figure to help you internalize the hierarchy of variable types based on the levels of measurement. The bottom level of the hierarchy (in green) reflects the R data type that is best aligned with a particular measurement level. Also recall that numeric data can either be interval or ratio, though we will generally treat these similarly.\n\n\n\nA hierarchy of variables and their corresponding R data types\n\n\nFor our last bit, let’s learn a little about working with functions that don’t come included in base R.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "packages_and_tidy.html",
    "href": "packages_and_tidy.html",
    "title": "2  Packages and the Tidyverse",
    "section": "",
    "text": "2.1 Loading Packages\nR’s open-source culture has encouraged a rich ecosystem of custom functions designed by scientists and researchers in the R userbase. These come in the form of ‘packages’, which are suites of several related functions. For example, there are packages for conducting statistical tests, producing data visualizations, generating publication-ready tables, and all manner of other tasks.\nLet’s try this out with one of the better known R packages–‘tidyverse’. This is actually a collection of several packages with a variety of interrelated functions for ‘tidying’, visualizing, and analyzing data. We will focus on what we need from ‘tidyverse’, but, if you’re curious, you can read more here: https://www.tidyverse.org/\nIf you’re on a lab computer, this package may already be installed. Let’s check by running the following command:\nlibrary(tidyverse)\nIf you receive an error when you run this, you likely do not have the package installed on your system. This is also probably the case if you are on your personal device and only recently acquired R.\nIf you got an error, run the following command:\ninstall.packages(\"tidyverse\")\nWith a few exceptions, you will always install new packages in this fashion: install.packages(“package_name”)\nAfter it’s done installing, go back and run the library(tidyverse) command again. Note that you always need to do this for an added package. Whether you’ve had it for a while or just installed it, you need to load any outside package into your current session by placing its name in the library() function.\nlibrary(tidyverse)",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "packages_and_tidy.html#bringing-in-our-data",
    "href": "packages_and_tidy.html#bringing-in-our-data",
    "title": "2  Packages and the Tidyverse",
    "section": "2.2 Bringing in our Data",
    "text": "2.2 Bringing in our Data\nLet’s try bringing in a data frame to play with a few tidyverse functions. We’ll use the load() function to bring in a subset of the General Social Survey, which contains a few variables from the 2022 survey wave. Run the following command and select the file “our_gss.rda”\n\nload(file.choose())\n\nThe file.choose() function will open up a file-explorer window that allows you to manually select an R data file to load in. We’ll talk about some other ways to import data files using R syntax next time.\nGo ahead and take a look at the data frame. Each GSS survey wave has about 600-700 variables in total, so I’ve plucked several and done a little pre-processing to get us a subset to work with. All the variables here have pretty straightforward names, but I’ll note that realrinc is a clear outlier there. This is short for ‘Real respondent’s income’ and reflects the respondent’s income reported in exact dollar amounts. I’ll put a summary here so you can take a look if you’re not following along with your own script.\n\nsummary(our_gss)\n\n      year            id            age           race          sex      \n Min.   :2022   1      :   1   Min.   :18.00   white:2514   female:1897  \n 1st Qu.:2022   2      :   1   1st Qu.:34.00   other: 412   male  :1627  \n Median :2022   3      :   1   Median :48.00   black: 565   NA's  :  20  \n Mean   :2022   4      :   1   Mean   :49.18   NA's :  53                \n 3rd Qu.:2022   5      :   1   3rd Qu.:64.00                             \n Max.   :2022   6      :   1   Max.   :89.00                             \n                (Other):3538   NA's   :208                               \n    realrinc                                      partyid   \n Min.   :   204.5   independent (neither, no response):835  \n 1st Qu.:  8691.2   strong democrat                   :595  \n Median : 18405.0   not very strong democrat          :451  \n Mean   : 27835.3   strong republican                 :431  \n 3rd Qu.: 33742.5   independent, close to democrat    :400  \n Max.   :141848.3   (Other)                           :797  \n NA's   :1554       NA's                              : 35  \n           happy               marital                            attend    \n not too happy: 799   divorced     : 608   never                     :1149  \n pretty happy :1942   married      :1468   about once or twice a year: 464  \n very happy   : 779   never married:1095   every week                : 441  \n NA's         :  24   separated    : 103   less than once a year     : 416  \n                      widowed      : 255   several times a year      : 346  \n                      NA's         :  15   (Other)                   : 693  \n                                           NA's                      :  35  \n    cappun    \n favor :2013  \n oppose:1327  \n NA's  : 204",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "packages_and_tidy.html#data-wrangling-with-tidyverse",
    "href": "packages_and_tidy.html#data-wrangling-with-tidyverse",
    "title": "2  Packages and the Tidyverse",
    "section": "2.3 Data Wrangling with Tidyverse",
    "text": "2.3 Data Wrangling with Tidyverse\nLet’s use this subset to explore some tidyverse functionality. One of the packages included in the tidyverse is dplyr, which includes several functions for efficiently manipulating data frames in preparation for analyses. We will encounter a number of these throughout our time with R, but I want to briefly introduce a few key dplyr functions and operations that we will dig into more next time.\n\n2.3.1 select()\nIt happens quite often that we have a data frame containing far more variables than we need for a given analysis. The select() function allows us to quickly subset data frames according to the variable columns we specify.\nThis function takes a data frame as its first input, and all following inputs are the variable columns that you want to keep\n\nsex_inc &lt;- select(our_gss, id, sex, realrinc)\n\nYou should now have an object that contains all 3,544 observations, but includes only the 3 columns that we specified with select().\n\nsummary(sex_inc)\n\n       id           sex          realrinc       \n 1      :   1   female:1897   Min.   :   204.5  \n 2      :   1   male  :1627   1st Qu.:  8691.2  \n 3      :   1   NA's  :  20   Median : 18405.0  \n 4      :   1                 Mean   : 27835.3  \n 5      :   1                 3rd Qu.: 33742.5  \n 6      :   1                 Max.   :141848.3  \n (Other):3538                 NA's   :1554      \n\n\n\n\n2.3.2 filter()\nfilter() functions similarly except that, instead of sub-setting by specific variables, it allows you to subset by specific values. So, let’s take the sex_inc object we just created above. We now have this subset of three variables—id, sex, and income—but let’s imagine we want to answer a question that’s specific to women.\nIn order to do that, we need to filter the data to include only observations where the value of the variable sex is ‘female’.\n\nfem_inc &lt;- filter(sex_inc, sex==\"female\")\n\nNote that the fem_inc object still has 3 variables, but there are now roughly half the observations, suggesting that we have successfully filtered out the male observations.\n\nsummary(fem_inc)\n\n       id           sex          realrinc       \n 1      :   1   female:1897   Min.   :   204.5  \n 3      :   1   male  :   0   1st Qu.:  7668.8  \n 4      :   1                 Median : 15337.5  \n 7      :   1                 Mean   : 22702.1  \n 9      :   1                 3rd Qu.: 27607.5  \n 10     :   1                 Max.   :141848.3  \n (Other):1891                 NA's   :883       \n\n\n\n\n2.3.3 summarize()\nAs the name suggests, summarize() allows us to quickly summarize information across variables. It will give us a new data frame that reflects the summaries that we ask for, which can be very useful for quickly generating descriptive statistics. We will use this to get the mean income value for our data frame.\n\nmean_inc &lt;- summarize(our_gss, \"mean_inc\"=mean(realrinc, na.rm=TRUE))\n\n\n\n\n\n\n\nNote\n\n\n\nYou probably noticed the na.rm = TRUE input that I supplied for the above function. This is short for ‘remove NAs’, which we need to do when a variable has any NA values. If we don’t, R will screw up, because it does not know to disregard NA values when calculating a column mean unless we tell it to.\n\n\nThis gives us a new data frame that we called mean_inc. It should have 1 row and 1 column, and it just gives us the average income of a person in our GSS subset—about $28,000/year.\n\nmean_inc\n\n  mean_inc\n1 27835.33\n\n\nNow, this is not really all that impressive when we are asking for a broad summary like this. In fact, if all we wanted was to see the average income, we could get that more easily, e.g.\n\nmean(our_gss$realrinc, na.rm = TRUE)\n\n[1] 27835.33\n\n\nThe true power of summarize() comes from chaining it together with other tidyverse functions. However, in order to do that, we will need to learn about one more new R operation. I’ll show you that in a moment, but let’s take a look at one more helpful tidyverse function.\n\n\n2.3.4 group_by()\nOften when we’re using a function like summarize(), we want to get summaries for all kinds of different subgroups within our data set. For example, we may want the mean for each value of sex or partyid, rather than for all people in the data frame. We can do this with group_by.\nThis function may seem a little unusual when used in isolation, because it does not seem to do much on the surface.\n\nour_gss &lt;- group_by(our_gss, partyid)\n\nWhen you run that function, you will not generate any new objects, and you will not notice anything different about the data frame.\nWhat it does is overlay a grouping structure on the data frame, which will in turn affect how other tidyverse functions operate.\nCompare the output of summarize() run on this grouped version of our data frame with the use of summarize() above.\n\nmean_inc &lt;- summarize(\n  our_gss,\n  \"mean_inc\" = mean(realrinc, na.rm = TRUE)\n  )\n\nmean_inc\n\n# A tibble: 9 × 2\n  partyid                            mean_inc\n  &lt;fct&gt;                                 &lt;dbl&gt;\n1 strong democrat                      30677.\n2 independent (neither, no response)   21570.\n3 not very strong republican           29101.\n4 not very strong democrat             31743.\n5 independent, close to democrat       27916.\n6 other party                          23891.\n7 independent, close to republican     26825.\n8 strong republican                    32376.\n9 &lt;NA&gt;                                 16922.\n\n\nWe ran the same summarize() command as before, but now it reflects the grouping structure that we imposed.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "packages_and_tidy.html#the-pipe",
    "href": "packages_and_tidy.html#the-pipe",
    "title": "2  Packages and the Tidyverse",
    "section": "2.4 The Pipe",
    "text": "2.4 The Pipe\nThis one might be a little unintuitive, so don’t worry if it doesn’t immediately click. We will continue to get plenty of practice with it over the next couple of sessions.\nThe pipe operator looks like this: |&gt;. What it does is take whatever is to the left of the symbol and ‘pipe’ it into the function on the right-hand side. That probably sounds a little strange, so let’s see some examples.\nWe’ll refer back to our summarize() command from above.\n\nmean_inc &lt;- summarize(our_gss, \"mean_inc\"=mean(realrinc, na.rm=TRUE))\n\nThis is equivalent to…\n\nmean_inc &lt;- our_gss |&gt;\n  summarize(\"mean_age\" = mean(realrinc, na.rm=TRUE))\n\nNotice that, in the first command, the first input that we give summarize() is the data frame that we want it to work with.\nIn the command featuring the pipe operator, we supply the data frame and then pipe it into summarize(). The real magic comes from chaining multiple pipes together. This will likely take a little practice to get used to, but it can become a very powerful tool in our R arsenal.\n\n2.4.1 Putting It All Together\nLet’s illustrate with an example. I’ll let you know what I want to do in plain English, and then I will execute that desire with multiple piped commands.\nUltimately, I want to see the mean income, but I want to see the mean broken down by sex and partyid.\nSo, I want to take a selection of variables from our_gss. I want these variables to be grouped by sex and partyid. Finally, I want to see a summary of the mean according to this variable grouping.\n\nsexpol_means &lt;- our_gss |&gt;\n  select(id, sex, realrinc, partyid) |&gt;\n  group_by(sex, partyid) |&gt;\n  summarize(\"mean_inc\" = mean(realrinc, na.rm=TRUE)) |&gt;\n  drop_na(sex, partyid)\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can use drop_na() to do as the function’s name suggests. When we learned about group_by() above, you may have noticed that a mean was reported for an NA category within the partyid variable. Any time you notice this and want your summaries to exclude these NA categories, just include that variable as an input to drop_na().\n\n\n\nsexpol_means\n\n# A tibble: 16 × 3\n# Groups:   sex [2]\n   sex    partyid                            mean_inc\n   &lt;fct&gt;  &lt;fct&gt;                                 &lt;dbl&gt;\n 1 female strong democrat                      30111.\n 2 female independent (neither, no response)   17923.\n 3 female not very strong republican           22448.\n 4 female not very strong democrat             24300.\n 5 female independent, close to democrat       19961.\n 6 female other party                          26595.\n 7 female independent, close to republican     20095.\n 8 female strong republican                    21584.\n 9 male   strong democrat                      31600.\n10 male   independent (neither, no response)   25474.\n11 male   not very strong republican           34544.\n12 male   not very strong democrat             41233.\n13 male   independent, close to democrat       36947.\n14 male   other party                          22450.\n15 male   independent, close to republican     31393.\n16 male   strong republican                    40210.\n\n\nSo, using dplyr, we can quickly subset and manipulate data frames in just a few lines of relatively straightforward code. Here we have all the means for each value of sex and partyid, which would have been a tedious task had we calculated them all manually.\nWe will see plenty more on the tidyverse, so don’t fret if you don’t feel completely confident with these yet. It takes practice getting used to Rs peculiarities. We will keep building with these in the next unit and hopefully accumulate some muscle memory.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "recoding_variables.html",
    "href": "recoding_variables.html",
    "title": "3  Recoding Variables",
    "section": "",
    "text": "3.1 Background\nToday’s venture concerns univariate analysis, i.e. the quantitative description of a single variable. Before we do that, however, we need to familiarize ourselves with some data-cleaning procedures.\nRecoding a variable involves manipulating the underlying structure of our variable such that we can use it for analysis. We did a little recoding during the last unit when we converted character vectors into factor variables. This allowed us to align R data types with the appropriate level of measurement.\nThere are also occasions when we need a variable to be translated from one level of measurement to another. For example, we may want to convert a ratio variable for “number of years of education” into an ordinal variable reflecting categories like “less than high school”, “high school diploma”, “Associates degree”, and so on.\nWe may also want to collapse the categories of ordinal variables for some analyses. Consider a variable with a Likert-scale agreement rating, where you responses like “strongly agree,” “moderately agree,” “slightly agree,” and so forth. You may decide to collapse these categories into categories of “Agree” and “Disagree”.\nWe will get some practice doing this sort of thing, which is an essential component of responsible analysis. Additionally, our next unit on bivariate analysis will require us to work with categorical variables in particular, so we need to be capable of converting any numeric variables.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Recoding Variables</span>"
    ]
  },
  {
    "objectID": "recoding_variables.html#converting-numeric-to-categorical",
    "href": "recoding_variables.html#converting-numeric-to-categorical",
    "title": "3  Recoding Variables",
    "section": "3.2 Converting Numeric to Categorical",
    "text": "3.2 Converting Numeric to Categorical\nWe will start by recoding age—a ratio variable—into an ordinal variable reflecting age groupings. The same strategies we use here will work for any numeric variable.\n\n3.2.1 Setting up our workspace\nAs usual, let’s make sure we load in tidyverse along with our GSS data.\n\nlibrary(tidyverse)\nload(\"our_gss.rda\")\n\nLet’s double check the structure of our data frame.\n\nstr(our_gss)\n\n'data.frame':   3544 obs. of  11 variables:\n $ year    : num  2022 2022 2022 2022 2022 ...\n $ id      : Factor w/ 4149 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ age     : num  72 80 57 23 62 27 20 47 31 72 ...\n $ race    : Factor w/ 3 levels \"white\",\"other\",..: 1 1 1 1 1 1 2 1 1 NA ...\n $ sex     : Factor w/ 2 levels \"female\",\"male\": 1 2 1 1 2 2 1 2 1 1 ...\n $ realrinc: num  40900 NA 18405 2250 NA ...\n $ partyid : Factor w/ 8 levels \"strong democrat\",..: 1 2 3 1 2 4 5 1 5 1 ...\n $ happy   : Ord.factor w/ 3 levels \"not too happy\"&lt;..: 1 1 1 1 2 2 2 2 2 2 ...\n $ marital : Factor w/ 5 levels \"divorced\",\"married\",..: 1 2 1 3 3 3 3 2 3 3 ...\n $ attend  : Factor w/ 9 levels \"never\",\"less than once a year\",..: 3 3 1 5 3 1 2 2 1 8 ...\n $ cappun  : Factor w/ 2 levels \"favor\",\"oppose\": 2 1 1 2 2 2 2 2 1 NA ...\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might notice the ‘int’ category, which is short for ‘integer’. This is a subtype of numeric data in R. Variables that are exclusively whole numbers are often recorded in this way, but we can work with them in R just like we can other sorts of numbers\n\n\n\n\n3.2.2 The ‘age’ variable\nWe can take a look at all the values of age (along with the # of respondents in each age category) using the count() function.\n\ncount(our_gss, age)\n\nHowever, I’m not going to display those results here, as it will be a particularly long table of values (with 70+ different ages). It’s fine to run it—it won’t crash R or anything—but it will it clutter up this page. Let’s take this as a good opportunity to take advantage of the fact that we are using one of the better funded and well-organized surveys in all of social sciences. As such, there’s extensive documentation about all of the variables measured for the GSS. Go ahead and take a look at the age variable via the GSS Data Explorer, which allows us to search for unique variables and view their response values, the specific question(s) that was asked on the survey, and several other variable characteristics.\nThe responses range from 18 - 89 (in addition to a few categories for non-response). However, note that there’s something unique about value 89. It’s not just 89 years of age, but 89 & older. This isn’t a real issue for our purposes, but take this as encouragement to interface with the codebook of any publicly available data you use. There’s some imprecision at the upper end of this variable, and that might not be obvious without referencing the codebook.\nFor the purposes of this exercise, let’s go ahead and turn age into a simple categorical variable with 3 levels—older, middle age, and younger. I’m going to choose the range somewhat arbitrarily for now. We can use univariate analysis to inform our decision about how to break up a numeric variable, so we will revisit this idea again later on.\n\nYounger = 18 - 35\nMiddle Age = 36 - 49\nOlder = 50 and up\n\nAt the end of the day, what we need to do is 1.) create a new variable column 2.) populate that column with ordinal labels that correspond with each respondent’s numeric age interval.\n\n\n3.2.3 New columns with mutate()\nFirst, let’s consider the mutate() function. This function takes a data frame and appends a new variable column. This new variable is the result of some calculation applied to an existing variable column.\nLet’s look at an application fo mutate() to get a feel for it. Now, this wouldn’t be the best idea for a couple reasons, but, as an illustration, let’s say we wanted to convert our yearly income values to an hourly wage (assuming 40 hrs/week).\nmutate() takes a data frame as its input, and then we provide the name of our new variable column(s) along with the calculation for this new variable. Below, I use mutate() to create a new column called hr_wage. Then, I tell R that the hr_wage variable should be calculated by taking each person’s income value and diving that by 52 weeks in a year, and then 40 hours in a week.\n\n# Without the pipe operator\nour_gss &lt;- mutate(\n  our_gss,\n  hr_wage = (realrinc/52)/40\n  )\n\n\n# With the pipe operator\nour_gss &lt;- our_gss |&gt;\n  mutate(\n    hr_wage = (realrinc/52)/40\n  )\n\nTake a look at your new data frame object. I’ll show a summary here to verify that we got our new column.\n\nsummary(our_gss)\n\n      year            id            age           race          sex      \n Min.   :2022   1      :   1   Min.   :18.00   white:2514   female:1897  \n 1st Qu.:2022   2      :   1   1st Qu.:34.00   other: 412   male  :1627  \n Median :2022   3      :   1   Median :48.00   black: 565   NA's  :  20  \n Mean   :2022   4      :   1   Mean   :49.18   NA's :  53                \n 3rd Qu.:2022   5      :   1   3rd Qu.:64.00                             \n Max.   :2022   6      :   1   Max.   :89.00                             \n                (Other):3538   NA's   :208                               \n    realrinc                                      partyid   \n Min.   :   204.5   independent (neither, no response):835  \n 1st Qu.:  8691.2   strong democrat                   :595  \n Median : 18405.0   not very strong democrat          :451  \n Mean   : 27835.3   strong republican                 :431  \n 3rd Qu.: 33742.5   independent, close to democrat    :400  \n Max.   :141848.3   (Other)                           :797  \n NA's   :1554       NA's                              : 35  \n           happy               marital                            attend    \n not too happy: 799   divorced     : 608   never                     :1149  \n pretty happy :1942   married      :1468   about once or twice a year: 464  \n very happy   : 779   never married:1095   every week                : 441  \n NA's         :  24   separated    : 103   less than once a year     : 416  \n                      widowed      : 255   several times a year      : 346  \n                      NA's         :  15   (Other)                   : 693  \n                                           NA's                      :  35  \n    cappun        hr_wage        \n favor :2013   Min.   : 0.09832  \n oppose:1327   1st Qu.: 4.17849  \n NA's  : 204   Median : 8.84856  \n               Mean   :13.38237  \n               3rd Qu.:16.22236  \n               Max.   :68.19631  \n               NA's   :1554      \n\n\nSo, we can use mutate() to add a new column that contains our recoded variable. We just need a way to specify a calculation that takes into account the specific intervals we want for our ordinal labels. For this, we need one more function.\n\n\n3.2.4 Custom intervals with cut()\nWe can use the cut() function to specify the intervals we want for our age groupings, and then we will combine it with mutate() to generate our recoded age variable. Specifically, cut() takes our intervals and turns each of them into a level in a new factor variable.\nBut first, I want to give a little context on interval notation in mathematics. It will help us all be a little more precise when we talk about ranges, and it will also help us understand an input we need to provide for cut().\n\n3.2.4.1 An aside on intervals\nIn mathematic terms, an interval is the set of all numbers in between two specified end points. In formal notation, these are indicated with the two endpoints placed in brackets, e.g [1,5] as the interval of 1 through 5.\nThere are some technical terms to describe whether or not we want to include the endpoints when we are talking about a particular interval.\n\nClosed interval: This is when both end points are included in the interval. Closedness is indicated with square brackets, so, the closed interval of 1 through 5 would be written just like I have above—[1,5]. This indicates any number \\(x\\) where \\(1 \\leq x \\leq 5\\)\nOpen interval: This is when neither endpoint is included in the interval. In interval notation, openness is indicated with parentheses rather than square brackets, so the open interval of 1 through 5 would be written as (1,5). This interval would indicate any number \\(x\\) where \\(1 &lt; x &lt; 5\\)\nLeft-open interval: This is when the left-hand endpoint is not included, but the right-hand endpoint is. This would be written as (1,5] in interval notation, and that interval would indicate any number \\(x\\) where \\(1 &lt; x \\leq 5\\)\nRight-open interval: When the right-hand endpoint isn’t included but the left endpoint is, you have a right-open interval. This would be written as [1,5) in interval notation and would indicate any number \\(x\\) where \\(1 \\leq x &lt; 5\\).\n\nWe will be working with the right-open interval format, and we can specify that in cut().\nNow, let’s return to our task at hand.\n\n\n3.2.4.2 Inputs for cut()\nWe’ll go ahead and work with cut directly in mutate(), as its going to be the calculation that we are providing for the new column we generate with mutate().\nAs a reminder, here are the intervals we need:\n\nYounger = 18 - 35\nMiddle Age = 36 - 49\nOlder = 50 and up\n\nThe following code may look a little chaotic at first glance, but it’s really the same sort of thing that we did with mutate() above. It’s just that cut() is a little bit more involved of a calculation than the simple division we did in our example.\nJust like above, we are applying mutate() to our_gss, and we are giving the name of our new variable column (age_ord, in this case) as the first input. Then, for the calculation, we give the cut() function and the arguments it needs.\ncut() and its inputs\n\nThe variable for which we want to specify intervals (age)\nbreaks: This is where we indicate the intervals. The first number we give is the low end of our lowest age group (18). The second number is the low end of our middle age group (36). The third number, as you probably guessed, is the low end of our highest age group. The last value should reflect our upper limit. In this case, I use the value Inf, which is short for ‘infinity’. This essentially tells R that the last category can include any values higher than the previous number we entered.\ninclude.lowest: Putting TRUE here tells R that, in each interval, we want the lowest value to be included. If we don’t do this, then our ‘younger’ age grouping would be 19 - 35 rather than 18 - 35. In other words, setting this to TRUE indicates a left-closed interval, and FALSE indicates a left-open interval.\nright: This is the input for specifying whether we want this to be a right-closed interval, and it takes a logical value (TRUE or FALSE). We want a right-open interval, so we will set this to FALSE.\nlabels: R will actually default to formal interval notation for the names of each level of our new factor variable, so it would be [18,36), [36,50), [50, Inf). However, we can provide a character vector to specify custom labels for these new factor levels. In the event that you have an ordinal variable, make sure that you always specify these labels in ascending order. In this case, that would be Younger &lt; Middle Age &lt; Older.\nordered_result: This takes a logical value and, as the name suggests, indicates whether we want the factor to be ordered or not. In our case, there is a clear progression in terms of age, so we need to set this to TRUE.\n\n\nour_gss &lt;- our_gss |&gt;\n  mutate(\n    age_ord = cut(\n      age,\n      breaks = c(18, 36, 50, Inf),\n      include.lowest = TRUE,\n      right = FALSE,\n      labels = c(\"Younger\", \"Middle Age\", \"Older\"),\n      ordered_result = TRUE\n    )\n  )\n\nGo ahead and take a look at our_gss, and we should now have an additional variable column. I’ll use the str() function here so we can confirm that our factor was ordered and added to the data frame.\n\nstr(our_gss)\n\n'data.frame':   3544 obs. of  13 variables:\n $ year    : num  2022 2022 2022 2022 2022 ...\n $ id      : Factor w/ 4149 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ age     : num  72 80 57 23 62 27 20 47 31 72 ...\n $ race    : Factor w/ 3 levels \"white\",\"other\",..: 1 1 1 1 1 1 2 1 1 NA ...\n $ sex     : Factor w/ 2 levels \"female\",\"male\": 1 2 1 1 2 2 1 2 1 1 ...\n $ realrinc: num  40900 NA 18405 2250 NA ...\n $ partyid : Factor w/ 8 levels \"strong democrat\",..: 1 2 3 1 2 4 5 1 5 1 ...\n $ happy   : Ord.factor w/ 3 levels \"not too happy\"&lt;..: 1 1 1 1 2 2 2 2 2 2 ...\n $ marital : Factor w/ 5 levels \"divorced\",\"married\",..: 1 2 1 3 3 3 3 2 3 3 ...\n $ attend  : Factor w/ 9 levels \"never\",\"less than once a year\",..: 3 3 1 5 3 1 2 2 1 8 ...\n $ cappun  : Factor w/ 2 levels \"favor\",\"oppose\": 2 1 1 2 2 2 2 2 1 NA ...\n $ hr_wage : num  19.66 NA 8.85 1.08 NA ...\n $ age_ord : Ord.factor w/ 3 levels \"Younger\"&lt;\"Middle Age\"&lt;..: 3 3 3 1 3 1 1 2 1 3 ...",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Recoding Variables</span>"
    ]
  },
  {
    "objectID": "recoding_variables.html#restructuring-categorical-variables",
    "href": "recoding_variables.html#restructuring-categorical-variables",
    "title": "3  Recoding Variables",
    "section": "3.3 Restructuring Categorical Variables",
    "text": "3.3 Restructuring Categorical Variables\nNow, let’s go ahead and tackle the other recoding situation I mentioned up at the top—collapsing the levels of categorical variables. We’ll work with partyid here, but the logic of this process will apply to any categorical variable.\n\n3.3.1 Collapsing categories\nI encourage you to take a look at partyid in the GSS Data Explorer, like we did for ‘age’..\nBecause partyid is a categorical variable, it has far fewer unique values than a ratio variable like age, so we can go ahead and take a look at all of its levels with the count() function.\n\n\n\n\n\n\nNote\n\n\n\nObserve that count(our_gss, partyid) basically provides the same information as summary(our_gss$partyid) when the variable is a factor. The only real difference is that count() organizes the info into a tidy data frame. In other words, you can use either function to take a look at factor variable like this.\n\n\n\ncount(our_gss, partyid)\n\n                             partyid   n\n1                    strong democrat 595\n2 independent (neither, no response) 835\n3         not very strong republican 361\n4           not very strong democrat 451\n5     independent, close to democrat 400\n6                        other party 106\n7   independent, close to republican 330\n8                  strong republican 431\n9                               &lt;NA&gt;  35\n\n\nLet’s go ahead and collapse these into categories of “Democrat”, “Independent”, “Other Party”, and “Republican”. We’ll use mutate() to make a new variable column called partyid_recoded. For the calculation of our new variable, we can use the fct_collapse() function. This function allows us to first give the name of a new factor level, and then we can give a character vector of the names of levels that want to be collapses into a single category.\nSo, to collapse our 2 Democrat levels into a single factor level, we would give the following input for fct_collapse():\n\"Democrat\" = c(\"strong democrat\", \"not very strong democrat\")\nAnd then repeat for each new factor level.\n\nour_gss &lt;- our_gss |&gt;\n  mutate(\n    partyid_recoded=fct_collapse(partyid, \n\"Democrat\" = c(\"strong democrat\", \"not very strong democrat\"),\n\"Republican\" = c(\"strong republican\",\"not very strong republican\"),\n\"Independent\" = c(\"independent, close to democrat\", \"independent (neither, no response)\", \"independent, close to republican\"),\n\"Other Party\" = c(\"other party\")\n))\n\nNow, let’s take a look at our new variable.\n\ncount(our_gss, partyid_recoded)\n\n  partyid_recoded    n\n1        Democrat 1046\n2     Independent 1565\n3      Republican  792\n4     Other Party  106\n5            &lt;NA&gt;   35\n\n\n\n\n3.3.2 Excluding levels\nYou may also want to work with an existing categorical variable, but only focus on certain values. In this case, we can create a recoded variable and simply code the levels we are uninterested in as NA.\nWe can use the fct_recode() function for this. For the first input, we will give it the factor-variable column that we want to recode. Then, we will let it know that we want to set the levels of “Other Party” and “Independent” to NULL. This will convert them to NAs and allow us to easily exclude them from our analyses. We will use fct_recode() within mutate() so we can create another recoded version of partyid_recoded. We’ll call this one dem_rep to distinguish it from the original partyid and our first recoded version.\n\nour_gss &lt;- our_gss |&gt;\n  mutate(dem_rep = fct_recode(\n    partyid_recoded, \n    NULL = \"Other Party\", \n    NULL = \"Independent\"))\n\nLet’s check to see that it worked.\n\ncount(our_gss, dem_rep)\n\n     dem_rep    n\n1   Democrat 1046\n2 Republican  792\n3       &lt;NA&gt; 1706\n\n\nNow that we have gotten all this pre-processing stuff out of the way, let’s go ahead and dig into some univariate analysis.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Recoding Variables</span>"
    ]
  },
  {
    "objectID": "univariate_categorical.html",
    "href": "univariate_categorical.html",
    "title": "4  Univariate: Categorical",
    "section": "",
    "text": "4.1 Frequency Distributions\nBefore we start examining the relationships between multiple variables, we have to look under the hood of our variables individually and make sure we have a good sense of what they are like.\nWe want to know the frequency distribution of the response values, the measures of central tendency, and the dispersion. In other words, we want to see how many respondents chose each response value, which response values are most typical for each variable, and the extent to which the response values vary.\nLuckily for us, R has plenty of tools for generating these quantitative summaries, as well as visualizing them in the form of barplots, histograms, and other common styles for displaying data.\nThis tutorial will cover each of these elements, and I will highlight any differences specific to certain levels of measurement along the way.\nLet’s start with categorical variables. We will start with frequency distributions by way of frequency tables and bar plots. Then, we will talk about measures of tendency and observe when they are appropriate for categorical data.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate: Categorical</span>"
    ]
  },
  {
    "objectID": "univariate_categorical.html#frequency-distributions",
    "href": "univariate_categorical.html#frequency-distributions",
    "title": "4  Univariate: Categorical",
    "section": "",
    "text": "4.1.1 Frequency tables\nFrequency tables are the most effective way to report the frequency distributions for categorical variables. There are actually a ton of different ways to produce this kind of table in R, but we are going to use some convenient functions from the janitor package, so let’s go ahead and load that in.\nIf you are on a lab computer, try to load it with library() first. If you get an error, go ahead and install it with install.packages() and then load it in.\nWhile we’re at it, we’ll also bring in tidyverse and load in our GSS subset.\n\nlibrary(janitor)\nlibrary(tidyverse)\nload(\"our_gss.rda\")\n\nLet’s go ahead and work with one of our partyid recodes from last time. We’ll use partyid_recoded, where we retained all the party choices but collapsed the categories of degree (e.g. strong democrat + not very strong democrat = Democrat).\nWe can use a simple command with tabyl() from the janitor package to produce our frequency tables. Let’s take a look at some output, and I’ll go through a couple details and mention some further specifications we can make.\nAll we need to do is give our data frame as the first argument, and then our particular variable column as the second argument.\n\n# Without pipe operator\ntabyl(our_gss, partyid_recoded)\n\n\n# With pipe operator\nour_gss |&gt;\n  tabyl(partyid_recoded)\n\n partyid_recoded    n     percent valid_percent\n        Democrat 1046 0.295146727    0.29809062\n     Independent 1565 0.441591422    0.44599601\n      Republican  792 0.223476298    0.22570533\n     Other Party  106 0.029909707    0.03020804\n            &lt;NA&gt;   35 0.009875847            NA\n\n\nNow, we have a frequency table for partyid_recoded!\nEach row is a level of partyid_recoded. The n column refers to the sample size, so, for example, 1,565 respondents indicated that they were Independents.\nWe then have two different columns for percentages. The percent column is the proportion of all respondents that chose each response value—while including NA values. Often, we want to get a proportion for only the sample of those who actually answered the question. This is what the valid_percent column indicates and is what we are often looking for.\nLet’s break it down a little for clarity.\nThe total number of respondents is the sum of the ‘n’ column.\n1565 + 1046 + 792 + 106 + 35 = 3,544\nNote that this is also the number of observations in our data frame\nThere are also 35 observations coded as NA, so the total number of valid respondents is 3,509.\nNow, I’ll give the calculations for the Independent row.\n\n# I'm going to define a few objects here for simplicity.\n\nall_respondents &lt;- 3544\n\nall_minus_na &lt;- (3544 - 35)\n\nindependents &lt;- 1565\n\n\n# Percent column\n\nindependents / all_respondents\n\n[1] 0.4415914\n\n# Valid percent column\n\nindependents / all_minus_na\n\n[1] 0.445996\n\n\nIn this case, there really aren’t that many NA values, so it does not change too much. In general, we will often discard NA categories in analyses we do for the course, but they give us important information about the survey and can be useful for some questions, so it’s important to keep track of them nonetheless.\nNow, let’s clean a couple things up with the adorn_() functions.\nLet’s add a row for totals, turn the proportions into percentages, and round the decimals. We can use the pipe operator to quickly leverage a few different adorn_() functions.\n\nour_gss |&gt;\n  tabyl(partyid_recoded) |&gt;\n  adorn_totals(where = \"row\") |&gt;\n  adorn_pct_formatting() |&gt;\n  adorn_rounding(digits = 2)\n\n partyid_recoded    n percent valid_percent\n        Democrat 1046   29.5%         29.8%\n     Independent 1565   44.2%         44.6%\n      Republican  792   22.3%         22.6%\n     Other Party  106    3.0%          3.0%\n            &lt;NA&gt;   35    1.0%             -\n           Total 3544  100.0%        100.0%\n\n\nWe don’t need to supply any input for adorn_pct_formatting()—which changes the proportions to percentages—but a couple of these functions do require inputs.\nFor adorn_totals(), we give “row” for the where input, which tells the function that we want the totals column to appear as a row.\nFor adorn_rounding(), we give an input for digits, which is the number of digits we want after the decimal point. We’ll keep 2 and thereby round to the hundredth.\nLastly, if we want, we can also make the column names a little nicer. It may not be obvious because we’ve just been displaying this table rather than storing it as an object, but it’s actually a data frame, so we can edit the column names directly.\nLet’s save it as an object so that’s easier to see.\n\nour_table &lt;- our_gss |&gt;\n  tabyl(partyid_recoded) |&gt;\n  adorn_totals(where = \"row\") |&gt;\n  adorn_pct_formatting() |&gt;\n  adorn_rounding(digits = 2)\n\nWe can then use the colnames() function. Let’s supply our table data frame as an input to get a feel for it.\n\ncolnames(our_table)\n\n[1] \"partyid_recoded\" \"n\"               \"percent\"         \"valid_percent\"  \n\n\nIt lists out the character vector of the 4 column names, and we can actually just write over them by supplying our desired column names as a character vector. If we assign that character vector to colnames(our_table), it will overwrite the column names. Make sure the labels are in the same order as they appear in the data frame.\n\ncolnames(our_table) &lt;- c(\n  \"Political Party\",\n  \"Frequency\",\n  \"Percent\",\n  \"Valid Percent\"\n  )\n\nour_table\n\n Political Party Frequency Percent Valid Percent\n        Democrat      1046   29.5%         29.8%\n     Independent      1565   44.2%         44.6%\n      Republican       792   22.3%         22.6%\n     Other Party       106    3.0%          3.0%\n            &lt;NA&gt;        35    1.0%             -\n           Total      3544  100.0%        100.0%\n\n\nExcellent! We’ll learn how to make this even nicer a bit later in the semester, but this looks pretty good for now.\nLet’s look at one last visualization technique for categorical variables.\n\n\n4.1.2 Bar plots\nFor categorical data, bar plots are often the way to go. These typically have the response values on the x axis, and the count or percentage of each response value is tracked on the Y axis.\nMuch like with tables, we’ll learn some fancier ways to do this later in the semester. But, in the meanwhile, base R has some great plotting functions that can be used very easily without much specification. They’re not the prettiest out of the box, but they will do everything we need.\nFor barplot(), we just need the variable column that we are trying to plot. But, there’s one thing to remember when you are trying to get a barplot this way—you have to provide the result of summary(my_variable) rather than the variable column directly.\nI’ll show you a couple ways to do that, which all do the exact same thing.\n\n# No pipe operator; summary() nested inside barplot()\n\nbarplot(summary(our_gss$partyid_recoded))\n\n\n# With pipe operator\nour_gss$partyid_recoded |&gt;\n  summary() |&gt;\n  barplot()\n\n\n# Make separate object for the summary() output\n\nmy_summary &lt;- summary(our_gss$partyid_recoded)\n\nbarplot(my_summary)\n\n\n\n\n\n\n\n\nNow, as I mentioned, we won’t spend too much time adjusting these plots for now, but there are a couple things we can do to make this a little better to look at for now.\nFor one, our response labels (Democrat, Republican, Independent, etc) Actually look pretty good in the display here, but it’s not uncommon for these to get cut off if we have several response categories or the response names are especially long. So, I’ll show you how to reduce the font size as one potential fix for that.\nWe can add the cex.names argument to barplot(). This argument takes a number, and the number should reflect an intended ratio of the default font size for our value labels on the x-axis. So, for example, I would enter ‘2’ if I wanted the font to be twice as big. For our purposes, I want to reduce the font a bit, so we’ll enter ‘.75’ to reduce the font size by 1/4.\n\nour_gss$partyid_recoded |&gt;\n  summary() |&gt;\n  barplot(cex.names = .75)\n\n\n\n\n\n\n\n\nWe can also add some simple descriptive information for the plot, such as a title and labels for the x and y axis. We can do so within barplot()\n\nour_gss$partyid_recoded |&gt;\n  summary() |&gt;\n  barplot(\n    cex.names = .75, \n    main = \"Count of GSS Respondents by Political Party\",\n    xlab = \"Political Party\",\n    ylab = \"Count\"\n  )\n\n\n\n\n\n\n\n\nAs a final point, I’ll also note that you can pass partyid_recoded into the function na.omit() before summary() if you ever want to produce a barplot that excludes NA responses. na.omit() will remove any NA values from the vector we provide it.\n\nour_gss$partyid_recoded |&gt;\n  na.omit() |&gt;\n  summary() |&gt;\n  barplot(\n    cex.names = .75,\n    main = \"Count of GSS Respondents by Political Party\",\n    xlab = \"Political party\"\n  )",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate: Categorical</span>"
    ]
  },
  {
    "objectID": "univariate_categorical.html#measures-of-central-tendency",
    "href": "univariate_categorical.html#measures-of-central-tendency",
    "title": "4  Univariate: Categorical",
    "section": "4.2 Measures of Central Tendency",
    "text": "4.2 Measures of Central Tendency\nNow, let’s talk about measures of central tendency. Because there is no mathematically meaningful relationship between the different levels of a categorical variable, our options for reporting measures of central tendency are a little more limited.\nThe classic central tendency measures are the mode, the median, and the mean. For categorical variables, it is not ever meaningful to take the mean. As we saw back on our first day with R, you will get a warning message if you try to run mean() on a character or factor variable.\nOur options are limited to the mode or the median, depending on the distribution of responses.\n\n4.2.1 Nominal variables\nIn the case nominal variables, there is no semblance of meaningful arrangement to the categories. In this case, we can only report the mode.\nOften times, we can just glean this from the frequency tables, or even just the summary() output. Let’s stick with partyid_recoded as an example.\n\nsummary(our_gss$partyid_recoded)\n\n   Democrat Independent  Republican Other Party        NA's \n       1046        1565         792         106          35 \n\n\nRemember that the mode is just the value with the most occurrences. So, the mode here is ‘Independent’.\nThis is likely sufficient for the variables we will work with, but you may also want a method that’s a little more exacting and does not depend on visually scanning for the mode.\nR does not actually have a built-in mode() function like it does for mean() and median(). Well, it does have mode(), but it evaluates the ‘mode’ of the data, which, in this case, is actually the ‘storage mode’, or the data type. This is not particularly helpful for us, but we can use another convenient package called DescTools.\nIf you are on a lab computer, go ahead and try loading it with library() first. If it does not load, then install it with install.packages(\"DescTools\") and then load it in.\n\nlibrary(DescTools)\n\nThen, we can just use the Mode() function within DescTools. Make sure you capitalize the ‘M’, as this is what distinguishes the DescTools function from the base R function.\nAlso, be sure to include na.rm = TRUE, as R will not know what to do with the NA values when calculating the mode, so we need to tell it to disregard them.\n\nMode(our_gss$partyid_recoded, na.rm = TRUE)\n\n[1] Independent\nattr(,\"freq\")\n[1] 1565\nLevels: Democrat Independent Republican Other Party\n\n\nSo, this also gives us our mode, and is a bit more precise a way of doing so—especially when we have a bunch of possible response values.\nNote that it also tells us how many responses were associated with that value (1565).\nNow, let’s talk about ordinal variables, where we have another consideration.\n\n\n4.2.2 Ordinal variables\nWhile ordinal variables are still categorical and thus unamenable to mathematical analysis, they do have a meaningful order. This means that it’s possible for us to take the median value of an ordinal variable.\nHowever, we can only do so when the variable is normally distributed. If there is significant skew in the distribution of responses, then we will need to report the mode for an ordinal variable.\nLet’s refresh our memory on distributions before we take some central tendency measures of ordinal variables in the GSS.\n\n4.2.2.0.1 Distribution assumptions\nIf the responses of an ordinal variable are normally distributed, we can take the median.\nIn the ideal form of the normal distribution, you have a mean of 0 and a standard deviation of 1, where roughly 68% of values are within 1 standard deviation in either direction of the mean, and roughly 95% of values are within 2 standard deviations in either direction.\nHowever, in practice, our distributions are unlikely to reflect the ideal normal distribution. But, if they are roughly normal, we can work with that.\nI’ll display a few distributions here as a reminder, starting with the classic normal distribution. If you produce a barplot and find the data looks generally like this—where the majority of variables are concentrated around the middle of the distribution—then you can report the median response rather than the mode.\nHere’s the normal distribution\n\n\n\n\n\n\n\n\n\nHere’s a left-skewed distribution. This is when there’s a prominent ‘tail’ on most of the left-hand side of the distribution. In other words, when most of the values are concentrated on the right-hand side of the distribution (the upper end), then we have a left-skew.\n\n\n\n\n\n\n\n\n\nLastly, here’s a right-skewed distribution. As you can probably guess from the description of a left-skewed distribution, a right-skew occurs when we have a long tail through most of the right-hand side of the distribution. In other words, most values are concentrated on the left-hand side.\n\n\n\n\n\n\n\n\n\nSo, if the distribution of your variable looks (mostly) like the normal distribution, go ahead and take the median of any ordinal variable.\nLet’s go ahead and practice doing that with the age_ord variable we created last time.\nFirst, we’ll verify that the distribution allows for reporting the median.\n\nour_gss$age_ord |&gt;\n  na.omit() |&gt;\n  summary() |&gt;\n  barplot(\n    main = \"Age Distribution of GSS Respondents\",\n    xlab = \"Age grouping\"\n  )\n\n\n\n\n\n\n\n\nWell, hold on now. In the case of age_ord, it looks like we’d actually want to take the mode, because it’s a bit left-skewed.\nLet’s try the happy variable. This is an ordinal variable in our subset that we haven’t used yet. We’ll see if its normally distributed.\n\n our_gss$happy |&gt;\n  na.omit() |&gt;\n  summary() |&gt;\n  barplot(\n    main = \"Happiness Distribution of 2022 GSS Respondents\",\n    xlab = \"Response value\"\n  )\n\n\n\n\n\n\n\n\nPerfect. The distribution is exactly what we want to see. Most of the variables are concentrated at the midpoint of the distribution, with tails on either side. So, as it’s roughly normally distributed, happy will serve as a good example of a case where taking the median of an ordinal variable is appropriate.\nNow, the median response value is fairly apparent in this case, but I’ll show you a method that you can use to calculate the median of any ordinal variable you might come across.\nWhile R has a built-in median() function, it actually only works with numeric data. Fear not, however, as we can also use the DescTools again.\nIn order to distinguish itself from the default median() function, the DescTools package uses a capitalized ‘M’. So, make sure you use Median() when you are working with an ordinal variable.\nWe also need to assure that NAs are not included for the calculation. As we have seen a few times, R will not know what to do with these if we don’t tell it to exclude them.\n\nMedian(our_gss$happy, na.rm = TRUE)\n\n[1] pretty happy\nLevels: not too happy &lt; pretty happy &lt; very happy\n\n\nPerfect! Looks like our median is ‘pretty happy’.\nAnd now we have all we need to report a univariate analysis of a categorical variable in this course:\n\nA frequency table\nA bar plot\nA median or mode\n\nFor now, it’s just important that you can accurately identify the median or mode of a categorical value. We’ll talk about ways to implement it in the tables a little later on when we focus on publication-ready tables and figures.\nNow, let’s talk about presenting a univariate analysis of numeric data.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Univariate: Categorical</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html",
    "href": "univariate_numeric.html",
    "title": "5  Univariate: Numeric",
    "section": "",
    "text": "5.1 Frequency Distributions\nWhile we also want to report on the frequency distribution and central tendency of numeric variables, there are some differences in the way we go about that relative to the techniques we just learned about for categorical variables.\nAdditionally, we will need to report on some measures of dispersion that can only be reported for numeric data, such as the standard deviation.\nThis section will walk us through the process of calculating and reporting necessary elements of a univariate analysis of numeric data.\nAs we saw when we were recoding age earlier in the unit, a frequency table is not really appropriate for numeric data. When there are upwards of 71 different response values—as we have with age—a table would be both spatially unwieldy and difficult to interpret.\nSo, let’s start with a style of plotting data that is essentially the barplot of numeric data.\nFirst, let’s go ahead and set up our environment by loading in our data and tidyverse per usual.\nlibrary(tidyverse)\nload(\"our_gss.rda\")",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#frequency-distributions",
    "href": "univariate_numeric.html#frequency-distributions",
    "title": "5  Univariate: Numeric",
    "section": "",
    "text": "5.1.1 Histograms\nA histogram is the result of taking a numeric variable, slicing it up into equal, ordinal intervals, and then plotting the frequency of each interval.\nThere are plenty of other ways for plotting numeric data—some of which we will see later when we focus on visualization—but for now, this is a simple and effective way to get a sense of the distribution of a numeric variable. This is especially important when we need to decide on an appropriate measure of central tendency.\nWe can create these easily using the hist() function, which is in the same family of base R plotting functions as barplot(), so much of what we learned for that function will also apply with hist(). This function will automatically apply a commonly used method called Sturges’s rule to divide our numeric variable into equal bins, so we do not have to specify anything in that regard.\nAll we need to do is give the hist() function our numeric variable. Let’s work with the realrinc variable for this example.\n\nhist(our_gss$realrinc)\n\n\n\n\n\n\n\n\nMuch like we did with barplot(), we can clean this up a little by adding a title and a better label for the x axis\n\nhist(\n  our_gss$realrinc,\n  main = \"Income distribution of GSS Respondents\",\n  xlab = \"Respondent's income in dollars\",\n  )\n\n\n\n\n\n\n\n\nNow we have our histogram!\nHowever, this visualization does suggest that we need to do some thinking about the appropriate measure of central tendency for realrinc.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#central-tendency",
    "href": "univariate_numeric.html#central-tendency",
    "title": "5  Univariate: Numeric",
    "section": "5.2 Central Tendency",
    "text": "5.2 Central Tendency\nFor numeric data, any of the 3 common measures of central tendency can be reported. However, the preferred measure for a given variable depends on its distribution.\nThe mean is the gold standard for numeric data, because it takes into account every single data point in its calculation, and is thus the most comprehensive index of central tendency.\nHowever, the mean can be misleading in some cases. The distribution should be (basically) normal in order for you to report the mean, and you will want to report the median in most other cases.\nI will first cover the choice of mean or median by revisiting some basics of distributions, and then I will address the rare circumstance where the mode is appropriate.\n\n5.2.1 Mean or Median?\nIn most cases, we will be making a decision between the mean and the median. When the distribution is normal, you should report the mean—otherwise, report the median. We’ll see one exception to this later, but that’s generally our task.\nWe saw some typical distributions in our last section on uivariate analysis of categorical data. We’ll return to some discussion of those basic distribution types but add a little more context for numeric data that will help us see the impact of making the wrong decision.\nI’m going to display a normal distribution, but this time, I’ll draw a vertical line indicating the location of the mean and median.\n\n\n\n\n\n\n\n\n\nIn this case, they appear to be overlapping. In fact, in a completely normal distribution, all three measures of central tendency will be equal. Though, remember that real-world data will rarely exhibit perfect normality. As long as it’s approximately normal, we can default to the mean because it accounts for every data point in the variable’s distribution.\nNow, let’s look back at realrinc.\n\n\n\n\n\n\n\n\n\nIn the case of realrinc, we definitely do not have much normality going on. For one, there are some apparent outlier cases. Much of our data seems to be concentrated within about $0 - $75,000, but then we have a bunch of values at roughly double the maximum of that narrower range. And even within the range where most of our responses are clustered, we have some clear right-skew.\nIn general, it’s ideal that the mean includes every data point, but this also makes it susceptible to non-normal distributions. As we can see, the mean will be pulled in the direction of the skew.\nIn this case, it becomes an advantage that the median does not take every data point into account. This makes it more resistant to skew, so, in any case where your numeric variable contains clear outliers and/or exhibits notable skew in either direction, you should report the median rather than the mean.\n\n\n5.2.2 The Mode\nBy and large, the mode is more appropriate for categorical data. But there are some circumstances where it makes sense to report the mode for numeric data.\nLet’s take a look at the original age variable to see an example.\n\nhist(\n  our_gss$age,\n  main = \"Age distribution of 2022 GSS Respondents\",\n  xlab = \"Respondent age in years\"\n)\n\n\n\n\n\n\n\n\nThis isn’t quite a normal distribution, but it’s not exactly skewed in either direction either. And there are no apparent outliers.\nWhat we have here is a bimodal distribution.\nThis is what happens when we have multiple peaks in a distribution—two, in this case. We have seen several different distributions so far, but they all had only one peak in the distribution. They were differentiated on the basis of that single peak’s location in the distribution. In the case of a multimodal distribution, we want to report on any notable peak.\nThis may be a peculiarity of the 2022 survey wave, because we wouldn’t necessarily expect this, but it looks like there are two distinct central tendencies for age. In this case, we want to capture the two distinct peaks, meaning we need to calculate two modes.\nThankfully, there’s a convenient function for this, but we need to install a new package, so let’s install and load in multimode.\n\ninstall.packages(\"multimode\")\nlibrary(multimode)\n\nThen, we just need to provide a few arguments to the locmodes() function.\nFirst, we give the variable for which we want to calculate multiple modes.\nThen we give an input for mod0, which asks us how many modes the function should be looking for. This should be based on visual inspection of the data. We have two peaks in our distribution, so we will enter ‘2’ for this input.\nLastly, we will set display to ‘TRUE’, which will show us a plot of the estimated modes superimposed onto the frequency distribution of the variable. This will let us evaluate whether the modes estimated by the function are plausible. They should match up with the visible peaks in the frequency distribution.\n\nlocmodes(\n  our_gss$age,\n  mod0 = 2,\n  display = TRUE\n)\n\n\n\n\n\n\n\n\n\nEstimated location\nModes:  32.4301  66.25356 \nAntimode: 50.89506 \n\nEstimated value of the density\nModes: 0.01938966  0.01730832 \nAntimode: 0.01444287 \n\nCritical bandwidth: 2.543659\n\n\nWe only really need to pay attention to the modes that it gives us (32.43 and 66.25), but it looks like this function spotted these values pretty effectively.\nFor a bimodal distribution like this, we can report both modes for the measure of central tendency. I don’t expect we will run into too much of this, but go ahead and deal with it like this in the event that you do.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#dispersion",
    "href": "univariate_numeric.html#dispersion",
    "title": "5  Univariate: Numeric",
    "section": "5.3 Dispersion",
    "text": "5.3 Dispersion\nNow, let’s talk about dispersion. This is the last major element of a univariate analysis of numeric data. Actually calculating this information is quite simple in R, so we will practice with some of our GSS variables and then talk about putting all of this information together.\n\n5.3.1 Range\nThis is perhaps the simplest measure of dispersion and comprises the minimum and maximum values of the distribution.\nR has a built in range() function, so we can simply provide one of our variables as the input. We’ll work with realrinc again. As we have done before, we will also need to provide na.rm = TRUE, as this variable column includes NAs and will confuse R otherwise.\n\nrange(our_gss$realrinc, na.rm = TRUE)\n\n[1]    204.5 141848.3\n\n\nAt this point, I’ll also offer a reminder about the summary() function, which we can use to see a variety of information about the dispersion (in an addition to the central tendency).\n\nsummary(our_gss$realrinc)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n   204.5   8691.2  18405.0  27835.3  33742.5 141848.3     1554 \n\n\nFor a numeric variable, summary() will show us the minimum & maximum, as well as the mean & median, and the 1st & 3rd quartiles.\nWe can actually think of most of these elements in terms of percentiles.\n\nThe minimum value is the 0th percentile of the distribution\nThe 1st quartile is the 25th percentile.\nThe median is the 50th percentile\nThe 3rd quartile is the 75th percentile\nAnd the maximum is the 100th percentile.\n\nsummary() is great for quickly assessing some of these descriptive statistics, but it’s a little less convenient for exporting this information into something like a table for our own univariate analysis.\nSo, I’ll also highlight the min() and max() functions, which will come in hand for us shortly. They work simply enough—we just need to provide our variable column, and they will output the minimum and maximum, respectively.\n\nmin(our_gss$realrinc, na.rm = TRUE)\n\n[1] 204.5\n\nmax(our_gss$realrinc, na.rm = TRUE)\n\n[1] 141848.3\n\n\n\n\n5.3.2 Standard Deviation\nThis is one of the more commonly reported dispersion metrics for numeric data. The standard deviation is a measure of how much the average value varies from the mean. In plainer terms, it’s a measure that tells us how spread out the distribution is.\n\n\n\n\n\n\n\n\n\nWe can grab the standard deviation quite easily with the sd() function\n\nsd(our_gss$realrinc, na.rm = TRUE)\n\n[1] 31962.34\n\n\nNow, let’s talk about putting all this together in our reporting of a univariate analysis for numeric data.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#putting-it-all-together",
    "href": "univariate_numeric.html#putting-it-all-together",
    "title": "5  Univariate: Numeric",
    "section": "5.4 Putting It All Together",
    "text": "5.4 Putting It All Together\nNow that we have worked through how to calculate all of these key statistics with R, let’s revisit some of the first tidyverse functions we learned about back on the first day.\nWe can use a combination of select() and summarize to quickly compile several bits of important information, such as the range, the central tendency, and the standard deviation.\nLet’s do this for realrinc.\n\nour_gss |&gt;\n  select(realrinc) |&gt;\n  summarize(\n    \"Minimum\" = min(realrinc, na.rm = TRUE),\n    \"Median\" = median(realrinc, na.rm = TRUE),\n    \"Maximum\" = max(realrinc, na.rm = TRUE),\n    \"SD\" = sd(realrinc, na.rm = TRUE)\n  )\n\n  Minimum Median  Maximum       SD\n1   204.5  18405 141848.3 31962.34\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might notice that there’s an attribute for each column that looks like &lt;dbl&gt;. This is short for ‘double’, which is itself shorthand for ‘double-precision floating-point format’. This is in reference to the underlying data type that R uses to store continuous numeric values. You really don’t need to know anything more about double-precision floating point than that for our purposes, but I mention it here because you might run into  or ‘double’ in R. When you do, just think ‘numeric data’.\n\n\nIf you have a numeric variable where the mean is more appropriate, you can just swap median() with mean() in the code template above. Be sure to also change the column name as well.\nAnd that should do us! For any univariate analysis you report in this course, you will just need to produce\n\na histogram displaying the variable’s frequency distribution\na table like the one above (with the appropriate measure of central tendency).",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "bivariate_categorical.html",
    "href": "bivariate_categorical.html",
    "title": "6  Bivariate: Categorical",
    "section": "",
    "text": "6.1 Contingency Tables\nNow that we know how to peek under the hood of our variables individually, it’s time to start looking at them in combination.\nWe will start with bivariate analysis, or the association between two variables.\nLet’s begin with the procedure for categorical variables. We will start with the basic display of two categorical variables and then get some practice with the chi-square test, which is a common statistical test that assumes both variables are categorical.\nWe will learn to recognize associations in the contingency table, and then we’ll use the chi-square test to estimate the likelihood that the relationship we find is genuine or merely an artifact of random chance.\nAlso known as cross-tabulations—or crosstabs for short—contingency tables are quite similar to frequency tables. Rather than showing the distribution of a single variable, they show us the distribution of our dependent variable across the levels of our independent variable. In technical terms, we can call this the conjoint distribution of two variables.\nLet’s dig in with an example. We’ll keep working with our GSS subset, so let’s go ahead and set up our workspace by loading in our packages & data. We will need both tidyverse and janitor for this tutorial.\nlibrary(tidyverse)\nlibrary(janitor)\nload(\"our_gss.rda\")",
    "crumbs": [
      "Day 3: Bivariate Analysis with the GSS",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bivariate: Categorical</span>"
    ]
  },
  {
    "objectID": "bivariate_categorical.html#contingency-tables",
    "href": "bivariate_categorical.html#contingency-tables",
    "title": "6  Bivariate: Categorical",
    "section": "",
    "text": "6.1.1 Creating our table\nWe’ll set up a simple hypothesis to illustrate. Let’s say that, based on theory, we predict that a person’s likelihood of supporting the death penalty will vary according to their political affiliation.\nNow, there’s enough literature on capital punishment and political affiliation that we would probably be justified in making a more specific hypothesis about the predicted direction of the association, but let’s just say we simply predict that rates of approval for the death penalty will vary across categories of Democrat and Republican.\nHypotheses are where we turn our research questions into very specific expectations about the data. If certain theoretical expectations are true, then we should find evidence in the data for the relationships suggested by theory. This is where we predict what the distribution of variables should look like in the event that these theoretical expectations are true.\nAnd remember that our hypotheses are generally going to be tested against a null hypothesis, which assumes that whatever relationship you predict is not the case.\nNull hypothesis: \\[H_{0} = There \\ will \\ be \\ no \\ difference \\ in \\ rates \\ of \\ death \\ penalty \\ approval \\ among \\ Democrats \\ and \\ Republicans \\]\nAlternative hypothesis: \\[H_{1} = Democrats \\ and \\ Republicans \\ will \\ have \\ different \\ rates \\ of \\ death \\ penalty \\ approval\\]\nIn sum, the alternative hypothesis is that the proportion of approval for the death penalty will be different across political parties. The null hypothesis is that there will be no such difference. Though the peculiarities of many statistical tests differ, this general idea is true for a great number of statistical procedures. You will estimate the distribution of values that would be the case under the null hypothesis and then evaluate the statistical significance of the extent to which your data deviates from a null distribution.\nWe’ll work with the dem_rep variable that we created when learning about variable re-coding, as well as the cappun variable.\nThankfully, we can use a familiar function in tabyl(), which we used to create our frequency table. We can simply add a second variable to produce a crosstab, though we will need to specify a couple of things so that the variables are properly formatted in terms of rows and columns.\nIf you did not save our_gss.rda after creating dem_rep in a previous exercise, you can go ahead and recreate it with the following code:\n\nour_gss &lt;- our_gss |&gt;\n  mutate(\n    partyid_recoded=fct_collapse(partyid, \n\"Democrat\" = c(\"strong democrat\", \"not very strong democrat\"),\n\"Republican\" = c(\"strong republican\",\"not very strong republican\"),\n\"Independent\" = c(\"independent, close to democrat\", \"independent (neither, no response)\", \"independent, close to republican\"),\n\"Other Party\" = c(\"other party\")\n))\n\nour_gss &lt;- our_gss |&gt;\n  mutate(dem_rep = fct_recode(\n    partyid_recoded, \n    NULL=\"Other Party\", \n    NULL=\"Independent\"))\n\nAnd then we can make a simple cross-tab like so:\n\nour_gss |&gt;\n  drop_na(dem_rep, cappun) |&gt;\n  tabyl(\n    var1 = cappun,\n    var2 = dem_rep\n  )\n\n cappun Democrat Republican\n  favor      426        633\n oppose      569        121\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhenever you make a two-way crosstab like this, make sure that you always give your dependent variable as var1 and your independent variable as var2. This will make sure your dependent variable appears as rows and your independent variable as columns, which makes everything much easier to parse for our purposes.\n\n\nFor each cell, the count of that particular category intersection is displayed. So, for example, row 1 tells us that 426 Democrats favor the death penalty compared to 633 Republicans who favor the death penalty. In the second row, we can see that 569 Democrats oppose the death penalty compared to 121 Republicans who oppose the death penalty.\nNow, let’s clean it up a little bit with the adorn_() functions like we did before.\nFirst, let’s add a totals row at the bottom, which will tell us how many respondents are in each category of the independent variable. Note that we always need to add adorn_totals() before all other adorn_() functions. It requires the raw counts as input, and these will be transformed by most other adorn_() functions, so we will get an error if we try to run adorn_totals() afterwards.\nWe’ll give one input to adorn_totals(), which will be where = \"row\", to indicate that we want the totals to appear as a row.\n\nour_gss |&gt;\n  drop_na(cappun, dem_rep) |&gt;\n  tabyl(\n    var1 = cappun,\n    var2 = dem_rep) |&gt;\n  adorn_totals(where = \"row\")\n\n cappun Democrat Republican\n  favor      426        633\n oppose      569        121\n  Total      995        754\n\n\nNow we can see that there are 995 Democrats in our sample and 754 Republicans.\nNow, let’s convert counts to proportions. Somewhat misleadingly, this is done with the adorn_percentages() function, and then we can add the adorn_pct_formatting() function to turn them into percentages proper.\nWe will also add the input \"col\" to adorn_percentages(), which will give us percentages that sum by column. This will make interpretation a little more convenient for us later on.\n\nour_gss |&gt;\n  drop_na(cappun, dem_rep) |&gt;\n  tabyl(\n    var1 = cappun,\n    var2 = dem_rep) |&gt;\n  adorn_totals(where = \"row\") |&gt;\n  adorn_percentages(\"col\") |&gt;\n  adorn_pct_formatting()\n\n cappun Democrat Republican\n  favor    42.8%      84.0%\n oppose    57.2%      16.0%\n  Total   100.0%     100.0%\n\n\nWhile percentages help give important context, it’s important that we still complement the percentages with the raw numbers. Thankfully, we can add those back in alongside the percentages with adorn_ns()\n\nour_gss |&gt;\n  drop_na(cappun, dem_rep) |&gt;\n  tabyl(\n    var1 = cappun,\n    var2 = dem_rep) |&gt;\n  adorn_totals(where = \"row\") |&gt;\n  adorn_percentages(\"col\") |&gt;\n  adorn_pct_formatting() |&gt;\n  adorn_ns()\n\n cappun     Democrat   Republican\n  favor  42.8% (426)  84.0% (633)\n oppose  57.2% (569)  16.0% (121)\n  Total 100.0% (995) 100.0% (754)\n\n\nGreat. For our final adornment, let’s add some better labels for the variables. We’ll use adorn_title() for this, and we’ll supply placement = top to let the function know we want the labels on top of each variable column. Then we can provide custom label names with row_name = and col_name =.\n\nour_gss |&gt;\n  drop_na(cappun, dem_rep) |&gt;\n  tabyl(\n    var1 = cappun,\n    var2 = dem_rep) |&gt;\n  adorn_totals(where = \"row\") |&gt;\n  adorn_percentages(\"col\") |&gt;\n  adorn_pct_formatting() |&gt;\n  adorn_ns() |&gt;\n  adorn_title(\n    placement = \"top\",\n    row_name = \"Death Penalty Attitude\",\n    col_name = \"Political Party\") \n\n                        Political Party             \n Death Penalty Attitude        Democrat   Republican\n                  favor     42.8% (426)  84.0% (633)\n                 oppose     57.2% (569)  16.0% (121)\n                  Total    100.0% (995) 100.0% (754)\n\n\nGreat! Like our other visualizations, we’ll clean this up a bit more later on, but this will do for now.\n\n\n6.1.2 Reading our table\nNow that have our two-way crosstab, let’s go ahead and think about what this means for our hypothesis.\n\n\n\n\n\n\nPolitical Party\n\n\n\n\n\nDeath Penalty Attitude\nDemocrat\nRepublican\n\n\nfavor\n42.8% (426)\n84.0% (633)\n\n\noppose\n57.2% (569)\n16.0% (121)\n\n\nTotal\n100.0% (995)\n100.0% (754)\n\n\n\n\n\nBecause our dependent variable makes up the rows, we can simply read across each row to get a sense of the patterns.\nReading across the ‘favor’ row, we can see that 42.8% of all Democrats in our sample favor the death penalty and 84% of all Republicans favor the death penalty. Reading across the ‘oppose’ row, we can see that 57.2% of all Democrats oppose the death penalty compared to 16% of all Republicans.\nIn sum, a much greater proportion of Republicans favor the death penalty compared to Democrats (84% vs. 42.8%). Conversely, a much greater proportion of Democrats oppose the death penalty compared to Republicans (57.2% vs. 16%). Thus, based on our sample, it seems like being a Democrat is associated with a greater likelihood of opposing the death penalty.\nBut how can we feel good about whether this association is likely to be reflective of something real, and not just an artifact of our particular sample or simply random chance? For this, we need to employ a statistical test.\nSpecifically, we will leverage the Chi-square test. This is used when both of the variables we’re interested in are categorical.",
    "crumbs": [
      "Day 3: Bivariate Analysis with the GSS",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bivariate: Categorical</span>"
    ]
  },
  {
    "objectID": "bivariate_categorical.html#chi-square-test",
    "href": "bivariate_categorical.html#chi-square-test",
    "title": "6  Bivariate: Categorical",
    "section": "6.2 Chi-square Test",
    "text": "6.2 Chi-square Test\nLet’s have a little refresher on what goes into the Chi-square test and the corresponding Chi-square value, which serves as a key indicator of the statistical significance for our findings.\n\n6.2.1 How the test works\nGiven the sample size of our data, the number of variables we are comparing, and the number of categories within those variables, we can calculate an expected distribution of those variables for the case where the variables are completely unrelated. This would give us a contingency table where our sample size is distributed across cells in a random fashion.\nThe Chi-square test involves estimating the extent to which the cell counts we actually observe in our contingency table are significantly different from the cell counts we expect if the variables are unrelated.\n\\[\\chi^{2} = {\\sum}{\\frac{(o - e)^{2}}{e}}\\]\nThis is the formal equation for the Chi-square test (slightly modified for interpretive simplicity). You will almost never need to calculate this by hand, but I will explain what the equation means in order to help us get a conceptual understanding of the test.\nOn the left-hand side of the equation, we have \\(\\chi^{2}\\), which is the symbol for Chi-square—Chi being in reference to the letter of the Greek alphabet.\nOn the right-hand side, we have \\({\\frac{(o - e)^{2}}{e}}\\), which is the heart of the calculation\nThe \\({\\sum}\\) operator tells us that we will be taking a sum. So, we need to calculate \\({\\frac{(o - e)^{2}}{e}}\\) for each cell in the contingency table, and then add all these values together.\n\no: observed valued\ne: expected value\n\nFor each cell, we take the count value that we find in our contingency table, subtract the count we would expect if there’s no relationship between the variables, square that difference, and then divide by the expected value.\nSo, the observed value is easy enough—it’s just the value we actually see in our contingency table.\nThe observed value of the cell in the first row of the first column is 426.\nNow, this raises the question of how exactly we calculate the expected values.\n\\[Expected Count = {\\frac{(RowTotal) \\cdot (ColumnTotal)}{GrandTotal}}\\]\nIn other words, for each cell, you take the sum of the values in that row, then take the sum of the values in that column, and then multiply these together. Then, you divide that value by the grand total.\nLet’s work through an example from our table.\n\n\n\n\n\n\nPolitical Party\n\n\n\n\n\n\nDeath Penalty Attitude\nDemocrat\nRepublican\nTotal\n\n\nfavor\n42.8% (426)\n84.0% (633)\n60.5% (1,059)\n\n\noppose\n57.2% (569)\n16.0% (121)\n39.5% (690)\n\n\nTotal\n100.0% (995)\n100.0% (754)\n100.0% (1,749)\n\n\n\n\n\nThe expected count for Democrats who favor the death penalty would be: \\[\\frac{(426+633)\\cdot(426+569)}{1749}\\]\n\n\n[1] 602.4614\n\n\nYou can get a sense right here that we may indeed be picking up on something in our cross-tab. Democrats appear to oppose the death penalty more so than Republicans, based on the raw frequencies. Now we can see that our observed value for Democrats who favor the death penalty (426) is quite a bit lower than what we would expect if that variable were unrelated to political affiliation (602.46). This is promising, but there’s a couple more things we need to do.\nThis calculation for the expected counts is actually downstream of a principle from probability theory that describes the joint probability of independent events: \\[P(A\\&B) = P(A)\\cdot P(B)\\]\nWhat this means is that, if two variables are independent, then the probability of any combination of these variables’ occurring simultaneously is equivalent to the product of each value’s individual probability.\nLet’s put this in context of our data:\n\\[P(Democrat\\&Favor) = P(Democrat)\\cdot P(Favor)\\]\nIf the two variables are unrelated, we expect that the probability of both being a democrat and favoring the death penalty is equal to the probability of being a democrat multiplied by the probability of favoring the death penalty.\n\n# Total of our sample\ngrand_total &lt;- (426 + 633 + 569 + 121)\n\n# Number of democrats\nnum_dems &lt;- (426 + 569)\n\n# Number of people who favor the death penalty\nnum_favs &lt;- (426 + 633)\n\n# Probability of being a democrat\ndem_prob &lt;- num_dems/grand_total \n\n# Probability of favoring the death penalty\nfav_prob &lt;- num_favs/grand_total\n\n# Expected probability of Favor/Democrat\nexp_dem_fav &lt;- dem_prob * fav_prob\n\n# Expected count of Favor/Democrat\nexp_dem_fav * grand_total\n\n[1] 602.4614\n\n\nThere it is worked out in a little more detail. The \\(Expected Count = {\\frac{(RowTotal) \\cdot (ColumnTotal)}{GrandTotal}}\\) equation is written to output the counts directly, but hopefully that helps clarify what exactly the concept of ‘expected’ counts is getting at.\nNow, all we’d need to do is plug these expected values into this formula: \\[\\chi^{2} = {\\sum}{\\frac{(o - e)^{2}}{e}}\\]\nI’ll calculate the Chi-square value of the cell for Democrats who favor the death penalty as an example.\n\nexp_val &lt;- 602.4614\n\nobs_val &lt;- 426\n\nchi_numerator &lt;- (obs_val - exp_val)^2\n\nchi_numerator/exp_val\n\n[1] 51.68568\n\n\nThen we do this for the three other cells in our table, add these 4 values together, and voila—we have our Chi-square statistic.\nI won’t go through the trouble here of manually calculating all the others, but know that our value is 302.18.\nThen you can use a look-up table for the corresponding p-values of Chi-squared statistics. You will need to know your degrees of freedom, which, for the Chi-square test, is:\n\\[df = (RowNumbers-1)\\cdot (ColumnNumbers-1)\\]\nSo, for our 2x2 contingency table, this would be:\n\ntotal_rows &lt;- 2\ntotal_columns &lt;- 2\n\n# Degrees of freedom\n(total_rows - 1) * (total_columns - 1)\n\n[1] 1\n\n\nHere’s an example of a Chi-square look-up table from the University of Sussex.\nAll you have to do is take your Chi-squared value & your degrees of freedom and then locate the column corresponding to the alpha level you are hoping for. In most cases, this will be an alpha of 0.05, which is standard across the social sciences. We want a p-value below this alpha level for statistical significance, which means that we want less than a 5% chance of a false positive (i.e. returning a statistically significant result when there actually is no association).\nWith an alpha of 0.05 and 1 degree of freedom, we need our Chi-squared value to be greater than 3.84 in order to claim statistical significance. In our case, our value of 302.18 is well above this, so we do indeed have a statistically significant finding.\nFinally, let’s learn a much simpler way to do all of this.\n\n\n6.2.2 The chisq.test() function\nI went about working through that example to make sure we all have a good idea of what’s going on underneath the hood of the Chi-squared test.\nThankfully for all of us, you will rarely if ever need to actually go about manually calculating all of this stuff. As usual, R has a nifty function that we can run directly on our contingency table—the chisq.test() function—which will automatically run all of these calculations and output a corresponding p-value. We just need to provide the function a contingency table as input, and the output of tabyl() will work for us here. I’m simply going to borrow some code from above and then add one more pipe operator to pass the table into the chisq.test() function.\n\n\n\n\n\n\nCaution\n\n\n\nNote here that I am running this on a bare-bones table without any of the adorn_() functions that we have used to dress up our tabyl() output. Because those functions add a bunch more information beyond the simple frequency counts, this will throw off chisq.test(), which will not know what to do with the extra info (e.g. the percents & totals row). Make sure you leave out any adornments when running the Chi-square test.\n\n\n\nour_gss |&gt;\n  drop_na(cappun, dem_rep) |&gt;\n  tabyl(\n    var1 = cappun,\n    var2 = dem_rep) |&gt;\n  chisq.test()\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tabyl(drop_na(our_gss, cappun, dem_rep), var1 = cappun, var2 = dem_rep)\nX-squared = 302.18, df = 1, p-value &lt; 2.2e-16\n\n\nThis gives us our Chi-squared value (302.18), our degrees of freedom (1), and a corresponding p-value. Note that this is in scientific notation, but it’s an extraordinarily small number—far below our 0.05 alpha threshold. I’ll display it in full so you can see for yourself\n\nformat(2.2e-16, scientific = FALSE)\n\n[1] \"0.00000000000000022\"\n\n\nSo, 2.2e-16 indicates the number 22 following 16 zeros (including one before the decimal place).\nIn the case that your p-value is this small, it’s common convention to simply report that it is &lt; 0.001.\nOne other nifty thing we can do with chisq.test() is store it as an object.\n\nour_chisq &lt;- our_gss |&gt;\n  drop_na(cappun, dem_rep) |&gt;\n  tabyl(\n    var1 = cappun,\n    var2 = dem_rep) |&gt;\n  chisq.test()\n\nIf we look inside this object, we’ll see a bunch of the information used to calculate these various statistics. Notably, it has the table of both observed and expected counts, so we can easily compare the two.\n\nour_chisq$expected\n\n cappun Democrat Republican\n  favor 602.4614   456.5386\n oppose 392.5386   297.4614\n\nour_chisq$observed\n\n cappun Democrat Republican\n  favor      426        633\n oppose      569        121\n\n\nAs you can see, far fewer Democrats favor the death penalty than would be expected in the case that this is independent from one’s political affiliation. Similarly, many more Republicans favor the death penalty than we would expect in the conjoint distribution of two independent variables. Now that we have assessed this difference with the Chi-square test, we can feel confident that the association originally suggested by our reading of the contingency tables—that Democrats are more likely to oppose the death penalty than Republicans—is statistically significant.\nSo, how can we interpret this in terms of our hypotheses?\nNull hypothesis: \\[H_{0} = There \\ will \\ be \\ no \\ difference \\ in \\ rates \\ of \\ death \\ penalty \\ approval \\ among \\ Democrats \\ and \\ Republicans \\]\nAlternative hypothesis: \\[H_{1} = Democrats \\ and \\ Republicans \\ will \\ have \\ different \\ rates \\ of \\ death \\ penalty \\ approval\\]\nWith a Chi-squared value of 302.18 at 1 degree of freedom, we can reject the null hypotheses that these two variables are unrelated. Instead, we find support for our hypothesis that political affiliation is associated with death-penalty attitudes. Our Chi-square value is associated with a p-value of &lt;0.001, which is statistically significant at an alpha threshold of 0.05. This suggests that it is exceedingly unlikely the relationship we observe in our cross-tabulation is the result of random chance or peculiarities of our sample.\nThe conjoint distribution of cappun and dem_rep shows us that Democrats are more likely to oppose the death penalty than Republicans. 57.2% of all Democrats oppose the death penalty compared to only 16% of all Republicans. Conversely, Democrats are less likely to favor the death penalty, as only 42.8% of all Democrats support the death penalty compared to 84% of all Republicans.",
    "crumbs": [
      "Day 3: Bivariate Analysis with the GSS",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bivariate: Categorical</span>"
    ]
  },
  {
    "objectID": "viz_introduction.html",
    "href": "viz_introduction.html",
    "title": "7  Introduction",
    "section": "",
    "text": "7.1 Visualization and Analytical Thinking\nUp to this point, we have worked with pretty simple visualizations, just so we can quickly glean important information about our statistical models without spending too much time focusing on the aesthetics of the visuals.\nIn this lab, we will learn a bit more about R’s graphical capability—especially through tidyverse’s ggplot—which provides us with incredible customizability. We will learn how to fine-tune some of the visuals we have already worked with, and we will preview some other common visual styles that can manage with ggplot.\nBefore we start working with some of these new visual tools, I want to take an opportunity to stress the importance of visualization more generally. It’s easy to see the process of presenting visuals as something somewhat superficial, but visualization can be critical for defining the kind of questions we can ask about our data.\nFor now, I’m going to obscure the code I’m using for this document. We will learn more about the kind of commands I used to generate the following figures, but I don’t want anyone to get bogged down initially. I’ll use these visuals to help impart an important lesson about data visualization’s in the research process.",
    "crumbs": [
      "Day 4: Making Better Visualizations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "viz_introduction.html#thirteen-data-sets",
    "href": "viz_introduction.html#thirteen-data-sets",
    "title": "7  Introduction",
    "section": "7.2 Thirteen Data Sets",
    "text": "7.2 Thirteen Data Sets\nLet’s take a look at a collection of thirteen different data sets. Each data set has 142 observations with 2 columns, labeled x & y.\nI’ll use some tidyverse commands to get some summary statistics for each of the data sets, including the mean of both variables and their standard deviations. Let’s see what seems to distinguish some of these data sets from one another.\n\n\n\n\n\nMean (x)\nMean (y)\nSD (x)\nSD (y)\n\n\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n\n\n\nWell, there’s not much we can say here. All the summary statistics are identical. Why don’t we try modeling a linear relationship between the x and y variables. Maybe looking at the correlations will tell us something. I’ll display the linear regression lines for each data set below.\n\n\n\n\n\n\n\n\n\nOkay. This is not revealing much either. All the lines seem to have the same slope, which shows a (slight) negative relationship where y decreases as x increases. The correlations aren’t revealing any notable distinctions.\nBut wait. One thing we can see here is that, while the correlations appear to be about the same, there are some differences in the ranges of values. Note that the regression lines don’t extend across the same range of x-axis values in each data set. Maybe there is something here after all.\nLet’s just go ahead and plot the actual data.\n\n\n\n\n\n\n\n\n\nNow there’s some distinction!\nThis is a tongue in cheek data set known as the ‘datasaurus dozen’. It’s often used in intro statistical classes to help illustrate the importance of visualization. It’s inspired by another conceptually similar data set known as ‘Anscombe’s quartet’ which likewise stresses the role of plotting data in producing well informed analyses.",
    "crumbs": [
      "Day 4: Making Better Visualizations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "viz_introduction.html#in-sum",
    "href": "viz_introduction.html#in-sum",
    "title": "7  Introduction",
    "section": "7.3 In Sum",
    "text": "7.3 In Sum\nSo, take this as a showcase of the importance of visualizing your data. This isn’t to discount summary statistics and other numeric description of data—those are still invaluable for us.\nRather, cases like Datasaurus or Anscombe’s quartet highlight the necessity of understanding the shape of your data. This will determine the kind of questions you can ask with the data, as well as the kind of statistical tools you need to describe it.\nFor example, in the case we just examined, those x and y variables do not have any kind of clear linear relationship. In that case, tools like standard OLS regression that assume linearity are not appropriate. Any relationship between the variables could only be explored through other statistical means.\nSo, making our figures and tables look aesthetically pleasing is indeed valuable in its own right, but don’t underestimate the utility of good visualization for the analytic process itself.",
    "crumbs": [
      "Day 4: Making Better Visualizations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data_viz.html",
    "href": "data_viz.html",
    "title": "8  Plotting and Counting",
    "section": "",
    "text": "8.1 Setting up our workspace\nIn this last portion of our in-class lab sessions on quantitative analysis, we’ll learn how to make some familiar visuals in new and exciting ways. For the most part, this will involve learning the graphing package of tidyverse, commonly known as ggplot, as well as gtsummary, a helpful package for producing tables. There are a couple of things to get used to with ggplot, but once you get a handle on its syntax, it will offer simple and extensive customizability that translates across a wide expanse of use cases. Even some of the graphics that we’ve seen in empirical papers for class have been produced using ggplot!\nBecause ggplot comes with tidyverse, we just need to load that in like we’ve often done. We’re also going to need janitor, and, as usual, we’ll also pull in our GSS data.\nlibrary(tidyverse)\nlibrary(janitor)\nload(\"our_gss.rda\")",
    "crumbs": [
      "Day 4: Making Better Visualizations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting and Counting</span>"
    ]
  },
  {
    "objectID": "data_viz.html#figures",
    "href": "data_viz.html#figures",
    "title": "8  Plotting and Counting",
    "section": "8.2 Figures",
    "text": "8.2 Figures\nSpecific to categorical variables, we learned about one key plot type: barplots. We’ll start our foray into ggplot by using it to recreate some figures in this style.\n\n8.2.1 Barplots & Basics of ggplot\nThe structure of figures made with ggplot is quite regular. The name ‘ggplot’ is actually a reference to the broader idea of a grammar of graphics, and though there are some specifics across the various plot types, we will offer ggplot mostly the same kind of information regardless. It’s certainly not necessary, but if you are curious, you can read some more about the grammar of graphics here\nFirst, ggplot expects that we will give it a dataframe. This is good because it also means that ggplot plays well with the pipe operator.\nNext, ggplot needs input for your aesthetic mappings, shortened to aes() within the function. This might sound a little hoighty toighty, but it simply refers to the specification of some key parameters for our figure. We want our data to be visually mapped in a certain sort of way, and this is where we set that up.\nThe simplest aesthetic mappings we can provide are the variables we want associated with the x & y axes. Let’s try with sex as an example. Because we are making a barplot, our y-axis will reflect the frequency of the values of the y-axis. We’ll see that ggplot can actually automatically count these for us, so we only need to specify the x-axis variable for a barplot in ggplot.\n\nour_gss |&gt;\n  drop_na(sex) |&gt;\n  ggplot(\n    aes(x = sex))\n\n\n\n\n\n\n\n\nNow, you might be asking yourself: what’s the deal? We only see the labels of the x-axis there.\nThe syntax of ggplot is often explained by making an analogy with paintings. You start by setting up a basic canvas with your aes() specifications, and then you add on different layers reflecting the shape of your data and various customizations to its appearance.\nThe first layer we typically add is a geometric object specification—shorted to geom in ggplot language. You can think of this as the place where you tell ggplot what kind of figure you want. There are a great many geoms, and you can read more about the full extent of them here.\nIn our case, we want a barplot, so we can add geom_bar() as an additional layer. We can simply add each new layer after a ‘+’ operator\n\nour_gss |&gt;\n  drop_na(sex) |&gt;\n  ggplot(\n    aes(x = sex)) +\n      geom_bar()\n\n\n\n\n\n\n\n\nVoila! There’s a pretty decent barplot. There are other kinds of statistical summaries we can take advantage of in ggplot, but the default behavior of geom_bar() will calculate the summary counts of the values of the x variable.\nNow we tweak a couple things to make these a bit nicer and illustrate some of the perks of ggplot’s customizability.\nLet’s add a little color. We can indicate this back up at the aes() layer. This works by adding a ‘fill’ aesthetic, and then we specify a variable. What we do there is tell R that we want the fill-color of the bars to vary on the basis of that variable. So, each bar will be filled with a different color for each response category of sex, in our case.\n\nour_gss |&gt;\n  drop_na(sex) |&gt;\n  ggplot(\n    aes(x = sex, fill = sex)) +\n      geom_bar()\n\n\n\n\n\n\n\n\nNeat! This looks much better, and we’ll play around with this more later, but now let’s see how to add some axis & legend labels along with a title. The legend label actually works out fine as it is with sex, but the shorthand variable names often do not translate well as legend titles (imagine if it were ‘realrinc’, for example). So, I’ll show you how to customize it just in case.\nFor the axis labels and title, we can add the labs layer\n\nour_gss |&gt;\n  drop_na(sex) |&gt;\n  ggplot(\n    aes(x = sex, fill = sex)) +\n      geom_bar() +\n  labs(\n    title = \"Sex distribution of 2022 GSS Respondents\",\n    x = \"Sex\", \n    y = \"Frequency\") \n\n\n\n\n\n\n\n\nThen we can add the guides layer to customize the legend title for the fill aesthetic.\n\nour_gss |&gt;\n  drop_na(sex) |&gt;\n  ggplot(\n    aes(x = sex, fill = sex)) +\n  geom_bar() +\n  labs(\n    title = \"Sex distribution of 2022 GSS Respondents\",\n    x = \"Sex\", \n    y = \"Frequency\") +\n  guides(fill = guide_legend(title=\"Respondent's sex\"))\n\n\n\n\n\n\n\n\nLastly on this point, I’ll also show you how to get rid of the legend entirely. You’ll often want to customize the legend in the fashion I laid out above, but we actually don’t really need it in this case. It’s redundant with the x-axis labels. So, you can always get rid of a legend by adding the guides() layer and simply setting fill to ‘none’.\n\nour_gss |&gt;\n  drop_na(sex) |&gt;\n  ggplot(\n    aes(x = sex, fill = sex)) +\n  geom_bar() +\n  labs(\n    title = \"Sex distribution of 2022 GSS Respondents\",\n    x = \"Sex\", \n    y = \"Frequency\") +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n\n8.2.2 Bivariate Barplots\nThe fill aesthetic actually allows us to quite easily create a bivariate version of the barplot.\nLet’s consider our bivariate analysis from a previous session: cappun and dem_rep.\nIf you need to recreate dem_rep from partyid, go ahead and run the following code:\n\nour_gss &lt;- our_gss |&gt;\n  mutate(\n    partyid_recoded=fct_collapse(partyid, \n\"Democrat\" = c(\"strong democrat\", \"not very strong democrat\"),\n\"Republican\" = c(\"strong republican\",\"not very strong republican\"),\n\"Independent\" = c(\"independent, close to democrat\", \"independent (neither, no response)\", \"independent, close to republican\"),\n\"Other Party\" = c(\"other party\")\n)) |&gt;\n  mutate(dem_rep = fct_recode(\n    partyid_recoded, \n    NULL=\"Other Party\", \n    NULL=\"Independent\"))\n\nAs a reminder, here is the cross-tab for that conjoint distribution:\n\n\n\n\n\n\nPolitical Party\n\n\n\n\n\nDeath Penalty Attitude\nDemocrat\nRepublican\n\n\nfavor\n42.8% (426)\n84.0% (633)\n\n\noppose\n57.2% (569)\n16.0% (121)\n\n\nTotal\n100.0% (995)\n100.0% (754)\n\n\n\n\n\nNow, we’ll use our same barplot code from above, but this time we will set our x variable to cappun and swap out the fill aesthetic to dem_rep. This means we want the fill-color of the bars to vary on the basis of the categories in dem_rep. I’ll also add cappun and dem_rep to drop_na() and update the labels & title.\n\nour_gss |&gt;\n  drop_na(sex, cappun, dem_rep) |&gt;\n  ggplot(\n    aes(x = cappun, fill = dem_rep)) +\n  geom_bar() +\n  labs(\n    title = \"Death Penalty Attitudes by Political Party\",\n    x = \"Death penalty attitude\", \n    y = \"Frequency\") +\n  guides(fill = guide_legend(title=\"Political party\"))\n\n\n\n\n\n\n\n\nGood stuff! This is a stacked barplot and is a common way to display multivariate categorical data. I personally find it a little easier to intrepret when the columns are side-by-side rather than stacked on top of each other, so I’ll show you how to do that too. This just requires setting a position mapping within geom_bar(). We set the position to ‘dodge’, which might sound a little counter-intuitive, which is because it actually has a more general purpose. The idea is that you want overlapping objects to ‘dodge’ one another, which will cause them to appear side by side rather than on top of one another. So, by doing so, we will un-nest our stacked bar charts and have each bar appear adjacent to one another.\n\nour_gss |&gt;\n  drop_na(sex, cappun, dem_rep) |&gt;\n  ggplot(\n    aes(x = cappun, fill = dem_rep)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Death Penalty Attitudes by Political Party\",\n    x = \"Death penalty attitude\", \n    y = \"Frequency\") +\n  guides(fill = guide_legend(title=\"Political party\"))\n\n\n\n\n\n\n\n\nNot too shabby. This is a great way to complement the presentation of a bivariate analysis, and it’s also great for quickly getting a sense of the relationship among variables of interest.",
    "crumbs": [
      "Day 4: Making Better Visualizations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting and Counting</span>"
    ]
  },
  {
    "objectID": "data_viz.html#tables",
    "href": "data_viz.html#tables",
    "title": "8  Plotting and Counting",
    "section": "8.3 Tables",
    "text": "8.3 Tables\nNow, let’s learn a way to get some pretty snazzy frequency tables and cross-tabs using the gt and gtsummary package, which are designed specifically for quickly producing publication-ready tables.\nI’ll load them in below. If you need to install them, you can run install.packages(\"gtsummary\") install.packages(\"gt\")\n\nlibrary(gtsummary)\nlibrary(gt)\n\n\n8.3.1 Frequency tables\nFirst, we’ll use gt to convert our tabyl() output from earlier into a cleaned up format that we can view outside of the R console, as well as easily export or even copy to our clipboard.\nThese will be relatively easy.\nFirst, let’s bring back some familiar code from a previous session.\n\nour_gss |&gt;\n    tabyl(cappun) |&gt;\n  adorn_totals(where = \"row\") |&gt;\n  adorn_pct_formatting() |&gt;\n  adorn_rounding(digits = 2)\n\n cappun    n percent valid_percent\n  favor 2013   56.8%         60.3%\n oppose 1327   37.4%         39.7%\n   &lt;NA&gt;  204    5.8%             -\n  Total 3544  100.0%        100.0%\n\n\nHere is our frequency table for cappun using tabyl(). There’s one more thing we need to do before we bring in gt.\nRemember that we can save this tabyl() output as an object. Let’s do that again now. We’ll save it as an object called our_tab.\n\nour_tab &lt;- our_gss |&gt;\n    tabyl(cappun) |&gt;\n  adorn_totals(where = \"row\") |&gt;\n  adorn_pct_formatting() |&gt;\n  adorn_rounding(digits = 2)\n\nThis table has all the info we need, but it would be nicer if we didn’t have the ‘cappun’ shorthand for the variable name. The other columns could be capitalized, and we could also stand to lose the underscore in ‘valid_percent’.\nThe tabyl() function actually produces a data frame, which we have just saved a moment ago. Let’s take a look at the column names like we did before, and then change these to look a little nicer.\n\n# Check the column names\ncolnames(our_tab)\n\n[1] \"cappun\"        \"n\"             \"percent\"       \"valid_percent\"\n\n# Overwrite the column names\ncolnames(our_tab) &lt;- c(\"Death Penalty Approval\", \"Frequency\", \"Percent\", \"Valid Percent\")\n\n\n# Check that it worked\nour_tab\n\n Death Penalty Approval Frequency Percent Valid Percent\n                  favor      2013   56.8%         60.3%\n                 oppose      1327   37.4%         39.7%\n                   &lt;NA&gt;       204    5.8%             -\n                  Total      3544  100.0%        100.0%\n\n\nNow, all we have to do is pipe our_tab into the gt() function\n\nour_tab &lt;- our_tab |&gt;\n  gt()\n\n# Check it out\nour_tab\n\n\n\n\n\n\n\nDeath Penalty Approval\nFrequency\nPercent\nValid Percent\n\n\n\n\nfavor\n2013\n56.8%\n60.3%\n\n\noppose\n1327\n37.4%\n39.7%\n\n\nNA\n204\n5.8%\n-\n\n\nTotal\n3544\n100.0%\n100.0%\n\n\n\n\n\n\n\nThis is not too bad for our purposes. This cleans the table up a hair while converting it into a format that we can see in the RStudio file viewer. This also allows us to copy it to our clipboard or export it as an image file.\n\n\n8.3.2 Contingency tables\nFor contingency tables, we will use an extension of the gt package called gtsummary. I’ll note that this package can be a little idiosyncratic compared to some of the ones we’ve learned about already, and you can really get into the weeds with it. But I’ll try to focus us here on just the things from which we can really get a lot of value. can also make a quick and easy contingency table for bivariate analysis. Let’s re-create our cappun x dem_rep table.\nThere’s a nifty function called tbl_cross() That includes a bunch of preset options intended for cross-tabs.\nRemember to always provide your dependent variable to the ‘row’ input.\n\nour_gss |&gt;\n  drop_na(cappun, dem_rep) |&gt;\n  tbl_cross(\n    row = cappun,\n    col = dem_rep,\n    percent = \"column\",\n    margin = c(\"column\", \"row\"),\n    label = list(\n      cappun = \"Death Penalty Attitude\",\n      dem_rep = \"Political Party\"\n    ))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolitical Party\n\nTotal\n\n\nDemocrat\nRepublican\n\n\n\n\nDeath Penalty Attitude\n\n\n\n\n\n\n\n\n    favor\n426 (43%)\n633 (84%)\n1,059 (61%)\n\n\n    oppose\n569 (57%)\n121 (16%)\n690 (39%)\n\n\nTotal\n995 (100%)\n754 (100%)\n1,749 (100%)\n\n\n\n\n\n\n\nSo, for tbl_cross() we provide our row variable and our column variable. If we set percent = \"column\", we will get percentages for each cell that will total across the rows—this is what we want. For margin = c(\"column\", \"row\"), we’re telling the function that we want totals for both columns and rows. Lastly, we give some better labels for our variables.\n\n\n8.3.3 Chi-squared test\nNow, a really neat feature of tbl_summary() is that it will automatically detect the type of variables that we have, and it will perform an appropriate statistical test. There’s a lot of customization that you can do here, but this will work great out of the box for us.\nAll we have to do is pipe the command we just produced into add_p(), which will automatically calculate a p-value using a relevant statistical test.\n\nour_gss |&gt;\n  drop_na(cappun, dem_rep) |&gt;\n  tbl_cross(\n    row = cappun,\n    col = dem_rep,\n    percent = \"column\",\n    margin = c(\"column\", \"row\"),\n    label = list(\n      cappun = \"Death Penalty Attitude\",\n      dem_rep = \"Political Party\"\n    )) |&gt;\n  add_p()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolitical Party\n\nTotal\np-value1\n\n\nDemocrat\nRepublican\n\n\n\n\nDeath Penalty Attitude\n\n\n\n\n\n\n&lt;0.001\n\n\n    favor\n426 (43%)\n633 (84%)\n1,059 (61%)\n\n\n\n\n    oppose\n569 (57%)\n121 (16%)\n690 (39%)\n\n\n\n\nTotal\n995 (100%)\n754 (100%)\n1,749 (100%)\n\n\n\n\n\n1 Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\nVery neat! This automatically recognized that we have categorical variables, ran a chi-squared test, and reported the p-value. Note that it doesn’t give the actual chi-squared value by default, but that information is in there. We can manually add a column like so:\n\nour_gss |&gt;\n  drop_na(cappun, dem_rep) |&gt;\n  tbl_cross(\n    row = cappun,\n    col = dem_rep,\n    percent = \"column\",\n    margin = c(\"column\", \"row\"),\n    label = list(\n      cappun = \"Death Penalty Attitude\",\n      dem_rep = \"Political Party\"\n    )) |&gt;\n  add_p() |&gt;\n  modify_header(statistic ~ \"Chi-square value\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolitical Party\n\nTotal\nChi-square value1\np-value1\n\n\nDemocrat\nRepublican\n\n\n\n\nDeath Penalty Attitude\n\n\n\n\n\n\n304\n&lt;0.001\n\n\n    favor\n426 (43%)\n633 (84%)\n1,059 (61%)\n\n\n\n\n\n\n    oppose\n569 (57%)\n121 (16%)\n690 (39%)\n\n\n\n\n\n\nTotal\n995 (100%)\n754 (100%)\n1,749 (100%)\n\n\n\n\n\n\n\n1 Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\nGood stuff. I’ll point out that we use the ~ symbol above, but don’t worry too much about that. It’s one of the peculiarities I alluded to about gtsummary. Just know that we have to use ~ there. Otherwise, there’s lots of great detail here, so this is pretty much good to go.\n\n\n8.3.4 Elaboration Model\nLastly, this also gives us an easy way to automatically produce a 3-way cross-tab for multivariate analysis. We’ll need to change a couple of things around, but this can be done without too much work in gtsummary.\nIn our set up here, we have cappun as our dependent variable and dem_rep as our independent variable. Let’s add sex as a test variable, and see whether sex has any effect on the relationship we have observed between death penalty attitudes and political party.\nFor this, we need the tbl_strata() function.\n\nour_gss |&gt;\n  select(cappun, sex, dem_rep) |&gt;\n  drop_na(cappun, sex, dem_rep) |&gt;\n  tbl_strata(\n    strata = sex,\n    .tbl_fun = \n      ~ .x |&gt;\n    tbl_cross(\n    row = cappun,\n    col = dem_rep,\n    percent = \"column\",\n    margin = c(\"column\", \"row\"),\n    label = list(\n      cappun = \"Death Penalty Attitude\",\n      dem_rep = \"Political Party\"\n    )) |&gt;\n    add_p()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfemale\n\n\nmale\n\n\n\nDemocrat\nRepublican\nTotal\np-value1\nDemocrat\nRepublican\nTotal\np-value1\n\n\n\n\nDeath Penalty Attitude\n\n\n\n\n\n\n&lt;0.001\n\n\n\n\n\n\n&lt;0.001\n\n\n    favor\n244 (41%)\n276 (80%)\n520 (55%)\n\n\n182 (46%)\n354 (88%)\n536 (67%)\n\n\n\n\n    oppose\n350 (59%)\n71 (20%)\n421 (45%)\n\n\n218 (55%)\n50 (12%)\n268 (33%)\n\n\n\n\nTotal\n594 (100%)\n347 (100%)\n941 (100%)\n\n\n400 (100%)\n404 (100%)\n804 (100%)\n\n\n\n\n\n1 Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\nThis is starting to sprawl a little, but it’s basically the same command as before, except that now we have put our tbl_cross() command inside of the tbl_strata().\ntbl_strata, first takes the ‘strata’ input, which is where we indicate our test variable. It’s the variable by which we want to stratify our data. The effect of strata = sex is somewhat similar to group_by() from tidyverse—it groups the data by our test variable.\n.tbl_fun = ~.x probably looks a little strange. This gets a little into the weeds, so don’t worry about it too much. To make use of this sample tbl_strata() code with your own data, you will just need to make sure you give your test variable to the strata = input. You can always leave the .tbl_fun = ~.x part the same, but I’ll give a little context for reference.\n.tbl_fun is shorthand for “table function”, and .x is an R-specific placeholder for function inputs. Essentially, we are saying “group the data by our strata variable, and then pass that stratified data into another function of our choosing.”\nThen, we can just give our tbl_cross() function from before, and this will be calculated according to the grouping structure of the test variable we provide.\nNow, as an exercise, let’s think about what we have here.\nWe have our original relationship predicting death-penalty attitudes based on political affiliation. But now we can see this relationship for both men and women. The female group has 941 respondents and the male group has 804.\nIf we look at the percentages for both the male & female groups, they are relatively similar and the basic relationship is about the same: Democrats are more likely to oppose the death penalty and less likely to favor the death penalty, regardless of sex. Not only this, but we can see that a chi-squared test has been run for both the male and female subgroups. Both of these tests returned statistically significant p-values, as we can see in the table.\nSo, this would be a case of replication. We have replication when our zero-order relationship remains statistically significant after introducing our test variable, and the nature of the relationship does not change. In this case, we observed a statistically significant relationship between political affiliation and death-penalty attitudes. After we added sex as a test variable, this original relationship remained statistically significant across the two response categories of sex. If the percentages had been markedly different across males and females, we would say that this was specification, as that would suggest that—even if political affiliation is associated with death penalty attitudes—the relationship is different for men and women.",
    "crumbs": [
      "Day 4: Making Better Visualizations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting and Counting</span>"
    ]
  }
]