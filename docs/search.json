[
  {
    "objectID": "viz_introduction.html",
    "href": "viz_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Visualization and Analytical Thinking\nUp to this point, we have worked with pretty simple visualizations, just so we can quickly glean important information about our statistical models without spending too much time focusing on the aesthetics of the visuals.\nIn this lab, we will learn a bit more about R’s graphical capability—especially through tidyverse’s ggplot—which provides us with incredible customizability. We will learn how to fine-tune some of the visuals we have already worked with, and we will preview some other common visual styles that can manage with ggplot.\nBefore we start working with some of these new visual tools, I want to take an opportunity to stress the importance of visualization more generally. It’s easy to see the process of presenting visuals as something somewhat superficial, but visualization can be critical for defining the kind of questions we can ask about our data.\nFor now, I’m going to obscure the code I’m using for this document. We will learn more about the kind of commands I used to generate the following figures, but I don’t want anyone to get bogged down initially. I’ll use these visuals to help impart an important lesson about data visualization’s in the research process.",
    "crumbs": [
      "Day 5: Making Better Visualizations",
      "Introduction"
    ]
  },
  {
    "objectID": "viz_introduction.html#thirteen-data-sets",
    "href": "viz_introduction.html#thirteen-data-sets",
    "title": "Introduction",
    "section": "Thirteen Data Sets",
    "text": "Thirteen Data Sets\nLet’s take a look at a collection of thirteen different data sets. Each data set has 142 observations with 2 columns, labeled x & y.\nI’ll use some tidyverse commands to get some summary statistics for each of the data sets, including the mean of both variables and their standard deviations. Let’s see what seems to distinguish some of these data sets from one another.\n\n\n\n\n\nMean (x)\nMean (y)\nSD (x)\nSD (y)\n\n\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n\n\n\nWell, there’s not much we can say here. All the summary statistics are identical. Why don’t we try modeling a linear relationship between the x and y variables. Maybe looking at the correlations will tell us something. I’ll display the linear regression lines for each data set below.\n\n\n\n\n\n\n\n\n\nOkay. This is not revealing much either. All the lines seem to have the same slope, which shows a (slight) negative relationship where y decreases as x increases. The correlations aren’t revealing any notable distinctions.\nBut wait. One thing we can see here is that, while the correlations appear to be about the same, there are some differences in the ranges of values. Note that the regression lines don’t extend across the same range of x-axis values in each data set. Maybe there is something here after all.\nLet’s just go ahead and plot the actual data.\n\n\n\n\n\n\n\n\n\nNow there’s some distinction!\nThis is a tongue in cheek data set known as the ‘datasaurus dozen’. It’s often used in intro statistical classes to help illustrate the importance of visualization. It’s inspired by another conceptually similar data set known as ‘Anscombe’s quartet’ which likewise stresses the role of plotting data in producing well informed analyses.",
    "crumbs": [
      "Day 5: Making Better Visualizations",
      "Introduction"
    ]
  },
  {
    "objectID": "viz_introduction.html#in-sum",
    "href": "viz_introduction.html#in-sum",
    "title": "Introduction",
    "section": "In Sum",
    "text": "In Sum\nSo, take this as a showcase of the importance of visualizing your data. This isn’t to discount summary statistics and other numeric description of data—those are still invaluable for us.\nRather, cases like Datasaurus or Anscombe’s quartet highlight the necessity of understanding the shape of your data. This will determine the kind of questions you can ask with the data, as well as the kind of statistical tools you need to describe it.\nFor example, in the case we just examined, those x and y variables do not have any kind of clear linear relationship. In that case, tools like standard OLS regression that assume linearity are not appropriate. Any relationship between the variables could only be explored through other statistical means.\nSo, making our figures and tables look aesthetically pleasing is indeed valuable in its own right, but don’t underestimate the utility of good visualization for the analytic process itself.",
    "crumbs": [
      "Day 5: Making Better Visualizations",
      "Introduction"
    ]
  },
  {
    "objectID": "univariate_numeric.html",
    "href": "univariate_numeric.html",
    "title": "5  Univariate: Numeric",
    "section": "",
    "text": "5.1 Frequency Distributions\nWhile we also want to report on the frequency distribution and central tendency of numeric variables, there are some differences in the way we go about that relative to the techniques we just learned about for categorical variables.\nAdditionally, we will need to report on some measures of dispersion that can only be reported for numeric data, such as the standard deviation.\nThis section will walk us through the process of calculating and reporting necessary elements of a univariate analysis of numeric data.\nAs we saw when we were recoding age earlier in the unit, a frequency table is not really appropriate for numeric data. When there are upwards of 71 different response values—as we have with age—a table would be both spatially unwieldy and difficult to interpret.\nSo, let’s start with a style of plotting data that is essentially the barplot of numeric data.\nFirst, let’s go ahead and set up our environment by loading in our data and tidyverse per usual.\nlibrary(tidyverse)\nload(\"our_gss.rda\")",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#frequency-distributions",
    "href": "univariate_numeric.html#frequency-distributions",
    "title": "5  Univariate: Numeric",
    "section": "",
    "text": "5.1.1 Histograms\nA histogram is the result of taking a numeric variable, slicing it up into equal, ordinal intervals, and then plotting the frequency of each interval.\nThere are plenty of other ways for plotting numeric data—some of which we will see later when we focus on visualization—but for now, this is a simple and effective way to get a sense of the distribution of a numeric variable. This is especially important when we need to decide on an appropriate measure of central tendency.\nWe can create these easily using the hist() function, which is in the same family of base R plotting functions as barplot(), so much of what we learned for that function will also apply with hist(). This function will automatically apply a commonly used method called Sturges’s rule to divide our numeric variable into equal bins, so we do not have to specify anything in that regard.\nAll we need to do is give the hist() function our numeric variable. Let’s work with the realrinc variable for this example.\n\nhist(our_gss$realrinc)\n\n\n\n\n\n\n\n\nMuch like we did with barplot(), we can clean this up a little by adding a title and a better label for the x axis\n\nhist(\n  our_gss$realrinc,\n  main = \"Income distribution of GSS Respondents\",\n  xlab = \"Respondent's income in dollars\",\n  )\n\n\n\n\n\n\n\n\nNow we have our histogram!\nHowever, this visualization does suggest that we need to do some thinking about the appropriate measure of central tendency for realrinc.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#central-tendency",
    "href": "univariate_numeric.html#central-tendency",
    "title": "5  Univariate: Numeric",
    "section": "5.2 Central Tendency",
    "text": "5.2 Central Tendency\nFor numeric data, any of the 3 common measures of central tendency can be reported. However, the preferred measure for a given variable depends on its distribution.\nThe mean is the gold standard for numeric data, because it takes into account every single data point in its calculation, and is thus the most comprehensive index of central tendency.\nHowever, the mean can be misleading in some cases. The distribution should be (basically) normal in order for you to report the mean, and you will want to report the median in most other cases.\nI will first cover the choice of mean or median by revisiting some basics of distributions, and then I will address the rare circumstance where the mode is appropriate.\n\n5.2.1 Mean or Median?\nIn most cases, we will be making a decision between the mean and the median. When the distribution is normal, you should report the mean—otherwise, report the median. We’ll see one exception to this later, but that’s generally our task.\nWe saw some typical distributions in our last section on uivariate analysis of categorical data. We’ll return to some discussion of those basic distribution types but add a little more context for numeric data that will help us see the impact of making the wrong decision.\nI’m going to display a normal distribution, but this time, I’ll draw a vertical line indicating the location of the mean and median.\n\n\n\n\n\n\n\n\n\nIn this case, we can’t even really tell the mean and median apart. In fact, in a completely normal distribution, all three measures of central tendency will be equal. Remember that real-world data will rarely exhibit perfect normality, but as long as it’s approximately normal, we can follow convention and report the mean.\nNow, let’s look back at realrinc.\n\n\n\n\n\n\n\n\n\nIn the case of realrinc, we definitely do not have much normality going on. For one, there are some apparent outlier cases. Much of our data seems to be concentrated within about $0 - $75,000, but then we have a bunch of values at roughly double the maximum of that narrower range. And even within the range where most of our responses are clustered, we have some clear right-skew.\nAs we can see, because the mean takes all values into account, this makes it susceptible to non-normal distributions. It gets pulled in the direction of the outlier, which, in this case, is also the direction of the skew.\nSo, in any case where your numeric variable contains clear outliers and/or exhibits notable skew in either direction, you should report the median rather than the mean.\n\n\n5.2.2 The Mode\nBy and large, the mode is more appropriate for categorical data. But there are some circumstances where it makes sense to report the mode for numeric data.\nLet’s take a look at the original age variable to see an example.\n\nhist(\n  our_gss$age,\n  main = \"Age distribution of 2022 GSS Respondents\",\n  xlab = \"Respondent age in years\"\n)\n\n\n\n\n\n\n\n\nThis isn’t quite a normal distribution, but it’s not exactly skewed in either direction either. And there are no apparent outliers.\nWhat we have here is a bimodal distribution.\nThis is what happens when we have multiple peaks in a distribution—two, in this case. We have seen several different distributions so far, but they all had only one peak in the distribution. They were differentiated on the basis of that single peak’s location in the distribution. In the case of a multimodal distribution, we want to report on any notable peak.\nThis may be a peculiarity of the 2022 survey wave, because we wouldn’t necessarily expect this, but it looks like there are two distinct central tendencies for age. In this case, we want to capture the two distinct peaks, meaning we need to calculate two modes.\nThankfully, there’s a convenient function for this, but we need to install a new package, so let’s install and load in multimode.\n\ninstall.packages(\"multimode\")\nlibrary(multimode)\n\nThen, we just need to provide a few arguments to the locmodes() function.\nFirst, we give the variable for which we want to calculate multiple modes.\nThen we give an input for mod0, which asks us how many modes the function should be looking for. This should be based on visual inspection of the data. We have two peaks in our distribution, so we will enter ‘2’ for this input.\nLastly, we will set display to ‘TRUE’, which will show us a plot of the estimated modes superimposed onto the frequency distribution of the variable. This will let us evaluate whether the modes estimated by the function are plausible. They should match up with the visible peaks in the frequency distribution.\n\nlocmodes(\n  our_gss$age,\n  mod0 = 2,\n  display = TRUE\n)\n\n\n\n\n\n\n\n\n\nEstimated location\nModes:  32.4301  66.25356 \nAntimode: 50.89506 \n\nEstimated value of the density\nModes: 0.01938966  0.01730832 \nAntimode: 0.01444287 \n\nCritical bandwidth: 2.543659\n\n\nWe only really need to pay attention to the modes that it gives us (32.43 and 66.25), but it looks like this function spotted these values pretty effectively.\nFor a bimodal distribution like this, we can report both modes for the measure of central tendency. I don’t expect we will run into too much of this, but go ahead and deal with it like this in the event that you do.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#dispersion",
    "href": "univariate_numeric.html#dispersion",
    "title": "5  Univariate: Numeric",
    "section": "5.3 Dispersion",
    "text": "5.3 Dispersion\nNow, let’s talk about dispersion. This is the last major element of a univariate analysis of numeric data. Actually calculating this information is quite simple in R, so we will practice with some of our GSS variables and then talk about putting all of this information together.\n\n5.3.1 Range\nThis is perhaps the simplest measure of dispersion and comprises the minimum and maximum values of the distribution.\nR has a built in range function, so we can simply provide one of our variables as the input. We’ll work with realrinc again. As we have done before, we will also need to provide na.rm = TRUE, as this variable column includes NAs and will confuse R otherwise.\n\nrange(our_gss$realrinc, na.rm = TRUE)\n\n[1]    204.5 141848.3\n\n\nAt this point, I’ll also offer a reminder about the summary() function, which we can use to see a variety of information about the dispersion (in an addition to the central tendency).\n\nsummary(our_gss$realrinc)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n   204.5   8691.2  18405.0  27835.3  33742.5 141848.3     1554 \n\n\nFor a numeric variable, summary() will show us the minimum & maximum, as well as the mean & median, and the 1st & 3rd quartiles.\nWe can actually think of most of these elements in terms of percentiles.\n\nThe minimum value is the 0th percentile of the distribution\nThe 1st quartile is the 25th percentile.\nThe median is the 50th percentile\nThe 3rd quartile is the 75th percentile\nAnd the maximum is the 100th percentile.\n\nsummary() is great for quickly assessing some of these descriptive statistics, but it’s a little less convenient for exporting this information into something like a table for our own univariate analysis.\nSo, I’ll also highlight the min() and max() functions, which will come in hand for us shortly. They work simply enough—we just need to provide our variable column, and they will output the minimum and maximum, respectively.\n\nmin(our_gss$realrinc, na.rm = TRUE)\n\n[1] 204.5\n\nmax(our_gss$realrinc, na.rm = TRUE)\n\n[1] 141848.3\n\n\n\n\n5.3.2 Standard Deviation\nThis is one of the more commonly reported dispersion metrics for numeric data. The standard deviation is a measure of how much the average value varies from the mean. In plainer terms, it’s a measure that tells us how spread out the distribution is.\n\n\n\n\n\n\n\n\n\nWe can grab the standard deviation quite easily with the sd() function\n\nsd(our_gss$realrinc, na.rm = TRUE)\n\n[1] 31962.34\n\n\nNow, let’s talk about putting all this together in our reporting of a univariate analysis for numeric data.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#putting-it-all-together",
    "href": "univariate_numeric.html#putting-it-all-together",
    "title": "5  Univariate: Numeric",
    "section": "5.4 Putting It All Together",
    "text": "5.4 Putting It All Together\nNow that we have worked through how to calculate all of these key statistics with R, let’s revisit some of the first tidyverse functions we learned about back on the first day.\nWe can use a combination of select() and summarize to quickly compile several bits of important information, such as the range, the central tendency, and the standard deviation.\nLet’s do this for realrinc.\n\nour_gss |&gt;\n  select(realrinc) |&gt;\n  summarize(\n    \"Minimum\" = min(realrinc, na.rm = TRUE),\n    \"Median\" = median(realrinc, na.rm = TRUE),\n    \"Maximum\" = max(realrinc, na.rm = TRUE),\n    \"SD\" = sd(realrinc, na.rm = TRUE)\n  )\n\n  Minimum Median  Maximum       SD\n1   204.5  18405 141848.3 31962.34\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might notice that there’s an attribute for each column that looks like &lt;dbl&gt;. This is short for ‘double’, which is itself shorthand for ‘double-precision floating-point format’. This is in reference to the underlying data type that R uses to store continuous numeric values. You really don’t need to know anything more about double-precision floating point than that for our purposes, but I mention it here because you might run into  or ‘double’ in R. When you do, just think ‘numeric data’.\n\n\nIf you have a numeric variable where the mean is more appropriate, you can just swap median() with mean() in the code template above. Be sure to also change the column name as well.\nAnd that should do us! For any univariate analysis you report in this course, you will just need to produce\n\na histogram displaying the variable’s frequency distribution\na table like the one above (with the appropriate measure of central tendency).",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "recoding_variables.html",
    "href": "recoding_variables.html",
    "title": "3  Recoding Variables",
    "section": "",
    "text": "3.1 Background\nToday’s venture concerns univariate analysis, i.e. the quantitative description of a single variable. Before we do that, however, we need to familiarize ourselves with some data-cleaning procedures.\nRecoding a variable involves manipulating the underlying structure of our variable such that we can use it for analysis. We did a little recoding during the last unit when we converted character vectors into factor variables. This allowed us to align R data types with the appropriate level of measurement.\nThere are also occasions when we need a variable to be translated from one level of measurement to another. For example, we may want to convert a ratio variable for “number of years of education” into an ordinal variable reflecting categories like “less than high school”, “high school diploma”, “Associates degree”, and so on.\nWe may also want to collapse the categories of ordinal variables for some analyses. Consider a variable with a Likert-scale agreement rating, where you responses like “strongly agree,” “moderately agree,” “slightly agree,” and so forth. You may decide to collapse these categories into categories of “Agree” and “Disagree”.\nWe will get some practice doing this sort of thing, which is an essential component of responsible analysis. Additionally, our next unit on bivariate analysis will require us to work with categorical variables in particular, so we need to be capable of converting any numeric variables.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Recoding Variables</span>"
    ]
  },
  {
    "objectID": "recoding_variables.html#converting-numeric-to-categorical",
    "href": "recoding_variables.html#converting-numeric-to-categorical",
    "title": "3  Recoding Variables",
    "section": "3.2 Converting Numeric to Categorical",
    "text": "3.2 Converting Numeric to Categorical\nWe will start by recoding age—a ratio variable—into an ordinal variable reflecting age groupings. The same strategies we use here will work for any numeric variable.\n\n3.2.1 Setting up our workspace\nAs usual, let’s make sure we load in tidyverse along with our GSS data.\n\nlibrary(tidyverse)\nload(\"our_gss.rda\")\n\nLet’s double check the structure of our data frame.\n\nstr(our_gss)\n\n'data.frame':   3544 obs. of  8 variables:\n $ year    : int  2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 ...\n $ id      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age     : int  72 80 57 23 62 27 20 47 31 72 ...\n $ race    : Factor w/ 3 levels \"white\",\"other\",..: 1 1 1 1 1 1 2 1 1 NA ...\n $ sex     : Factor w/ 2 levels \"female\",\"male\": 1 2 1 1 2 2 1 2 1 1 ...\n $ realrinc: num  40900 NA 18405 2250 NA ...\n $ partyid : Factor w/ 8 levels \"strong democrat\",..: 1 2 3 1 2 4 5 1 5 1 ...\n $ happy   : Ord.factor w/ 3 levels \"not too happy\"&lt;..: 1 1 1 1 2 2 2 2 2 2 ...\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might notice the ‘int’ category, which is short for ‘integer’. This is a subtype of numeric data in R. Variables that are exclusively whole numbers are often recorded in this way, but we can work with them in R just like we can other sorts of numbers\n\n\n\n\n3.2.2 The ‘age’ variable\nWe can take a look at all the values of age (along with the # of respondents in each age category) using the count() function.\n\ncount(our_gss, age)\n\nHowever, I’m not going to display those results here, as it will be a particularly long table of values (with 70+ different ages). It’s fine to run it—it won’t crash R or anything—but it will it clutter up this page. Let’s take this as a good opportunity to take advantage of the fact that we are using one of the better funded and well-organized surveys in all of social sciences. As such, there’s extensive documentation about all of the variables measured for the GSS. Go ahead and take a look at the age variable via the GSS Data Explorer, which allows us to search for unique variables and view their response values, the specific question(s) that was asked on the survey, and several other variable characteristics.\nThe responses range from 18 - 89 (in addition to a few categories for non-response). However, note that there’s something unique about value 89. It’s not just 89 years of age, but 89 & older. This isn’t a real issue for our purposes, but take this as encouragement to interface with the codebook of any publicly available data you use. There’s some imprecision at the upper end of this variable, and that might not be obvious without referencing the codebook.\nFor the purposes of this exercise, let’s go ahead and turn age into a simple categorical variable with 3 levels—older, middle age, and younger. I’m going to choose the range somewhat arbitrarily for now. We can use univariate analysis to inform our decision about how to break up a numeric variable, so we will revisit this idea again later on.\n\nYounger = 18 - 35\nMiddle Age = 36 - 49\nOlder = 50 and up\n\nAt the end of the day, what we need to do is 1.) create a new variable column 2.) populate that column with ordinal labels that correspond with each respondent’s numeric age interval.\n\n\n3.2.3 New columns with mutate()\nFirst, let’s consider the mutate() function. This function takes a data frame and appends a new variable column. This new variable is the result of some calculation applied to an existing variable column.\nLet’s look at an application fo mutate() to get a feel for it. Now, this wouldn’t be the best idea for a couple reasons, but, as an illustration, let’s say we wanted to convert our yearly income values to an hourly wage (assuming 40 hrs/week).\nmutate() takes a data frame as its input, and then we provide the name of our new variable column(s) along with the calculation for this new variable. Below, I use mutate() to create a new column called hr_wage. Then, I tell R that the hr_wage variable should be calculated by taking each person’s income value and diving that by 52 weeks in a year, and then 40 hours in a week.\n\n# Without the pipe operator\nour_gss &lt;- mutate(\n  our_gss,\n  hr_wage = (realrinc/52)/40\n  )\n\n\n# With the pipe operator\nour_gss &lt;- our_gss |&gt;\n  mutate(\n    hr_wage = (realrinc/52)/40\n  )\n\nTake a look at your new data frame object. I’ll show a summary here to verify that we got our new column.\n\nsummary(our_gss)\n\n      year            id              age           race          sex      \n Min.   :2022   Min.   :   1.0   Min.   :18.00   white:2514   female:1897  \n 1st Qu.:2022   1st Qu.: 886.8   1st Qu.:34.00   other: 412   male  :1627  \n Median :2022   Median :1772.5   Median :48.00   black: 565   NA's  :  20  \n Mean   :2022   Mean   :1772.5   Mean   :49.18   NA's :  53                \n 3rd Qu.:2022   3rd Qu.:2658.2   3rd Qu.:64.00                             \n Max.   :2022   Max.   :3545.0   Max.   :89.00                             \n                                 NA's   :208                               \n    realrinc                                      partyid   \n Min.   :   204.5   independent (neither, no response):835  \n 1st Qu.:  8691.2   strong democrat                   :595  \n Median : 18405.0   not very strong democrat          :451  \n Mean   : 27835.3   strong republican                 :431  \n 3rd Qu.: 33742.5   independent, close to democrat    :400  \n Max.   :141848.3   (Other)                           :797  \n NA's   :1554       NA's                              : 35  \n           happy         hr_wage        \n not too happy: 799   Min.   : 0.09832  \n pretty happy :1942   1st Qu.: 4.17849  \n very happy   : 779   Median : 8.84856  \n NA's         :  24   Mean   :13.38237  \n                      3rd Qu.:16.22236  \n                      Max.   :68.19631  \n                      NA's   :1554      \n\n\nSo, we can use mutate() to add a new column that contains our recoded variable. We just need a way to specify a calculation that takes into account the specific intervals we want for our ordinal labels. For this, we need one more function.\n\n\n3.2.4 Custom intervals with cut()\nWe can use the cut() function to specify the intervals we want for our age groupings, and then we will combine it with mutate() to generate our recoded age variable. Specifically, cut() takes our intervals and turns each of them into a level in a new factor variable.\nBut first, I want to give a little context on interval notation in mathematics. It will help us all be a little more precise when we talk about ranges, and it will also help us understand an input we need to provide for cut().\n\n3.2.4.1 An aside on intervals\nIn mathematic terms, an interval is the set of all numbers in between two specified end points. In formal notation, these are indicated with the two endpoints placed in brackets, e.g [1,5] as the interval of 1 through 5.\nThere are some technical terms to describe whether or not we want to include the endpoints when we are talking about a particular interval.\n\nClosed interval: This is when both end points are included in the interval. Closedness is indicated with square brackets, so, the closed interval of 1 through 5 would be written just like I have above—[1,5]. This indicates any number \\(x\\) where \\(1 \\leq x \\leq 5\\)\nOpen interval: This is when neither endpoint is included in the interval. In interval notation, openness is indicated with parentheses rather than square brackets, so the open interval of 1 through 5 would be written as (1,5). This interval would indicate any number \\(x\\) where \\(1 &lt; x &lt; 5\\)\nLeft-open interval: This is when the left-hand endpoint is not included, but the right-hand endpoint is. This would be written as (1,5] in interval notation, and that interval would indicate any number \\(x\\) where \\(1 &lt; x \\leq 5\\)\nRight-open interval: When the right-hand endpoint isn’t included but the left endpoint is, you have a right-open interval. This would be written as [1,5) in interval notation and would indicate any number \\(x\\) where \\(1 \\leq x &lt; 5\\).\n\nWe will be working with the right-open interval format, and we can specify that in cut().\nNow, let’s return to our task at hand.\n\n\n3.2.4.2 Inputs for cut()\nWe’ll go ahead and work with cut directly in mutate(), as its going to be the calculation that we are providing for the new column we generate with mutate().\nAs a reminder, here are the intervals we need:\n\nYounger = 18 - 35\nMiddle Age = 36 - 49\nOlder = 50 and up\n\nThe following code may look a little chaotic at first glance, but it’s really the same sort of thing that we did with mutate() above. It’s just that cut() is a little bit more involved of a calculation than the simple division we did in our example.\nJust like above, we are applying mutate() to our_gss, and we are giving the name of our new variable column (age_ord, in this case) as the first input. Then, for the calculation, we give the cut() function and the arguments it needs.\ncut() and its inputs\n\nThe variable for which we want to specify intervals (age)\nbreaks: This is where we indicate the intervals. The first number we give is the low end of our lowest age group (18). The second number is the low end of our middle age group (36). The third number, as you probably guessed, is the low end of our highest age group. The last value should reflect our upper limit. In this case, I use the value Inf, which is short for ‘infinity’. This essentially tells R that the last category can include any values higher than the previous number we entered.\ninclude.lowest: Putting TRUE here tells R that, in each interval, we want the lowest value to be included. If we don’t do this, then our ‘younger’ age grouping would be 19 - 35 rather than 18 - 35. In other words, setting this to TRUE indicates a left-closed interval, and FALSE indicates a left-open interval.\nright: This is the input for specifying whether we want this to be a right-closed interval, and it takes a logical value (TRUE or FALSE). We want a right-open interval, so we will set this to FALSE.\nlabels: R will actually default to formal interval notation for the names of each level of our new factor variable, so it would be [18,36), [36,50), [50, Inf). However, we can provide a character vector to specify custom labels for these new factor levels. In the event that you have an ordinal variable, make sure that you always specify these labels in ascending order. In this case, that would be Younger &lt; Middle Age &lt; Older.\nordered_result: This takes a logical value and, as the name suggests, indicates whether we want the factor to be ordered or not. In our case, there is a clear progression in terms of age, so we need to set this to TRUE.\n\n\nour_gss &lt;- our_gss |&gt;\n  mutate(\n    age_ord = cut(\n      age,\n      breaks = c(18, 36, 50, Inf),\n      include.lowest = TRUE,\n      right = FALSE,\n      labels = c(\"Younger\", \"Middle Age\", \"Older\"),\n      ordered_result = TRUE\n    )\n  )\n\nGo ahead and take a look at our_gss, and our new column should bring us to 10 variables instead of 9. I’ll use the str() function here so we can confirm that our factor was ordered and added to the data frame.\n\nstr(our_gss)\n\n'data.frame':   3544 obs. of  10 variables:\n $ year    : int  2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 ...\n $ id      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age     : int  72 80 57 23 62 27 20 47 31 72 ...\n $ race    : Factor w/ 3 levels \"white\",\"other\",..: 1 1 1 1 1 1 2 1 1 NA ...\n $ sex     : Factor w/ 2 levels \"female\",\"male\": 1 2 1 1 2 2 1 2 1 1 ...\n $ realrinc: num  40900 NA 18405 2250 NA ...\n $ partyid : Factor w/ 8 levels \"strong democrat\",..: 1 2 3 1 2 4 5 1 5 1 ...\n $ happy   : Ord.factor w/ 3 levels \"not too happy\"&lt;..: 1 1 1 1 2 2 2 2 2 2 ...\n $ hr_wage : num  19.66 NA 8.85 1.08 NA ...\n $ age_ord : Ord.factor w/ 3 levels \"Younger\"&lt;\"Middle Age\"&lt;..: 3 3 3 1 3 1 1 2 1 3 ...",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Recoding Variables</span>"
    ]
  },
  {
    "objectID": "recoding_variables.html#restructuring-categorical-variables",
    "href": "recoding_variables.html#restructuring-categorical-variables",
    "title": "3  Recoding Variables",
    "section": "3.3 Restructuring Categorical Variables",
    "text": "3.3 Restructuring Categorical Variables\nNow, let’s go ahead and tackle the other recoding situation I mentioned up at the top—collapsing the levels of categorical variables. We’ll work with partyid here, but the logic of this process will apply to any categorical variable.\n\n3.3.1 Collapsing categories\nI encourage you to take a look at partyid in the GSS Data Explorer, like we did for ‘age’..\nBecause partyid is a categorical variable, it has far fewer unique values than a ratio variable like age, so we can go ahead and take a look at all of its levels with the count() function.\n\n\n\n\n\n\nNote\n\n\n\nObserve that count(our_gss, partyid) basically provides the same information as summary(our_gss$partyid) when the variable is a factor. The only real difference is that count() organizes the info into a tidy data frame. In other words, you can use either function to take a look at factor variable like this.\n\n\n\ncount(our_gss, partyid)\n\n                             partyid   n\n1                    strong democrat 595\n2 independent (neither, no response) 835\n3         not very strong republican 361\n4           not very strong democrat 451\n5     independent, close to democrat 400\n6                        other party 106\n7   independent, close to republican 330\n8                  strong republican 431\n9                               &lt;NA&gt;  35\n\n\nLet’s go ahead and collapse these into categories of “Democrat”, “Independent”, “Other Party”, and “Republican”. We’ll use mutate() to make a new variable column called partyid_recoded. For the calculation of our new variable, we can use the fct_collapse() function. This function allows us to first give the name of a new factor level, and then we can give a character vector of the names of levels that want to be collapses into a single category.\nSo, to collapse our 3 Democrat levels into a single factor level, we would give the following input for fct_collapse():\n\"Democrat\" = c(\"strong democrat\", \"not very strong democrat\")\nAnd then repeat for each new factor level.\n\nour_gss &lt;- our_gss |&gt;\n  mutate(\n    partyid_recoded=fct_collapse(partyid, \n\"Democrat\" = c(\"strong democrat\", \"not very strong democrat\"),\n\"Republican\" = c(\"strong republican\",\"not very strong republican\"),\n\"Independent\" = c(\"independent, close to democrat\", \"independent (neither, no response)\", \"independent, close to republican\"),\n\"Other Party\" = c(\"other party\")\n))\n\nNow, let’s take a look at our new variable.\n\ncount(our_gss, partyid_recoded)\n\n  partyid_recoded    n\n1        Democrat 1046\n2     Independent 1565\n3      Republican  792\n4     Other Party  106\n5            &lt;NA&gt;   35\n\n\n\n\n3.3.2 Excluding levels\nYou may also want to work with an existing categorical variable, but only focus on certain values. In this case, we can create a recoded variable and simply code the levels we are uninterested in as NA.\nWe can use the fct_recode() function for this. For the first input, we will give it the factor-variable column that we want to recode. Then, we will let it know that we want to set the levels of “Other Party” and “Independent” to NULL. This will convert them to NAs and allow us to easily exclude them from our analyses. We will use fct_recode() within mutate() so we can create another recoded version of partyid_recoded. We’ll call this one dem_rep to distinguish it from the original partyid and our first recoded version.\n\nour_gss &lt;- our_gss |&gt;\n  mutate(dem_rep = fct_recode(\n    partyid_recoded, \n    NULL=\"Other Party\", \n    NULL=\"Independent\"))\n\nLet’s check to see that it worked.\n\ncount(our_gss, dem_rep)\n\n     dem_rep    n\n1   Democrat 1046\n2 Republican  792\n3       &lt;NA&gt; 1706\n\n\nNow that we have gotten all this pre-processing stuff out of the way, let’s go ahead and dig into some univariate analysis.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Recoding Variables</span>"
    ]
  }
]