[
  {
    "objectID": "viz_introduction.html",
    "href": "viz_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Visualization and Analytical Thinking\nUp to this point, we have worked with pretty simple visualizations, just so we can quickly glean important information about our statistical models without spending too much time focusing on the aesthetics of the visuals.\nIn this lab, we will learn a bit more about R’s graphical capability—especially through tidyverse’s ggplot—which provides us with incredible customizability. We will learn how to fine-tune some of the visuals we have already worked with, and we will preview some other common visual styles that can manage with ggplot.\nBefore we start working with some of these new visual tools, I want to take an opportunity to stress the importance of visualization more generally. It’s easy to see the process of presenting visuals as something somewhat superficial, but visualization can be critical for defining the kind of questions we can ask about our data.\nFor now, I’m going to obscure the code I’m using for this document. We will learn more about the kind of commands I used to generate the following figures, but I don’t want anyone to get bogged down initially. I’ll use these visuals to help impart an important lesson about data visualization’s in the research process.",
    "crumbs": [
      "Day 5: Making Better Visualizations",
      "Introduction"
    ]
  },
  {
    "objectID": "viz_introduction.html#thirteen-data-sets",
    "href": "viz_introduction.html#thirteen-data-sets",
    "title": "Introduction",
    "section": "Thirteen Data Sets",
    "text": "Thirteen Data Sets\nLet’s take a look at a collection of thirteen different data sets. Each data set has 142 observations with 2 columns, labeled x & y.\nI’ll use some tidyverse commands to get some summary statistics for each of the data sets, including the mean of both variables and their standard deviations. Let’s see what seems to distinguish some of these data sets from one another.\n\n\n\n\n\nMean (x)\nMean (y)\nSD (x)\nSD (y)\n\n\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n54.3\n47.8\n16.8\n26.9\n\n\n\n\n\nWell, there’s not much we can say here. All the summary statistics are identical. Why don’t we try modeling a linear relationship between the x and y variables. Maybe looking at the correlations will tell us something. I’ll display the linear regression lines for each data set below.\n\n\n\n\n\n\n\n\n\nOkay. This is not revealing much either. All the lines seem to have the same slope, which shows a (slight) negative relationship where y decreases as x increases. The correlations aren’t revealing any notable distinctions.\nBut wait. One thing we can see here is that, while the correlations appear to be about the same, there are some differences in the ranges of values. Note that the regression lines don’t extend across the same range of x-axis values in each data set. Maybe there is something here after all.\nLet’s just go ahead and plot the actual data over our regression lines.\n\n\n\n\n\n\n\n\n\nNow there’s some distinction!\nThis is a tongue in cheek data set known as the ‘datasaurus dozen’. It’s often used in intro statistical classes to help illustrate the importance of visualization. It’s inspired by another conceptually similar data set known as ‘Anscombe’s quartet’ which likewise stresses the role of plotting data in producing well informed analyses.",
    "crumbs": [
      "Day 5: Making Better Visualizations",
      "Introduction"
    ]
  },
  {
    "objectID": "viz_introduction.html#in-sum",
    "href": "viz_introduction.html#in-sum",
    "title": "Introduction",
    "section": "In Sum",
    "text": "In Sum\nSo, take this as a showcase of the importance of visualizing your data. This isn’t to discount summary statistics and other numeric description of data—those are still invaluable for us.\nRather, cases like Datasaurus or Anscombe’s quartet highlight the necessity of understanding the shape of your data. This will determine the kind of questions you can ask with the data, as well as the kind of statistical tools you need to describe it.\nFor example, in the case we just examined, those x and y variables do not have any kind of clear linear relationship. In that case, tools like regression that assume linearity are not appropriate. Any relationship between the variables could only be explored through other statistical means.\nSo, making our figures and tables look aesthetically pleasing is indeed valuable in its own right, but don’t underestimate the utility of good visualization for the analytic process itself.",
    "crumbs": [
      "Day 5: Making Better Visualizations",
      "Introduction"
    ]
  },
  {
    "objectID": "univariate_numeric.html",
    "href": "univariate_numeric.html",
    "title": "5  Univariate: Numeric",
    "section": "",
    "text": "5.1 Frequency Distributions\nWhile we also want to report on the frequency distribution and central tendency of numeric variables, there are some differences in the way we go about that relative to the techniques we just learned about for categorical variables.\nAdditionally, we will need to report on some measures of dispersion that can only be reported for numeric data, such as the standard deviation.\nThis section will walk us through the process of calculating and reporting necessary elements of a univariate analysis of numeric data.\nAs we saw when we were recoding age earlier in the unit, a frequency table is not really appropriate for numeric data. When there are upwards of 71 different response values—as we have with age—a table would be both spatially unwieldy and difficult to interpret.\nSo, let’s start with a style of plotting data that is essentially the barplot of numeric data.\nFirst, let’s go ahead and set up our environment by loading in our data and tidyverse per usual.\nlibrary(tidyverse)\nload(\"our_gss.rda\")",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#frequency-distributions",
    "href": "univariate_numeric.html#frequency-distributions",
    "title": "5  Univariate: Numeric",
    "section": "",
    "text": "5.1.1 Histograms\nA histogram is the result of taking a numeric variable, slicing it up into equal, ordinal intervals, and then plotting the frequency of each interval.\nThere are plenty of other ways for plotting numeric data—some of which we will see later when we focus on visualization—but for now, this is a simple and effective way to get a sense of the distribution of a numeric variable. This is especially important when we need to decide on an appropriate measure of central tendency.\nWe can create these easily using the hist() function, which is in the same family of base R plotting functions as barplot(), so much of what we learned for that function will also apply with hist(). This function will automatically apply a commonly used method called Sturges’s rule to divide our numeric variable into equal bins, so we do not have to specify anything in that regard.\nAll we need to do is give the hist() function our numeric variable. Let’s work with the realrinc variable for this example.\n\nhist(our_gss$realrinc)\n\n\n\n\n\n\n\n\nMuch like we did with barplot(), we can clean this up a little by adding a title and a better label for the x axis\n\nhist(\n  our_gss$realrinc,\n  main = \"Income distribution of GSS Respondents\",\n  xlab = \"Respondent's income in dollars\",\n  )\n\n\n\n\n\n\n\n\nNow we have our histogram!\nHowever, this visualization does suggest that we need to do some thinking about the appropriate measure of central tendency for realrinc.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#central-tendency",
    "href": "univariate_numeric.html#central-tendency",
    "title": "5  Univariate: Numeric",
    "section": "5.2 Central Tendency",
    "text": "5.2 Central Tendency\nFor numeric data, any of the 3 common measures of central tendency can be reported. However, the preferred measure for a given variable depends on its distribution.\nThe mean is the gold standard for numeric data, because it takes into account every single data point in its calculation, and is thus the most comprehensive index of central tendency.\nHowever, the mean can be misleading in some cases. The distribution should be (basically) normal in order for you to report the mean, and you will want to report the median in most other cases.\nI will first cover the choice of mean or median by revisiting some basics of distributions, and then I will address the rare circumstance where the mode is appropriate.\n\n5.2.1 Mean or Median?\nIn most cases, we will be making a decision between the mean and the median. When the distribution is normal, you should report the mean—otherwise, report the median. We’ll see one exception to this later, but that’s generally our task.\nWe saw some typical distributions in our last section on uivariate analysis of categorical data. We’ll return to some discussion of those basic distribution types but add a little more context for numeric data that will help us see the impact of making the wrong decision.\nI’m going to display a normal distribution, but this time, I’ll draw a vertical line indicating the location of the mean and median.\n\n\n\n\n\n\n\n\n\nIn this case, we can’t even really tell the mean and median apart. In fact, in a completely normal distribution, all three measures of central tendency will be equal. Remember that real-world data will rarely exhibit perfect normality, but as long as it’s approximately normal, we can follow convention and report the mean.\nNow, let’s look back at realrinc.\n\n\n\n\n\n\n\n\n\nIn the case of realrinc, we definitely do not have much normality going on. For one, there are some apparent outlier cases. Much of our data seems to be concentrated within about $0 - $75,000, but then we have a bunch of values at roughly double the maximum of that narrower range. And even within the range where most of our responses are clustered, we have some clear right-skew.\nAs we can see, because the mean takes all values into account, this makes it susceptible to non-normal distributions. It gets pulled in the direction of the outlier, which, in this case, is also the direction of the skew.\nSo, in any case where your numeric variable contains clear outliers and/or exhibits notable skew in either direction, you should report the median rather than the mean.\n\n\n5.2.2 The Mode\nBy and large, the mode is more appropriate for categorical data. But there are some circumstances where it makes sense to report the mode for numeric data.\nLet’s take a look at the original age variable to see an example.\n\nhist(\n  our_gss$age,\n  main = \"Age distribution of 2022 GSS Respondents\",\n  xlab = \"Respondent age in years\"\n)\n\n\n\n\n\n\n\n\nThis isn’t quite a normal distribution, but it’s not exactly skewed in either direction either. And there are no apparent outliers.\nWhat we have here is a bimodal distribution.\nThis is what happens when we have multiple peaks in a distribution—two, in this case. We have seen several different distributions so far, but they all had only one peak in the distribution. They were differentiated on the basis of that single peak’s location in the distribution. In the case of a multimodal distribution, we want to report on any notable peak.\nThis may be a peculiarity of the 2022 survey wave, because we wouldn’t necessarily expect this, but it looks like there are two distinct central tendencies for age. In this case, we want to capture the two distinct peaks, meaning we need to calculate two modes.\nThankfully, there’s a convenient function for this, but we need to install a new package, so let’s install and load in multimode.\n\ninstall.packages(\"multimode\")\nlibrary(multimode)\n\nThen, we just need to provide a few arguments to the locmodes() function.\nFirst, we give the variable for which we want to calculate multiple modes.\nThen we give an input for mod0, which asks us how many modes the function should be looking for. This should be based on visual inspection of the data. We have two peaks in our distribution, so we will enter ‘2’ for this input.\nLastly, we will set display to ‘TRUE’, which will show us a plot of the estimated modes superimposed onto the frequency distribution of the variable. This will let us evaluate whether the modes estimated by the function are plausible. They should match up with the visible peaks in the frequency distribution.\n\nlocmodes(\n  our_gss$age,\n  mod0 = 2,\n  display = TRUE\n)\n\n\n\n\n\n\n\n\n\nEstimated location\nModes:  32.4301  66.25356 \nAntimode: 50.89506 \n\nEstimated value of the density\nModes: 0.01938966  0.01730832 \nAntimode: 0.01444287 \n\nCritical bandwidth: 2.543659\n\n\nWe only really need to pay attention to the modes that it gives us (32.43 and 66.25), but it looks like this function spotted these values pretty effectively.\nFor a bimodal distribution like this, we can report both modes for the measure of central tendency. I don’t expect we will run into too much of this, but go ahead and deal with it like this in the event that you do.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#dispersion",
    "href": "univariate_numeric.html#dispersion",
    "title": "5  Univariate: Numeric",
    "section": "5.3 Dispersion",
    "text": "5.3 Dispersion\nNow, let’s talk about dispersion. This is the last major element of a univariate analysis of numeric data. Actually calculating this information is quite simple in R, so we will practice with some of our GSS variables and then talk about putting all of this information together.\n\n5.3.1 Range\nThis is perhaps the simplest measure of dispersion and comprises the minimum and maximum values of the distribution.\nR has a built in range function, so we can simply provide one of our variables as the input. We’ll work with realrinc again. As we have done before, we will also need to provide na.rm = TRUE, as this variable column includes NAs and will confuse R otherwise.\n\nrange(our_gss$realrinc, na.rm = TRUE)\n\n[1]    204.5 141848.3\n\n\nAt this point, I’ll also offer a reminder about the summary() function, which we can use to see a variety of information about the dispersion (in an addition to the central tendency).\n\nsummary(our_gss$realrinc)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n   204.5   8691.2  18405.0  27835.3  33742.5 141848.3     1554 \n\n\nFor a numeric variable, summary() will show us the minimum & maximum, as well as the mean & median, and the 1st & 3rd quartiles.\nWe can actually think of most of these elements in terms of percentiles.\n\nThe minimum value is the 0th percentile of the distribution\nThe 1st quartile is the 25th percentile.\nThe median is the 50th percentile\nThe 3rd quartile is the 75th percentile\nAnd the maximum is the 100th percentile.\n\nsummary() is great for quickly assessing some of these descriptive statistics, but it’s a little less convenient for exporting this information into something like a table for our own univariate analysis.\nSo, I’ll also highlight the min() and max() functions, which will come in hand for us shortly. They work simply enough—we just need to provide our variable column, and they will output the minimum and maximum, respectively.\n\nmin(our_gss$realrinc, na.rm = TRUE)\n\n[1] 204.5\n\nmax(our_gss$realrinc, na.rm = TRUE)\n\n[1] 141848.3\n\n\n\n\n5.3.2 Standard Deviation\nThis is one of the more commonly reported dispersion metrics for numeric data. The standard deviation is a measure of how much the average value varies from the mean. In plainer terms, it’s a measure that tells us how spread out the distribution is.\n\n\n\n\n\n\n\n\n\nWe can grab the standard deviation quite easily with the sd() function\n\nsd(our_gss$realrinc, na.rm = TRUE)\n\n[1] 31962.34\n\n\nNow, let’s talk about putting all this together in our reporting of a univariate analysis for numeric data.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  },
  {
    "objectID": "univariate_numeric.html#putting-it-all-together",
    "href": "univariate_numeric.html#putting-it-all-together",
    "title": "5  Univariate: Numeric",
    "section": "5.4 Putting It All Together",
    "text": "5.4 Putting It All Together\nNow that we have worked through how to calculate all of these key statistics with R, let’s revisit some of the first tidyverse functions we learned about back on the first day.\nWe can use a combination of select() and summarize to quickly compile several bits of important information, such as the range, the central tendency, and the standard deviation.\nLet’s do this for realrinc.\n\nour_gss |&gt;\n  select(realrinc) |&gt;\n  summarize(\n    \"Minimum\" = min(realrinc, na.rm = TRUE),\n    \"Median\" = median(realrinc, na.rm = TRUE),\n    \"Maximum\" = max(realrinc, na.rm = TRUE),\n    \"SD\" = sd(realrinc, na.rm = TRUE)\n  )\n\n  Minimum Median  Maximum       SD\n1   204.5  18405 141848.3 31962.34\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might notice that there’s an attribute for each column that looks like &lt;dbl&gt;. This is short for ‘double’, which is itself shorthand for ‘double-precision floating-point format’. This is in reference to the underlying data type that R uses to store continuous numeric values. You really don’t need to know anything more about double-precision floating point than that for our purposes, but I mention it here because you might run into  or ‘double’ in R. When you do, just think ‘numeric data’.\n\n\nIf you have a numeric variable where the mean is more appropriate, you can just swap median() with mean() in the code template above. Be sure to also change the column name as well.\nAnd that should do us! For any univariate analysis you report in this course, you will just need to produce\n\na histogram displaying the variable’s frequency distribution\na table like the one above (with the appropriate measure of central tendency).",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate: Numeric</span>"
    ]
  }
]