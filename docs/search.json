[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to R for Quantitative Social Science Research",
    "section": "",
    "text": "Preface\nWelcome to this introductory R tutorial for SOC 300!\nHere you will find all the step-by-step instructions for completing our initial foray into R for quantitative analysis of social science data. We will begin by establishing some common ground in basic R operations and functionality. After we lay this foundation, we will progress through various data processing tasks—from importing and cleaning public data to visualizing and analyzing these data for consumption by interested stakeholders.\nYou will receive an R script file with the commands detailed here, so that you can easily run and manipulate them on your own device, but you will always be able to refer to this mini-textbook in the event that you would like to see everything in one place and refer to some more detailed documentation on various R operations.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Background",
    "section": "",
    "text": "What is R?\nBefore we start exploring some of R’s basic functionality, I’m going to set the stage a little on what R and R studio are, why we are using these tools in particular, and what we will need to know before we dig in.\nAt it’s core, R is a programming language. There’s a lot to say about this from a computer science perspective, but, for our purposes, you can just think of R as a language with a very particular structure that’s designed to tell our computers what to do.\nThere are all sorts of different programming languages out there, and they all offer certain benefits or cater to particular computing needs. Unlike some general purpose languages like C, C++, or Python, R is relatively specialized, and this is part of what makes it so useful for us. R is designed with statistical computing as a primary motivator, and now—roughly 30 years into its tenure—stands as one of the most widely adopted resources for statistical data science in the social sciences and beyond.\nThough there can be a bit of a learning curve when getting used to R, we will focus on exactly the things that we need and build ourselves up slowly. Once you get used to it, R will allow you to perform incredibly complex statistical procedures with relative ease, and it can even help us with other related tasks like visualizing and presenting our analyses.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "background.html#what-is-r-studio",
    "href": "background.html#what-is-r-studio",
    "title": "Background",
    "section": "What is R Studio?",
    "text": "What is R Studio?\nWe are going to pair R with the software R Studio, which is what’s known as an Integrated Development Environment (IDE). Programming languages can be leveraged in a number of different ways. You could run R commands entirely from a Windows command line or Mac terminal. But that would probably not be very ideal for us—not to mention sort of ethereal and frustrating for those of without any programming experience. IDEs provide user-friendly interfaces for working with programming languages, so that we can easily manage our code, quickly generate and view the output of our analyses, and generally keep track of what we are doing with R. There are lots of other IDEs out there, but R Studio is an ideal balance of ease and power, so it will serve as our IDE of choice.\nThe best way to think about R Studio’s relationship to R is by framing it as an analogy with a desktop computer. R Studio is to R as a monitor is to a computer. R is the thing that’s doing all the heavy lifting computationally, and R Studio is the thing that allows us to view and interact with R in a way that’s simple and straightforward.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "background.html#why-r",
    "href": "background.html#why-r",
    "title": "Background",
    "section": "Why R?",
    "text": "Why R?\nUltimately, R is just one of several different options we could have gone with for a course like this. STATA, SPSS, Python, and even MS Excel are used with regularity for quantitative analysis in academia and industry. However, R has a few advantages that make it well-suited for us.\nWhile software like Excel, SPSS, and STATA arguably have more accessible, user-friendly interfaces, the upper limit of their capabilities is far lower than R. On the other end of the spectrum, Python is a little overkill for our purposes. While it has some excellent resources for data science, it’s also used for a wider variety of programming tasks related to web- and software development. I once heard a computational sociologist joke that using Python for something that can be done in R is like using a nuke when all you need is a hammer. While R is not quite as expansive as Python, it’s specialization in statistical computing provides a helpful balance for us when it comes to our goals for the course.\nAnother big perk of R is that it is completely free. This is true for Python as well, but SPSS, STATA, and Excel all require the purchase of a license, and some of them can get very pricey. You have access to all of these programs as an NC State student, but you will be able to use R regardless of whether you are on an NC State computer or even enrolled in the university at all. Relatedly, R is completely open-source—you can view it’s source code, and even modify it for your own purposes. This makes R incredibly customizable, and this open-source culture has brought about a dedicated community of researchers and data scientists who regularly contribute new functional add-ons to R.\nLastly, R is quite marketable as a technical skill. Especially for those who want to go on to do research of any kind, experience with R will likely be seen as a plus. I’ve been on the lookout for various research, teaching, and industry jobs as I get ready to enter the job market, and I see calls for R as a required or preferenced skill all the time.\nIn sum, R provides us with the ideal balance of computing power and feasibility while helping keep our pockets full and giving us some skills that translate well beyond the course.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "background.html#acquiring-r-and-r-studio",
    "href": "background.html#acquiring-r-and-r-studio",
    "title": "Background",
    "section": "Acquiring R and R Studio",
    "text": "Acquiring R and R Studio\nAll of the CHASS computers (and likely most NC State computers) come with R and R Studio, so you do not need to download them, but you may find it convenient to work on R assignments using your own personal device, so I’ve provided some instructions below.\nNote that you will want to install R first,\n\nDownloading R\nYou can download R from the Comprehensive R Archive Network (CRAN).\nWhen you click that link, you will arrive at CRAN’s homepage. Navigate to the sidebar on the left, find ‘Download’ near the top, and then click ‘CRAN’. This will take you to the ‘mirrors’ page. Mirrors are just different host locations for downloading the R installation files. This allows you to maximize download speed by choosing a nearby server, so scroll down to ‘USA’ and choose one of those (I usually opt for the Durham, NC mirror).\nUnless you are very experienced with computers, you should download one of the options listed as ‘pre-compiled binary distributions’. These will typically be the first options listed. Don’t even worry about what that means if you’re not familiar. Just choose the one that reflects your operating system (there are options for Windows, Mac, and Linux) That should download an R installer, and you can follow the directions to complete a default installation.\n\n\nDownloading R Studio\nR Studio is a little more straightforward to download. Just navigate to its homepage, scroll down a little, and you will find a big button that says ‘Download R Studio for [your operating system]’. Go ahead and run the installer with the default settings.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "1  R Fundamentals",
    "section": "",
    "text": "1.1 Basic Operations\nIn this first section, we are going to start from the ground up and start to familiarize ourselves with the way R works and what it expects from us. We will begin with the most basic building blocks of R data and work our way up to the data frame—the object that will be most relevant for us. While you won’t generally need to build data frames from scratch within R for your own research, it’s a good way to familiarize yourself with the structure of data in R. While we will start here with a rather simple data frame, all the principles you learn here will scale up as we start to work with much larger and more complex data frames.\nFirst, we will start by exploring some of the basic characteristics of R.\nR can be used as a simple calculator and will process both numbers and conventional mathematical operator symbols. You can run the commands below by placing your cursor at the beginning or end of the line in your script file and pressing CTRL+Enter (Windows) or Command+Return (Mac)\n5+2\n\n[1] 7\nYou should see the result displayed in the console below.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "getting_started.html#storing-objects",
    "href": "getting_started.html#storing-objects",
    "title": "1  R Fundamentals",
    "section": "1.2 Storing Objects",
    "text": "1.2 Storing Objects\nR is especially helpful for allowing us to create and store objects that we can call and manipulate later. We can create names for these objects and then use R’s ‘assignment operator,’ the &lt;- symbol, to assign a value to our specified object name. Here, we’ll assign the previous calculation to an object that we are calling our_object.\nIf you run this command on your own device, you should see our_object populate in the upper-right Environment window. This is where you can find all of the objects that you create in your R session. We can run the object itself, as well as combine it with other operations\n\nour_object &lt;- 5+2\n\nThere are some more baroque ways around this, but it’s best to operate under the impression that object names cannot include spaces (or start with numbers). This kind of thing is common in some programming languages, so there are a couple stylistic conventions to address this. I tend to use what’s called ‘snake case,’ which involves replacing spaces with underscores. There’s also ‘camel case,’ where each word has the first letter capitalized, e.g. MyVariableName. I would settle on one that you like and be consistent with it.\n\nour_object\n\n[1] 7\n\nour_object + 3\n\n[1] 10\n\nour_object * 100\n\n[1] 700",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "getting_started.html#a-note-on-functions",
    "href": "getting_started.html#a-note-on-functions",
    "title": "1  R Fundamentals",
    "section": "1.3 A Note on Functions",
    "text": "1.3 A Note on Functions\nR is also useful for its implementation of functions, which you can think of in the sense you likely learned in your math classes. Functions are defined procedures that take some input value, transform that value according to the procedure, and then output a new value.\nR comes with a great deal of already defined functions, and we can use these to perform all sorts of helpful operations. You can call a function by indicating it’s common name and then placing it’s required inputs between parentheses, e.g. function_name(input).Note that function inputs are also often referred to as ‘arguments’. We’ll get a lot of mileage out of functions, and part of the initial learning curve of R will be related to getting used to the range of available functions and the syntax you must follow to call them.\nNow, let’s take a step back and think about some of our basic building blocks in R.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "getting_started.html#vectors-and-r-data-types",
    "href": "getting_started.html#vectors-and-r-data-types",
    "title": "1  R Fundamentals",
    "section": "1.4 Vectors and R Data Types",
    "text": "1.4 Vectors and R Data Types\nYou can think of vectors as ordered sets of values. We can use the c() function (short for ‘combine’) to create a vector made up of the values we provide. Let’s make a few different vectors—each one will have 5 separate items in it, and we separate those items with commas. Note that when we want R to process something as text (and not a named object, number, or function), we put it in quotation marks.\n\nnum_vec &lt;- c(1.2, 3.4, 5.6, 7.1, 2.8)\n\ncharacter_vec &lt;- c(\"east\", \"west\", \"south\", \"south\", \"north\") \n\nlogical_vec &lt;- c(TRUE, FALSE, TRUE, FALSE, FALSE) \n\nLet’s talk a bit about what we have here. Each of these vectors represents a data type in R, or, in other words, one of the basic ways in which R stores data. There are some more data types out there, but these are the most most relevant for us.\n\nNumeric Data: As the name suggests, this is the typical fashion in which numbers are stored in R. Numeric data encompasses both continuous values and discrete values. These are essentially numbers that can have decimal places vs. integers (whole numbers).\nCharacter Data: Character here refers to the idea of character strings. This is typically how R stores text data—as distinct strings of text. Note that, while numbers are typically processed as numeric by R, numbers can also become character data if you place them between quotation marks.\nLogical Data: In R syntax, upper-case ‘true’ and ‘false’ have fixed values and, when used without quotes, will refer to these pre-defined logical values. We probably won’t use this data type much for analyses, but we will run into them in other places. They can be useful for sorting and searching through subsets of data, and we will also use logical values to turn certain procedures on or off in some functions.\n\nMany R functions will respond differently to different data types, so it’s important to keep these in mind when you need to troubleshoot errors.\nTake the mean() function, for example. As the name implies, this function will return the arithmetic mean of a numeric vector. Let’s give it the one we just made above:\n\nmean(num_vec)\n\n[1] 4.02\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n(1.2+3.4+5.6+7.1+2.8)/5\n\n[1] 4.02\n\n\nObserve that mean() gives the same response as if we had manually calculated it. Functions can make our lives a lot easier with larger amounts of data, but always make sure you’re familiar with what’s going on under the hood of any given function.\n\n\nBut, what happens when we run the following command?\n\nmean(character_vec)\n\nWarning in mean.default(character_vec): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\n\nIt doesn’t make any sense to take the mean of the cardinal directions, so it will throw a warning message. We need a variable that can be represented numerically. As we’ll see, it’s a good habit to make sure you know the data type of your variables before you begin your analysis.\nNow that we’ve talked about some of these basic building blocks for data, let’s talk about putting them together.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "getting_started.html#data-frames",
    "href": "getting_started.html#data-frames",
    "title": "1  R Fundamentals",
    "section": "1.5 Data Frames",
    "text": "1.5 Data Frames\nFor the most part, we will be working with data frames. These are collections of data organized in rows and columns. In data science, it’s generally preferable for data to take a particular shape wherein each row indicates a single observation, and each column represents a unique variable. This is called the ‘tidy’ data format.\n\n1.5.1 Building a Data Frame\nLet’s use the vectors we created above to mock up a little data frame. We will imagine some variables that those vectors could represent. But first, let’s make a couple more vectors.\nLet’s add a vector of participant IDs associated with imaginary people in our mock data set. In accordance with tidy data, each of our rows will then represent a unique person. The column vectors will represent the variables that we are measuring for each person. Lastly, the individual cells will represent the specific values measured for each variable.\nFor reasons that will become clear in the next section, we are also going to add one more character vector.\n\np_id_vec&lt;-c(\"p1\", \"p2\", \"p3\", \"p4\", \"p5\")\n\nordinal_vec&lt;-c(\"small\", \"medium\", \"medium\", \"large\", \"medium\")\n\nNow, let’s use a function to create a data frame and store it in a new object.\nWe can use data.frame() for this. data.frame() expects that we will give it some vectors, which it will then organize into columns. We could just give it the vectors, and it would take the vector names as column names, e.g.:\n\nour_df &lt;- data.frame(p_id_vec, num_vec, character_vec, ordinal_vec, logical_vec)\n\nOr we could specify new variable names and use the = sign to associate them with the vector. We will go with this latter strategy because our current vector names do not translate well to variable names.\nWe’ll imagine building a small data frame of dog owners and rename our vectors accordingly.\n\nour_df&lt;-data.frame(\n  p_id = p_id_vec,\n  dog_size = ordinal_vec,\n  side_of_town = character_vec,\n  food_per_day = num_vec, \n  has_a_labrador = logical_vec\n)\n\n\n\n\n\n\n\nTip\n\n\n\nAs a slight tangent, note that we can use line breaks to our advantage with longer strings of code. The above command is identical to the one below, but some find the line-break strategy more intuitively readable. It’s most important that your code works, so you don’t have to organize it like that, but know that’s an option\n\nour_df &lt;- data.frame(p_id = p_id_vec, dog_size = ordinal_vec, side_of_town = character_vec, food_per_day = num_vec, has_a_labrador = logical_vec)\n\n\n\nNow our vectors make up meaningful variables in our mock data frame.\n\np_id = An ID for each participant in our survey of dog owners\ndog_size = Owner’s ranking of their dog’s size\nside_of_town = Which part of town the owners reside\nfood_per_day = The amount of food each owner feeds their dog daily (in ounces)\nhas_a_labrador = true/false indicator for whether the owner has a lab or not\n\n\n\n1.5.2 Examining our Data Frame\nTake a look at our new data frame by clicking on the object in our Environment window at the upper right, or by running the command View(our_df).\nOnce we have created a data frame, we can refer to individual variable vectors with the $ operator in R\n\nour_df$food_per_day\n\n[1] 1.2 3.4 5.6 7.1 2.8\n\nmean(our_df$food_per_day)\n\n[1] 4.02\n\n\nWe can look at some basic characteristics of our variables with the summary() function. Note that it will return different information depending on the data type of the variable\n\nsummary(our_df)\n\n     p_id             dog_size         side_of_town        food_per_day \n Length:5           Length:5           Length:5           Min.   :1.20  \n Class :character   Class :character   Class :character   1st Qu.:2.80  \n Mode  :character   Mode  :character   Mode  :character   Median :3.40  \n                                                          Mean   :4.02  \n                                                          3rd Qu.:5.60  \n                                                          Max.   :7.10  \n has_a_labrador \n Mode :logical  \n FALSE:3        \n TRUE :2        \n                \n                \n                \n\n\nLet’s think about these for a second.\nThe summary of has_a_labrador makes sense. It’s recognized as a logical vector and tells us the number of TRUEs and FALSEs\nfood_per_day works as well. We’re dealing with a continuous variable that allows for decimal places, so it makes sense to take the mean and look at the range and distribution.\nBut how about side_of_town? What that summary tells us is that this variable is a character type (or class). ‘Length’ refers to the size of the vector. So, a vector containing 5 items would be a vector of length 5. But does it make sense for us to treat the side_of_town variable as 5 totally separate strings of characters?\n\nsummary(our_df$side_of_town)\n\n   Length     Class      Mode \n        5 character character \n\n\nNot quite. When we have two entries of “south”, for example, we want those responses to be grouped together and not treated as unique entries.\n\nour_df$side_of_town\n\n[1] \"east\"  \"west\"  \"south\" \"south\" \"north\"\n\n\nFor this, we will want another key R data type.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "getting_started.html#factors",
    "href": "getting_started.html#factors",
    "title": "1  R Fundamentals",
    "section": "1.6 Factors",
    "text": "1.6 Factors\n\n1.6.1 Unorderd Factors\nFactors are often the best way to treat categorical variables (nominal or ordinal) in R. Factors are a certain kind of vector that can only contain a number of pre-defined values. Each of these pre-defined values is considered a ‘level’ of the factor. So, we want side_of_town to be a factor variable with 4 levels: east, west, south, and north.\nWe can turn this variable into a factor with R’s as.factor() function.\n\nour_df$side_of_town &lt;- as.factor(our_df$side_of_town)\n\nCheck the summary() output again and notice how the output is reported now. Instead of simply listing that the vector contained 5 character strings, we can now see the different levels and the number of people who belong to each side of town.\n\nsummary(our_df)\n\n     p_id             dog_size         side_of_town  food_per_day \n Length:5           Length:5           east :1      Min.   :1.20  \n Class :character   Class :character   north:1      1st Qu.:2.80  \n Mode  :character   Mode  :character   south:2      Median :3.40  \n                                       west :1      Mean   :4.02  \n                                                    3rd Qu.:5.60  \n                                                    Max.   :7.10  \n has_a_labrador \n Mode :logical  \n FALSE:3        \n TRUE :2        \n                \n                \n                \n\n\n\n\n1.6.2 Ordered Factors\nNow, let’s think about dog_size. This should clearly be a factor variable as well. But, unlike food_per_day, the levels of this variable have an apparent order, from small to large.\nThe factor() function allows us to turn a vector into a factor, as well as manually specify the levels. Additionally, we can activate a process in the function letting it know that we want the order to matter.\n\nour_df$dog_size &lt;- factor(\n  our_df$dog_size, \n  levels=c(\"small\", \"medium\", \"large\"),\n  ordered = TRUE \n  )\n\nTake a look back at the summary. Now, instead of 5 separate character strings, we can see the breakdown of how many people have a dog of a certain size.\n\nsummary(our_df)\n\n     p_id             dog_size side_of_town  food_per_day  has_a_labrador \n Length:5           small :1   east :1      Min.   :1.20   Mode :logical  \n Class :character   medium:3   north:1      1st Qu.:2.80   FALSE:3        \n Mode  :character   large :1   south:2      Median :3.40   TRUE :2        \n                               west :1      Mean   :4.02                  \n                                            3rd Qu.:5.60                  \n                                            Max.   :7.10                  \n\n\nNote that the str() command is also useful for quickly gleaning the various data types of variable columns within a data frame. It will show us our variable names, the data types, and then a preview of the first several values in each variable column.\nWe can also verify that dog_size has been successfully re-coded as an ordered factor.\n\nstr(our_df)\n\n'data.frame':   5 obs. of  5 variables:\n $ p_id          : chr  \"p1\" \"p2\" \"p3\" \"p4\" ...\n $ dog_size      : Ord.factor w/ 3 levels \"small\"&lt;\"medium\"&lt;..: 1 2 2 3 2\n $ side_of_town  : Factor w/ 4 levels \"east\",\"north\",..: 1 4 3 3 2\n $ food_per_day  : num  1.2 3.4 5.6 7.1 2.8\n $ has_a_labrador: logi  TRUE FALSE TRUE FALSE FALSE\n\n\nThere are cases where you will want to convert a column like p_id to a factor variable as well, but often we just need a variable like p_id to serve as a searchable index for individual observations, so we can leave it be for now.\nThis is all part of the process of data cleaning, where we make sure our data is structured in a fashion that’s amenable to analysis. This re-coding of variables is an essential component, and we’ll see plenty more tasks in this vein when we work with GSS data later on.\nAs we close this section, here is a figure to help you internalize the hierarchy of variable types based on the levels of measurement. The bottom level of the hierarchy (in green) reflects the R data type that is best aligned with a particular measurement level. Also recall that numeric data can either be interval or ratio, though we will generally treat these similarly.\n\n\n\nA hierarchy of variables and their corresponding R data types\n\n\nFor our last bit, let’s learn a little about working with functions that don’t come included in base R.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals</span>"
    ]
  },
  {
    "objectID": "packages_and_tidy.html",
    "href": "packages_and_tidy.html",
    "title": "2  Packages and the Tidyverse",
    "section": "",
    "text": "2.1 Loading Packages\nR’s open-source culture has encouraged a rich ecosystem of custom functions designed by scientists and researchers in the R userbase. These come in the form of ‘packages’, which are suites of several related functions. For example, there are packages for conducting statistical tests, producing data visualizations, generating publication-ready tables, and all manner of other tasks.\nLet’s try this out with one of the better known R packages–‘tidyverse’. This is actually a collection of several packages with a variety of interrelated functions for ‘tidying’, visualizing, and analyzing data. We will focus on what we need from ‘tidyverse’, but, if you’re curious, you can read more here: https://www.tidyverse.org/\nIf you’re on a lab computer, this package may already be installed. Let’s check by running the following command:\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nIf you receive an error when you run this, you likely do not have the package installed on your system. This is also probably the case if you are on your personal device and only recently acquired R.\nIf you got an error, run the following command:\ninstall.packages(\"tidyverse\")\nWith a few exceptions, you will always install new packages in this fashion: install.packages(“package_name”)\nAfter it’s done installing, go back and run the library(tidyverse) command again. Note that you always need to do this for an added package. Whether you’ve had it for a while or just installed it, you need to load any outside package into your current session by placing its name in the library() function.\nlibrary(tidyverse)",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "packages_and_tidy.html#bringing-in-our-data",
    "href": "packages_and_tidy.html#bringing-in-our-data",
    "title": "2  Packages and the Tidyverse",
    "section": "2.2 Bringing in our Data",
    "text": "2.2 Bringing in our Data\nLet’s try bringing in a data frame to play with a few tidyverse functions. We’ll use the load() function to bring in a subset of the General Social Survey, which contains a few variables from the 2022 survey wave. Run the following command and select the file “our_gss.rda”\n\nload(file.choose())\n\nThe file.choose() function will open up a file-explorer window that allows you to manually select an R data file to load in. We’ll talk about some other ways to import data files using R syntax next time.\nGo ahead and take a look at the data frame. Each GSS survey wave has about 600-700 variables in total, so I’ve plucked several and done a little pre-processing to get us a subset to work with. All the variables here have pretty straightforward names, but I’ll note that realrinc is a clear outlier there. This is short for ‘Real respondent’s income’ and reflects the respondent’s income reported in exact dollar amounts. I’ll put a summary here so you can take a look if you’re not following along with your own script.\n\nsummary(our_gss)\n\n      year            id              age           race          sex      \n Min.   :2022   Min.   :   1.0   Min.   :18.00   white:2514   female:1897  \n 1st Qu.:2022   1st Qu.: 886.8   1st Qu.:34.00   other: 412   male  :1627  \n Median :2022   Median :1772.5   Median :48.00   black: 565   NA's  :  20  \n Mean   :2022   Mean   :1772.5   Mean   :49.18   NA's :  53                \n 3rd Qu.:2022   3rd Qu.:2658.2   3rd Qu.:64.00                             \n Max.   :2022   Max.   :3545.0   Max.   :89.00                             \n                                 NA's   :208                               \n    realrinc                                      partyid   \n Min.   :   204.5   independent (neither, no response):835  \n 1st Qu.:  8691.2   strong democrat                   :595  \n Median : 18405.0   not very strong democrat          :451  \n Mean   : 27835.3   strong republican                 :431  \n 3rd Qu.: 33742.5   independent, close to democrat    :400  \n Max.   :141848.3   (Other)                           :797  \n NA's   :1554       NA's                              : 35  \n           happy     \n not too happy: 799  \n pretty happy :1942  \n very happy   : 779  \n NA's         :  24",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "packages_and_tidy.html#data-wrangling-with-tidyverse",
    "href": "packages_and_tidy.html#data-wrangling-with-tidyverse",
    "title": "2  Packages and the Tidyverse",
    "section": "2.3 Data Wrangling with Tidyverse",
    "text": "2.3 Data Wrangling with Tidyverse\nLet’s use this subset to explore some tidyverse functionality. One of the packages included in the tidyverse is dplyr, which includes several functions for efficiently manipulating data frames in preparation for analyses. We will encounter a number of these throughout our time with R, but I want to briefly introduce a few key dplyr functions and operations that we will dig into more next time.\n\n2.3.1 select()\nIt happens quite often that we have a data frame containing far more variables than we need for a given analysis. The select() function allows us to quickly subset data frames according to the variable columns we specify.\nThis function takes a data frame as its first input, and all following inputs are the variable columns that you want to keep\n\nsex_inc &lt;- select(our_gss, id, sex, realrinc)\n\nYou should now have an object that contains all 3,544 observations, but includes only the 3 columns that we specified with select().\n\nsummary(sex_inc)\n\n       id             sex          realrinc       \n Min.   :   1.0   female:1897   Min.   :   204.5  \n 1st Qu.: 886.8   male  :1627   1st Qu.:  8691.2  \n Median :1772.5   NA's  :  20   Median : 18405.0  \n Mean   :1772.5                 Mean   : 27835.3  \n 3rd Qu.:2658.2                 3rd Qu.: 33742.5  \n Max.   :3545.0                 Max.   :141848.3  \n                                NA's   :1554      \n\n\n\n\n2.3.2 filter()\nfilter() functions similarly except that, instead of sub-setting by specific variables, it allows you to subset by specific values. So, let’s take the sex_inc object we just created above. We now have this subset of three variables—id, sex, and income—but let’s imagine we want to answer a question that’s specific to women.\nIn order to do that, we need to filter the data to include only observations where the value of the variable sex is ‘female’.\n\nfem_inc &lt;- filter(sex_inc, sex==\"female\")\n\nNote that the fem_inc object still has 3 variables, but there are now roughly half the observations, suggesting that we have successfully filtered out the male observations.\n\nsummary(fem_inc)\n\n       id           sex          realrinc       \n Min.   :   1   female:1897   Min.   :   204.5  \n 1st Qu.: 879   male  :   0   1st Qu.:  7668.8  \n Median :1783                 Median : 15337.5  \n Mean   :1772                 Mean   : 22702.1  \n 3rd Qu.:2664                 3rd Qu.: 27607.5  \n Max.   :3544                 Max.   :141848.3  \n                              NA's   :883       \n\n\n\n\n2.3.3 summarize()\nAs the name suggests, summarize() allows us to quickly summarize information across variables. It will give us a new data frame that reflects the summaries that we ask for, which can be very useful for quickly generating descriptive statistics. We will use this to get the mean income value for our data frame.\n\nmean_inc &lt;- summarize(our_gss, \"mean_inc\"=mean(realrinc, na.rm=TRUE))\n\n\n\n\n\n\n\nNote\n\n\n\nYou probably noticed the na.rm = TRUE input that I supplied for the above function. This is short for ‘remove NAs’, which we need to do when a variable has any NA values. If we don’t, R will screw up, because it does not know to disregard NA values when calculating a column mean unless we tell it to.\n\n\nThis gives us a new data frame that we called mean_inc. It should have 1 row and 1 column, and it just gives us the average income of a person in our GSS subset—about $28,000/year.\n\nmean_inc\n\n  mean_inc\n1 27835.33\n\n\nNow, this is not really all that impressive when we are asking for a broad summary like this. In fact, if all we wanted was to see the average income, we could get that more easily, e.g.\n\nmean(our_gss$realrinc, na.rm = TRUE)\n\n[1] 27835.33\n\n\nThe true power of summarize() comes from chaining it together with other tidyverse functions. However, in order to do that, we will need to learn about one more new R operation. I’ll show you that in a moment, but let’s take a look at one more helpful tidyverse function.\n\n\n2.3.4 group_by()\nOften when we’re using a function like summarize(), we want to get summaries for all kinds of different subgroups within our data set. For example, we may want the mean for each value of sex or partyid, rather than for all people in the data frame. We can do this with group_by.\nThis function may seem a little unusual when used in isolation, because it does not seem to do much on the surface.\n\nour_gss &lt;- group_by(our_gss, partyid)\n\nWhen you run that function, you will not generate any new objects, and you will not notice anything different about the data frame.\nWhat it does is overlay a grouping structure on the data frame, which will in turn affect how other tidyverse functions operate.\nCompare the output of summarize() run on this grouped version of our data frame with the use of summarize() above.\n\nmean_inc &lt;- summarize(\n  our_gss,\n  \"mean_inc\" = mean(realrinc, na.rm = TRUE)\n  )\n\nmean_inc\n\n# A tibble: 9 × 2\n  partyid                            mean_inc\n  &lt;fct&gt;                                 &lt;dbl&gt;\n1 strong democrat                      30677.\n2 independent (neither, no response)   21570.\n3 not very strong republican           29101.\n4 not very strong democrat             31743.\n5 independent, close to democrat       27916.\n6 other party                          23891.\n7 independent, close to republican     26825.\n8 strong republican                    32376.\n9 &lt;NA&gt;                                 16922.\n\n\nWe ran the same summarize() command as before, but now it reflects the grouping structure that we imposed.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "packages_and_tidy.html#the-pipe",
    "href": "packages_and_tidy.html#the-pipe",
    "title": "2  Packages and the Tidyverse",
    "section": "2.4 The Pipe",
    "text": "2.4 The Pipe\nThis one might be a little unintuitive, so don’t worry if it doesn’t immediately click. We will continue to get plenty of practice with it over the next couple of sessions.\nThe pipe operator looks like this: |&gt;. What it does is take whatever is to the left of the symbol and ‘pipe’ it into the function on the right-hand side. That probably sounds a little strange, so let’s see some examples.\nWe’ll refer back to our summarize() command from above.\n\nmean_inc &lt;- summarize(our_gss, \"mean_inc\"=mean(realrinc, na.rm=TRUE))\n\nThis is equivalent to…\n\nmean_inc &lt;- our_gss |&gt;\n  summarize(\"mean_age\" = mean(realrinc, na.rm=TRUE))\n\nNotice that, in the first command, the first input that we give summarize() is the data frame that we want it to work with.\nIn the command featuring the pipe operator, we supply the data frame and then pipe it into summarize(). The real magic comes from chaining multiple pipes together. This will likely take a little practice to get used to, but it can become a very powerful tool in our R arsenal.\n\n2.4.1 Putting It All Together\nLet’s illustrate with an example. I’ll let you know what I want to do in plain English, and then I will execute that desire with multiple piped commands.\nUltimately, I want to see the mean income, but I want to see the mean broken down by sex and partyid.\nSo, I want to take a selection of variables from our_gss. I want these variables to be grouped by sex and partyid. Finally, I want to see a summary of the mean according to this variable grouping.\n\nsexpol_means &lt;- our_gss |&gt;\n  select(id, sex, realrinc, partyid) |&gt;\n  group_by(sex, partyid) |&gt;\n  summarize(\"mean_inc\" = mean(realrinc, na.rm=TRUE)) |&gt;\n  drop_na(sex, partyid)\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can use drop_na() to do as the function’s name suggests. When we learned about group_by() above, you may have noticed that a mean was reported for an NA category within the partyid variable. Any time you notice this and want your summaries to exclude these NA categories, just include that variable as an input to drop_na().\n\n\n\nsexpol_means\n\n# A tibble: 16 × 3\n# Groups:   sex [2]\n   sex    partyid                            mean_inc\n   &lt;fct&gt;  &lt;fct&gt;                                 &lt;dbl&gt;\n 1 female strong democrat                      30111.\n 2 female independent (neither, no response)   17923.\n 3 female not very strong republican           22448.\n 4 female not very strong democrat             24300.\n 5 female independent, close to democrat       19961.\n 6 female other party                          26595.\n 7 female independent, close to republican     20095.\n 8 female strong republican                    21584.\n 9 male   strong democrat                      31600.\n10 male   independent (neither, no response)   25474.\n11 male   not very strong republican           34544.\n12 male   not very strong democrat             41233.\n13 male   independent, close to democrat       36947.\n14 male   other party                          22450.\n15 male   independent, close to republican     31393.\n16 male   strong republican                    40210.\n\n\nSo, using dplyr, we can quickly subset and manipulate data frames in just a few lines of relatively straightforward code. Here we have all the means for each value of sex and partyid, which would have been a tedious task had we calculated them all manually.\nWe will see plenty more on the tidyverse, so don’t fret if you don’t feel completely confident with these yet. It takes practice getting used to Rs peculiarities. We will keep building with these in the next unit and hopefully accumulate some muscle memory.",
    "crumbs": [
      "Day 1: Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "recoding_variables.html",
    "href": "recoding_variables.html",
    "title": "3  Recoding Variables",
    "section": "",
    "text": "3.1 Background\nToday’s venture concerns univariate analysis, i.e. the quantitative description of a single variable. Before we do that, however, we need to familiarize ourselves with some data-cleaning procedures.\nRecoding a variable involves manipulating the underlying structure of our variable such that we can use it for analysis. We did a little recoding during the last unit when we converted character vectors into factor variables. This allowed us to align R data types with the appropriate level of measurement.\nThere are also occasions when we need a variable to be translated from one level of measurement to another. For example, we may want to convert a ratio variable for “number of years of education” into an ordinal variable reflecting categories like “less than high school”, “high school diploma”, “Associates degree”, and so on.\nWe may also want to collapse the categories of ordinal variables for some analyses. Consider a variable with a Likert-scale agreement rating, where you responses like “strongly agree,” “moderately agree,” “slightly agree,” and so forth. You may decide to collapse these categories into categories of “Agree” and “Disagree”.\nWe will get some practice doing this sort of thing, which is an essential component of responsible analysis. Additionally, our next unit on bivariate analysis will require us to work with categorical variables in particular, so we need to be capable of converting any numeric variables.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Recoding Variables</span>"
    ]
  },
  {
    "objectID": "recoding_variables.html#converting-numeric-to-categorical",
    "href": "recoding_variables.html#converting-numeric-to-categorical",
    "title": "3  Recoding Variables",
    "section": "3.2 Converting Numeric to Categorical",
    "text": "3.2 Converting Numeric to Categorical\nWe will start by recoding age—a ratio variable—into an ordinal variable reflecting age groupings. The same strategies we use here will work for any numeric variable.\n\n3.2.1 Setting up our workspace\nAs usual, let’s make sure we load in tidyverse along with our GSS data.\n\nlibrary(tidyverse)\nload(\"our_gss.rda\")\n\nLet’s double check the structure of our data frame.\n\nstr(our_gss)\n\n'data.frame':   3544 obs. of  8 variables:\n $ year    : int  2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 ...\n $ id      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age     : int  72 80 57 23 62 27 20 47 31 72 ...\n $ race    : Factor w/ 3 levels \"white\",\"other\",..: 1 1 1 1 1 1 2 1 1 NA ...\n $ sex     : Factor w/ 2 levels \"female\",\"male\": 1 2 1 1 2 2 1 2 1 1 ...\n $ realrinc: num  40900 NA 18405 2250 NA ...\n $ partyid : Factor w/ 8 levels \"strong democrat\",..: 1 2 3 1 2 4 5 1 5 1 ...\n $ happy   : Ord.factor w/ 3 levels \"not too happy\"&lt;..: 1 1 1 1 2 2 2 2 2 2 ...\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might notice the ‘int’ category, which is short for ‘integer’. This is a subtype of numeric data in R. Variables that are exclusively whole numbers are often recorded in this way, but we can work with them in R just like we can other sorts of numbers\n\n\n\n\n3.2.2 The ‘age’ variable\nWe can take a look at all the values of age (along with the # of respondents in each age category) using the count() function.\n\ncount(our_gss, age)\n\nHowever, I’m not going to display those results here, as it will be a particularly long table of values (with 70+ different ages). It’s fine to run it—it won’t crash R or anything—but it will it clutter up this page. Let’s take this as a good opportunity to take advantage of the fact that we are using one of the better funded and well-organized surveys in all of social sciences. As such, there’s extensive documentation about all of the variables measured for the GSS. Go ahead and take a look at the age variable via the GSS Data Explorer, which allows us to search for unique variables and view their response values, the specific question(s) that was asked on the survey, and several other variable characteristics.\nThe responses range from 18 - 89 (in addition to a few categories for non-response). However, note that there’s something unique about value 89. It’s not just 89 years of age, but 89 & older. This isn’t a real issue for our purposes, but take this as encouragement to interface with the codebook of any publicly available data you use. There’s some imprecision at the upper end of this variable, and that might not be obvious without referencing the codebook.\nFor the purposes of this exercise, let’s go ahead and turn age into a simple categorical variable with 3 levels—older, middle age, and younger. I’m going to choose the range somewhat arbitrarily for now. We can use univariate analysis to inform our decision about how to break up a numeric variable, so we will revisit this idea again later on.\n\nYounger = 18 - 35\nMiddle Age = 36 - 49\nOlder = 50 and up\n\nAt the end of the day, what we need to do is 1.) create a new variable column 2.) populate that column with ordinal labels that correspond with each respondent’s numeric age interval.\n\n\n3.2.3 New columns with mutate()\nFirst, let’s consider the mutate() function. This function takes a data frame and appends a new variable column. This new variable is the result of some calculation applied to an existing variable column.\nLet’s look at an application fo mutate() to get a feel for it. Now, this wouldn’t be the best idea for a couple reasons, but, as an illustration, let’s say we wanted to convert our yearly income values to an hourly wage (assuming 40 hrs/week).\nmutate() takes a data frame as its input, and then we provide the name of our new variable column(s) along with the calculation for this new variable. Below, I use mutate() to create a new column called hr_wage. Then, I tell R that the hr_wage variable should be calculated by taking each person’s income value and diving that by 52 weeks in a year, and then 40 hours in a week.\n\n# Without the pipe operator\nour_gss &lt;- mutate(\n  our_gss,\n  hr_wage = (realrinc/52)/40\n  )\n\n\n# With the pipe operator\nour_gss &lt;- our_gss |&gt;\n  mutate(\n    hr_wage = (realrinc/52)/40\n  )\n\nTake a look at your new data frame object. I’ll show a summary here to verify that we got our new column.\n\nsummary(our_gss)\n\n      year            id              age           race          sex      \n Min.   :2022   Min.   :   1.0   Min.   :18.00   white:2514   female:1897  \n 1st Qu.:2022   1st Qu.: 886.8   1st Qu.:34.00   other: 412   male  :1627  \n Median :2022   Median :1772.5   Median :48.00   black: 565   NA's  :  20  \n Mean   :2022   Mean   :1772.5   Mean   :49.18   NA's :  53                \n 3rd Qu.:2022   3rd Qu.:2658.2   3rd Qu.:64.00                             \n Max.   :2022   Max.   :3545.0   Max.   :89.00                             \n                                 NA's   :208                               \n    realrinc                                      partyid   \n Min.   :   204.5   independent (neither, no response):835  \n 1st Qu.:  8691.2   strong democrat                   :595  \n Median : 18405.0   not very strong democrat          :451  \n Mean   : 27835.3   strong republican                 :431  \n 3rd Qu.: 33742.5   independent, close to democrat    :400  \n Max.   :141848.3   (Other)                           :797  \n NA's   :1554       NA's                              : 35  \n           happy         hr_wage        \n not too happy: 799   Min.   : 0.09832  \n pretty happy :1942   1st Qu.: 4.17849  \n very happy   : 779   Median : 8.84856  \n NA's         :  24   Mean   :13.38237  \n                      3rd Qu.:16.22236  \n                      Max.   :68.19631  \n                      NA's   :1554      \n\n\nSo, we can use mutate() to add a new column that contains our recoded variable. We just need a way to specify a calculation that takes into account the specific intervals we want for our ordinal labels. For this, we need one more function.\n\n\n3.2.4 Custom intervals with cut()\nWe can use the cut() function to specify the intervals we want for our age groupings, and then we will combine it with mutate() to generate our recoded age variable. Specifically, cut() takes our intervals and turns each of them into a level in a new factor variable.\nBut first, I want to give a little context on interval notation in mathematics. It will help us all be a little more precise when we talk about ranges, and it will also help us understand an input we need to provide for cut().\n\n3.2.4.1 An aside on intervals\nIn mathematic terms, an interval is the set of all numbers in between two specified end points. In formal notation, these are indicated with the two endpoints placed in square brackets, e.g [1,5] as the interval of 1 through 5.\nThere are some technical terms to describe whether or not we want to include the endpoints when we are talking about a particular interval.\n\nClosed interval: This is when both end points are included in the interval. Closedness is indicated with square brackets, so, the closed interval of 1 through 5 would be written just like I have above—[1,5]. This indicates any number ‘x’ where 1 &lt;= x &lt;= 5\nOpen interval: This is when neither endpoint is included in the interval. In interval notation, openness is indicated with parentheses rather than square brackets, so the open interval of 1 through 5 would be written as (1,5). This interval would indicate any number ‘x’ where 1 &lt; x &lt; 5\nLeft-open interval: This is when the left-hand endpoint is not included, but the right-hand endpoint is. This would be written as (1,5] in interval notation, and that interval would indicate any number ‘x’ where 1 &lt; x &lt;= 5\nRight-open interval: When the right-hand endpoint isn’t included but the left endpoint is, you have a right-open interval. This would be written as [1,5) in interval notation and would indicate any number ‘x’ where 1 &lt;= x &lt; 5.\n\nWe will be working with the right-open interval format, and we can specify that in cut().\nNow, let’s return to our task at hand.\n\n\n3.2.4.2 Inputs for cut()\nWe’ll go ahead and work with cut directly in mutate(), as its going to be the calculation that we are providing for the new column we generate with mutate().\nAs a reminder, here are the intervals we need:\n\nYounger = 18 - 35\nMiddle Age = 36 - 49\nOlder = 50 and up\n\nThe following code may look a little chaotic at first glance, but it’s really the same sort of thing that we did with mutate() above. It’s just that cut() is a little bit more involved of a calculation than the simple division we did in our example.\nJust like above, we are applying mutate() to our_gss, and we are giving the name of our new variable column (age_ord, in this case) as the first input. Then, for the calculation, we give the cut() function and the arguments it needs.\ncut() and its inputs\n\nThe variable for which we want to specify intervals (age)\nbreaks: This is where we indicate the intervals. The first number we give is the low end of our lowest age group (18). The second number is the low end of our middle age group (36). The third number, as you probably guessed, is the low end of our highest age group. The last value should reflect our upper limit. In this case, I use the value Inf, which is short for ‘infinity’. This essentially tells R that the last category can include any values higher than the previous number we entered.\ninclude.lowest: Putting TRUE here tells R that, in each interval, we want the lowest value to be included. If we don’t do this, then our ‘younger’ age grouping would be 19 - 35 rather than 18 - 35. In other words, setting this to TRUE indicates a left-closed interval, and FALSE indicates a left-open interval.\nright: This is the input for specifying whether we want this to be a right-closed interval, and it takes a logical value (TRUE or FALSE). We want a right-open interval, so we will set this to FALSE.\nlabels: R will actually default to formal interval notation for the names of each level of our new factor variable, so it would be [18,36), [36,50), [50, Inf). However, we can provide a character vector to specify custom labels for these new factor levels. In the event that you have an ordinal variable, make sure that you always specify these labels in ascending order. In this case, that would be Younger &lt; Middle Age &lt; Older.\nordered_result: This takes a logical value and, as the name suggests, indicates whether we want the factor to be ordered or not. In our case, there is a clear progression in terms of age, so we need to set this to TRUE.\n\n\nour_gss &lt;- our_gss |&gt;\n  mutate(\n    age_ord = cut(\n      age,\n      breaks = c(18, 36, 51, Inf),\n      include.lowest = TRUE,\n      right = FALSE,\n      labels = c(\"Younger\", \"Middle Age\", \"Older\"),\n      ordered_result = TRUE\n    )\n  )\n\nGo ahead and take a look at our_gss, and our new column should bring us to 10 variables instead of 9. I’ll use the str() function here so we can confirm that our factor was ordered and added to the data frame.\n\nstr(our_gss)\n\n'data.frame':   3544 obs. of  10 variables:\n $ year    : int  2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 ...\n $ id      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age     : int  72 80 57 23 62 27 20 47 31 72 ...\n $ race    : Factor w/ 3 levels \"white\",\"other\",..: 1 1 1 1 1 1 2 1 1 NA ...\n $ sex     : Factor w/ 2 levels \"female\",\"male\": 1 2 1 1 2 2 1 2 1 1 ...\n $ realrinc: num  40900 NA 18405 2250 NA ...\n $ partyid : Factor w/ 8 levels \"strong democrat\",..: 1 2 3 1 2 4 5 1 5 1 ...\n $ happy   : Ord.factor w/ 3 levels \"not too happy\"&lt;..: 1 1 1 1 2 2 2 2 2 2 ...\n $ hr_wage : num  19.66 NA 8.85 1.08 NA ...\n $ age_ord : Ord.factor w/ 3 levels \"Younger\"&lt;\"Middle Age\"&lt;..: 3 3 3 1 3 1 1 2 1 3 ...",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Recoding Variables</span>"
    ]
  },
  {
    "objectID": "recoding_variables.html#restructuring-categorical-variables",
    "href": "recoding_variables.html#restructuring-categorical-variables",
    "title": "3  Recoding Variables",
    "section": "3.3 Restructuring Categorical Variables",
    "text": "3.3 Restructuring Categorical Variables\nNow, let’s go ahead and tackle the other recoding situation I mentioned up at the top—collapsing the levels of categorical variables. We’ll work with partyid here, but the logic of this process will apply to any categorical variable.\n\n3.3.1 Collapsing categories\nI encourage you to take a look at partyid in the GSS Data Explorer, like we did for ‘age’..\nBecause partyid is a categorical variable, it has far fewer unique values than a ratio variable like age, so we can go ahead and take a look at all of its levels with the count() function.\n\n\n\n\n\n\nNote\n\n\n\nObserve that count(our_gss, partyid) basically provides the same information as summary(our_gss$partyid) when the variable is a factor. The only real difference is that count() organizes the info into a tidy data frame. In other words, you can use either function to take a look at factor variable like this.\n\n\n\ncount(our_gss, partyid)\n\n                             partyid   n\n1                    strong democrat 595\n2 independent (neither, no response) 835\n3         not very strong republican 361\n4           not very strong democrat 451\n5     independent, close to democrat 400\n6                        other party 106\n7   independent, close to republican 330\n8                  strong republican 431\n9                               &lt;NA&gt;  35\n\n\nLet’s go ahead and collapse these into categories of “Democrat”, “Independent”, “Other Party”, and “Republican”. We’ll use mutate() to make a new variable column called partyid_recoded. For the calculation of our new variable, we can use the fct_collapse() function. This function allows us to first give the name of a new factor level, and then we can give a character vector of the names of levels that want to be collapses into a single category.\nSo, to collapse our 3 Democrat levels into a single factor level, we would give the following input for fct_collapse():\n\"Democrat\" = c(\"strong democrat\", \"not very strong democrat\")\nAnd then repeat for each new factor level.\n\nour_gss &lt;- our_gss |&gt;\n  mutate(\n    partyid_recoded=fct_collapse(partyid, \n\"Democrat\" = c(\"strong democrat\", \"not very strong democrat\"),\n\"Republican\" = c(\"strong republican\",\"not very strong republican\"),\n\"Independent\" = c(\"independent, close to democrat\", \"independent (neither, no response)\", \"independent, close to republican\"),\n\"Other Party\" = c(\"other party\")\n))\n\nNow, let’s take a look at our new variable.\n\ncount(our_gss, partyid_recoded)\n\n  partyid_recoded    n\n1        Democrat 1046\n2     Independent 1565\n3      Republican  792\n4     Other Party  106\n5            &lt;NA&gt;   35\n\n\n\n\n3.3.2 Excluding levels\nYou may also want to work with an existing categorical variable, but only focus on certain values. In this case, we can create a recoded variable and simply code the levels we are uninterested in as NA.\nWe can use the fct_recode() function for this. For the first input, we will give it the factor-variable column that we want to recode. Then, we will let it know that we want to set the levels of “Other Party” and “Independent” to NULL. This will convert them to NAs and allow us to easily exclude them from our analyses. We will use fct_recode() within mutate() so we can create another recoded version of partyid_recoded. We’ll call this one dem_rep to distinguish it from the original partyid and our first recoded version.\n\nour_gss &lt;- our_gss |&gt;\n  mutate(dem_rep = fct_recode(\n    partyid_recoded, \n    NULL=\"Other Party\", \n    NULL=\"Independent\"))\n\nLet’s check to see that it worked.\n\ncount(our_gss, dem_rep)\n\n     dem_rep    n\n1   Democrat 1046\n2 Republican  792\n3       &lt;NA&gt; 1706\n\n\nNow that we have gotten all this pre-processing stuff out of the way, let’s go ahead and dig into some univariate analysis.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Recoding Variables</span>"
    ]
  },
  {
    "objectID": "univariate_categorical.html",
    "href": "univariate_categorical.html",
    "title": "4  Categorical Variables",
    "section": "",
    "text": "4.1 Frequency Distributions\nBefore we start examining the relationships between multiple variables, we have to look under the hood of our variables individually and make sure we have a good sense of what they are like.\nWe want to know the frequency distribution of the response values, the measures of central tendency, and the dispersion. In other words, we want to see how many respondents chose each response value, which response values are most typical for each variable, and the extent to which the response values vary.\nLuckily for us, R has plenty of tools for generating these quantitative summaries, as well as visualizing them in the form of barplots, histograms, and other common styles for displaying data.\nThis tutorial will cover each of these elements, and I will highlight any differences specific to certain levels of measurement along the way.\nLet’s start with categorical variables. We will start with frequency distributions by way of frequency tables and bar plots. Then, we will talk about measures of tendency and observe when they are appropriate for categorical data.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Categorical Variables</span>"
    ]
  },
  {
    "objectID": "univariate_categorical.html#frequency-distributions",
    "href": "univariate_categorical.html#frequency-distributions",
    "title": "4  Categorical Variables",
    "section": "",
    "text": "4.1.1 Frequency tables\nFrequency tables are the most effective way to report the frequency distributions for categorical variables. There are actually a ton of different ways to produce this kind of table in R, but we are going to use some convenient functions from the janitor package, so let’s go ahead and load that in.\nIf you are on a lab computer, try to load it with library() first. If you get an error, go ahead and install it with install.packages() and then load it in.\nWhile we’re at it, we’ll also bring in tidyverse and load in our GSS subset.\n\nlibrary(janitor)\nlibrary(tidyverse)\nload(\"our_gss.rda\")\n\nLet’s go ahead and work with one of our partyid recodes from last time. We’ll use partyid_recoded, where we retained all the party choices but collapsed the categories of degree (e.g. strong democrat + not very strong democrat = Democrat).\nWe can use a simple command with tabyl() from the janitor package to produce our frequency tables. Let’s take a look at some output, and I’ll go through a couple details and mention some further specifications we can make.\nAll we need to do is give our data frame as the first argument, and then our particular variable column as the second argument.\n\n# Without pipe operator\ntabyl(our_gss, partyid_recoded)\n\n\n# With pipe operator\nour_gss |&gt;\n  tabyl(partyid_recoded)\n\n partyid_recoded    n     percent valid_percent\n        Democrat 1046 0.295146727    0.29809062\n     Independent 1565 0.441591422    0.44599601\n      Republican  792 0.223476298    0.22570533\n     Other Party  106 0.029909707    0.03020804\n            &lt;NA&gt;   35 0.009875847            NA\n\n\nNow, we have a frequency table for partyid_recoded!\nEach row is a level of partyid_recoded. The n column refers to the sample size, so, for example, 1,565 respondents indicated that they were Independents.\nWe then have two different columns for percentages. The percent column is the proportion of all respondents that chose each response value—while including NA values. Often, we want to get a proportion for only the sample of those who actually answered the question. This is what the valid_percent column indicates and is what we are often looking for.\nLet’s break it down a little for clarity.\nThe total number of respondents is the sum of the ‘n’ column.\n1565 + 1046 + 792 + 106 + 35 = 3,544 Note that this is also the number of observations in our data frame\nThere are also 35 observations coded as NA, so the total number of valid respondents is 3,509.\nNow, I’ll give the calculations for the Independent row.\n\n# I'll define a few objects here for simplicity.\n\nall_respondents &lt;- 3544\n\nall_minus_na &lt;- (3544 - 35)\n\nindependents &lt;- 1565\n\n\n# Percent column\n\nindependents / all_respondents\n\n[1] 0.4415914\n\n# Valid percent column\n\nindependents / all_minus_na\n\n[1] 0.445996\n\n\nIn this case, there really aren’t that many NA values, so it does not change too much. In general, we will often discard NA categories in analyses we do for the course, but they give us important information about the survey and can be useful for some questions, so it’s important to keep track of them nonetheless.\nNow, let’s clean a couple things up with the adorn_() functions.\nLet’s add a row for totals, turn the proportions into percentages, and round the decimals. We can use the pipe operator to quickly leverage a bunch of different adorn() functions.\n\nour_gss |&gt;\n  tabyl(partyid_recoded) |&gt;\n  adorn_totals(where = \"row\") |&gt;\n  adorn_pct_formatting() |&gt;\n  adorn_rounding(digits = 2)\n\n partyid_recoded    n percent valid_percent\n        Democrat 1046   29.5%         29.8%\n     Independent 1565   44.2%         44.6%\n      Republican  792   22.3%         22.6%\n     Other Party  106    3.0%          3.0%\n            &lt;NA&gt;   35    1.0%             -\n           Total 3544  100.0%        100.0%\n\n\nWe don’t need to supply any input for adorn_pct_formatting()—which changes the proportions to percentages—but a couple of these functions do require inputs.\nFor adorn_totals(), we give “row” for the where input, which tells the function that we want the totals column to appear as a row.\nFor adorn_rounding(), we give an input for digits, which is the number of digits we want after the decimal point. We’ll keep 2 and thereby round to the hundredth.\nLastly, if we want, we can also make the column names a little nicer. It may not be obvious because we’ve just been displaying this table rather than storing it as an object, but it’s actually a data frame, so we can edit the column names directly.\nLet’s save it as an object so that’s easier to see.\n\nour_table &lt;- our_gss |&gt;\n  tabyl(partyid_recoded) |&gt;\n  adorn_totals(where = \"row\", na.rm = TRUE) |&gt;\n  adorn_pct_formatting() |&gt;\n  adorn_rounding(digits = 2)\n\nWe can then use the colnames() function. Let’s supply our table data frame as an input to get a feel for it.\n\ncolnames(our_table)\n\n[1] \"partyid_recoded\" \"n\"               \"percent\"         \"valid_percent\"  \n\n\nIt lists out the character vector of the 4 column names, and we can actually just write over them by supplying our desired column names as a character vector. If we assign that character vector to colnames(our_table), it will overwrite the column names. Make sure the labels are in the same order as they appear in the data frame.\n\ncolnames(our_table) &lt;- c(\n  \"Political Party\",\n  \"Frequency\",\n  \"Percent\",\n  \"Valid Percent\"\n  )\n\nour_table\n\n Political Party Frequency Percent Valid Percent\n        Democrat      1046   29.5%         29.8%\n     Independent      1565   44.2%         44.6%\n      Republican       792   22.3%         22.6%\n     Other Party       106    3.0%          3.0%\n            &lt;NA&gt;        35    1.0%             -\n           Total      3544  100.0%        100.0%\n\n\nExcellent! We’ll learn how to make this even nicer a bit later in the semester, but this looks pretty good for now.\nLet’s look at one last visualization technique for categorical variables.\n\n\n4.1.2 Bar plots\nFor categorical data, bar plots are often the way to go. These typically have the response values on the x axis, and the count or percentage of each response value is tracked on the Y axis.\nMuch like with tables, we’ll learn some fancier ways to do this later in the semester. But, in the meanwhile, base R has some great plotting functions that can be used very easily without much specification. They’re not the prettiest out of the box, but they will do everything we need.\nFor barplot(), we just need the variable column that we are trying to plot. But, there’s one thing to remember when you are trying to get a barplot this way—you have to provide the result of summary(my_variable) rather than the variable column directly.\nI’ll show you a couple ways to do that, which all do the exact same thing.\n\n# No pipe operator; summary() nested inside barplot()\n\nbarplot(summary(our_gss$partyid_recoded))\n\n\n# With pipe operator\nour_gss$partyid_recoded |&gt;\n  summary() |&gt;\n  barplot()\n\n\n# Make separate object for the summary() output\n\nmy_summary &lt;- summary(our_gss$partyid_recoded)\n\nbarplot(my_summary)\n\n\n\n\n\n\n\n\nNow, as I mentioned, we won’t spend too much time adjusting these plots for now, but there are a couple things we can do to make this a little better to look at for now.\nFor one, our response labels (Democrat, Republican, Independent, etc) Actually look pretty good in the display here, but it’s not uncommon for these to get cut off if we have several response categories or the response names are especially long. So, I’ll show you how to reduce the font size as one potential fix for that.\nWe can add the cex.names argument to barplot(). This argument takes a number, and the number should reflect an intended ratio of the default font size for our value labels on the x-axis. So, for example, I would enter ‘2’ if I wanted the font to be twice as big. For our purposes, I want to reduce the font a bit, so we’ll enter ‘.75’ to reduce the font size by 1/4.\n\nour_gss$partyid_recoded |&gt;\n  summary() |&gt;\n  barplot(cex.names = .75)\n\n\n\n\n\n\n\n\nLastly, we can also add some simple descriptive information for the plot, such as a title and labels for the x and y axis. We can do so within barplot()\n\nour_gss$partyid_recoded |&gt;\n  summary() |&gt;\n  barplot(\n    cex.names = .75, \n    main = \"Count of GSS Respondents by Political Party\",\n    xlab = \"Political Party\",\n    ylab = \"Count\"\n  )",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Categorical Variables</span>"
    ]
  },
  {
    "objectID": "univariate_categorical.html#measures-of-central-tendency",
    "href": "univariate_categorical.html#measures-of-central-tendency",
    "title": "4  Categorical Variables",
    "section": "4.2 Measures of Central Tendency",
    "text": "4.2 Measures of Central Tendency\nNow, let’s talk about measures of central tendency. Because there is no mathematically meaningful relationship between the different levels of a categorical variable, our options for reporting measures of central tendency are a little more limited.\nThe classic central tendency measures are the mode, the median, and the mean. For categorical variables, it is not ever meaningful to take the mean. As we saw back on our first day with R, you will get a warning message if you try to run mean() on a character or factor variable.\nOur options are limited to the mode or the median, depending on the distribution of responses.\n\n4.2.1 Nominal variables\nIn the case nominal variables, there is no semblance of meaningful arrangement to the categories. In this case, we can only report the mode.\nOften times, we can just glean this from the frequency tables, or even just the summary() output. Let’s stick with partyid_recoded as an example.\n\nsummary(our_gss$partyid_recoded)\n\n   Democrat Independent  Republican Other Party        NA's \n       1046        1565         792         106          35 \n\n\nRemember that the mode is just the value with the most occurrences. So, the mode here is ‘Independent’.\nThis is likely sufficient for the variables we will work with, but it’s possible for some variables to have so many factor levels that it can be a little annoying to try and spot the mode visually. So, I’ll show you how to calculate it manually as well.\nWe can use the summary() output again. For a factor variable, it gives us each of the levels of the factor along with the number of respondents in each level. So, we can use a nifty base R function called which.max() along with summary(). This will return the maximum value in our summary output, which will correspond to the most frequent level of the factor.\n\nparty_sum &lt;- summary(our_gss$partyid_recoded)\n\nwhich.max(party_sum)\n\nIndependent \n          2 \n\n\nSo, this also gives us our mode, and is a bit more precise a way of doing so—especially when we have a bunch of possible response values.\nNow, let’s talk about ordinal variables, where we have another consideration.\n\n\n4.2.2 Ordinal variables\nWhile ordinal variables are still categorical and thus unamenable to mathematical analysis, they do have a meaningful order. This means that it’s possible for us to take the median value of an ordinal variable.\nHowever, we can only do so when the variable is normally distributed. If there is significant skew in the distribution of responses, then we will need to report the mode for an ordinal variable.\nLet’s refresh our memory on distributions before we take some central tendency measures of ordinal variables in the GSS.\n\n4.2.2.0.1 Distribution assumptions\nIf the responses of an ordinal variable are normally distributed, we can take the median.\nIn the ideal form of the normal distribution, you have a mean of 0 and a standard deviation of 1, where roughly 68% of values are within 1 standard deviation in either direction of the mean, and roughly 95% of values are within 2 standard deviations in either direction.\nHowever, in practice, our distributions are unlikely to reflect the ideal normal distribution. But, if they are roughly normal, we can work with that.\nI’ll display a few distributions here as a reminder, starting with the classic normal distribution. If you produce a barplot and find the data looks generally like this—where the majority of variables are concentrated around the middle of the distribution—then you can report the median response rather than the mode.\nHere’s the normal distribution\n\n\n\n\n\n\n\n\n\nHere’s a left-skewed distribution. This is when there’s a prominent ‘tail’ on most of the left-hand side of the distribution. In other words, when most of the values are concentrated on the right-hand side of the distribution (the upper end), then we have a left-skew.\n\n\n\n\n\n\n\n\n\nLastly, here’s a right-skewed distribution. As you can probably guess from the description of a left-skewed distribution, a right-skew occurs when we have a long tail through most of the right-hand side of the distribution. In other words, most values are concentrated on the left-hand side.\n\n\n\n\n\n\n\n\n\nSo, if the distribution of your variable looks (mostly) like the normal distribution, go ahead and take the median of any ordinal variable.\nLet’s go ahead and practice doing that with the age_ord variable we created last time.\nIn the case of age_ord, we’d actually want to take the mode, because it’s a bit left-skewed. We can see that when we make the barplot\n\nour_gss$age_ord |&gt;\n  summary() |&gt;\n  barplot(\n    main = \"Age Distribution of GSS Respondents\",\n    xlab = \"Age grouping\"\n  )\n\n\n\n\n\n\n\n\nBut wait, let’s try the happy variable. This is an ordinal variable in our subset that we haven’t used yet. We’ll see if its normally distributed.\n\nour_gss$happy |&gt;\n  summary() |&gt;\n  barplot(\n    main = \"Happiness Distribution of 2022 GSS Respondents\",\n    xlab = \"Response value\"\n  )\n\n\n\n\n\n\n\n\nPerfect. We can ignore the NAs for this, and, otherwise, the distribution is exactly what we want to see. Most of the variables are concentrated at the midpoint of the distribution, with tails on either side. So, happy will serve as a good example of a case where taking the median of an ordinal variable is appropriate.\nFor this, we can use the quantile() function. It has a lot of nifty uses, but we will only really use it for this purpose—calculating the median of a normally distributed ordinal variable.\nIn technical terms, quantiles are markers that split up a distribution into equal parts.\nFor our purposes, you can think of quantiles in terms of percentiles, where, for example, being in the 90th percentile means you are performing at or above 90% of others in the sample. Percentiles are a specific type of quantile where the distribution is divided into 100 equal parts.\nWe can use the quantile() function to get the value at the 50th percentile of the distribution, which is another way of saying the median.\n\nquantile(\nour_gss$happy, \n.5, \ntype = 1,\nna.rm = TRUE\n)\n\n         50% \npretty happy \nLevels: not too happy &lt; pretty happy &lt; very happy\n\n\n\nFor the first input, we provide the ordinal variable column for which we want to calculate the median.\nThen, we give a proportion equivalent to the percentile that we want. Because want to take the value at the 50th percentile, we will enter 0.5.\nThe specifics of the ‘type’ argument are a little above our paygrade, but we need to specify this so that quantile() knows its working with an ordered factor.\nLastly, we’ll specify that we want NAs to be removed in the calculation, as R will throw an error unless we tell it to disregard these.\n\nSo, we can see that ‘pretty happy’ would be the median response.",
    "crumbs": [
      "Day 2: Survey Data and Univariate Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Categorical Variables</span>"
    ]
  },
  {
    "objectID": "viz_introduction.html",
    "href": "viz_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Visualization and Analytical Thinking\nUp to this point, we have worked with pretty simple visualizations, just so we can quickly glean important information about our statistical models without spending too much time focusing on the aesthetics of the visuals.\nIn this lab, we will learn a bit more about R’s graphical capability—especially through tidyverse’s ggplot—which provides us with incredible customizability. We will learn how to fine-tune some of the visuals we have already worked with, and we will preview some other common visual styles that can manage with ggplot.\nBefore we start working with some of these new visual tools, I want to take an opportunity to stress the importance of visualization more generally. It’s easy to see the process of presenting visuals as something somewhat superficial, but visualization can be critical for defining the kind of questions we can ask about our data.\nFor now, I’m going to obscure the code I’m using for this document. We will learn more about the kind of commands I used to generate the following figures, but I don’t want anyone to get bogged down initially. I’ll use these visuals to help impart an important lesson about data visualization’s in the research process.",
    "crumbs": [
      "Day 5: Making Better Visualizations",
      "Introduction"
    ]
  },
  {
    "objectID": "viz_introduction.html#thirteen-data-sets",
    "href": "viz_introduction.html#thirteen-data-sets",
    "title": "Introduction",
    "section": "Thirteen Data Sets",
    "text": "Thirteen Data Sets\nLet’s take a look at a collection of thirteen different data sets. Each data set has 142 observations with 2 columns, labeled x & y.\nI’ll use some tidyverse commands to get some summary statistics for each of the data sets, including the mean of both variables and their standard deviations. Let’s see what seems to distinguish some of these data sets from one another.\n\n\n# A tibble: 13 × 4\n   mean_x mean_y std_x std_y\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   54.3   47.8  16.8  26.9\n 2   54.3   47.8  16.8  26.9\n 3   54.3   47.8  16.8  26.9\n 4   54.3   47.8  16.8  26.9\n 5   54.3   47.8  16.8  26.9\n 6   54.3   47.8  16.8  26.9\n 7   54.3   47.8  16.8  26.9\n 8   54.3   47.8  16.8  26.9\n 9   54.3   47.8  16.8  26.9\n10   54.3   47.8  16.8  26.9\n11   54.3   47.8  16.8  26.9\n12   54.3   47.8  16.8  26.9\n13   54.3   47.8  16.8  26.9\n\n\nWell, there’s not much we can say here. All the summary statistics are identical. Why don’t we try modeling a linear relationship between the x and y variables. Maybe looking at the correlations will tell us something. I’ll display the linear regression lines for each data set below.\n\n\n\n\n\n\n\n\n\nOkay. This is not revealing much either. All the lines seem to have the same slope, which shows a (slight) negative relationship where y decreases as x increases. The correlations aren’t revealing any notable distinctions.\nBut wait. One thing we can see here is that, while the correlations appear to be about the same, there are some differences in the ranges of values. Note that the regression lines don’t extend across the same range of x-axis values in each data set. Maybe there is something here after all.\nLet’s just go ahead and plot the actual data over our regression lines.\n\n\n\n\n\n\n\n\n\nNow there’s some distinction!\nThis is a tongue in cheek data set known as the ‘datasaurus dozen’. It’s often used in intro statistical classes to help illustrate the importance of visualization. It’s inspired by another conceptually similar data set known as ‘Anscombe’s quartet’ which likewise stresses the role of plotting data in producing well informed analyses.",
    "crumbs": [
      "Day 5: Making Better Visualizations",
      "Introduction"
    ]
  },
  {
    "objectID": "viz_introduction.html#in-sum",
    "href": "viz_introduction.html#in-sum",
    "title": "Introduction",
    "section": "In Sum",
    "text": "In Sum\nSo, take this as a showcase of the importance of visualizing your data. This isn’t to discount summary statistics and other numeric description of data—those are still invaluable for us.\nRather, cases like Datasaurus or Anscombe’s quartet highlight the necessity of understanding the shape of your data. This will determine the kind of questions you can ask with the data, as well as the kind of statistical tools you need to describe it.\nFor example, in the case we just examined, those x and y variables do not have any kind of clear linear relationship. In that case, tools like regression that assume linearity are not appropriate. Any relationship between the variables could only be explored through other statistical means.\nSo, making our figures and tables look aesthetically pleasing is indeed valuable in its own right, but don’t underestimate the utility of good visualization for the analytic process itself.",
    "crumbs": [
      "Day 5: Making Better Visualizations",
      "Introduction"
    ]
  }
]