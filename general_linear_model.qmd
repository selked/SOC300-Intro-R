# General Linear Model
```{r, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
library(sysfonts)
sysfonts::font_add_google("Roboto Condensed")
sysfonts::font_add("Arial Narrow", regular = "ARIALN.ttf")
library(showtext)
library(hrbrthemes)
library(tidyverse)
library(scales)
library(wesanderson)
showtext.auto()
```

In the last section, we explored some common techniques for displaying and interpreting the bivariate distribution when all our variables are categorical.

That's a situation you will often find yourself as a data analyst, but the Chi-square test also represents a simple framework for learning and applying the general logic underlying the vast majority of statistical inferences---the comparison of our observed distribution to an estimated null distribution. 

However, we also very often need to work with data that comprise a much wider variety of measurement levels. While any interval-ratio variable can be re-coded into an ordinal or nominal variable, we often want our statistical models to preserve the finer granularity of information captured by interval-ratio variables. Additionally, multivariate analyses quickly become quite cumbersome beyond 3 variables. It will not be long before you are utterly drowning in contingency tables. 

For these situations---when we have a mixture of numeric & categorical variables or multiple control variables---we often call on the **General Linear Model** (GLM).

This is not something that we will work with much in our own projects for this course, only because it's a bit more involved than we have the time cover in the depth it deserves. We could easily spend the entire semester covering the GLM and its various permutations. While it offers us a great deal of statistical power, we have to account for a wider array of underlying assumptions than something like the Chi-square test.

As such, I will use this section to provide a largely conceptual overview of two more commonly encountered use-cases of the GLM: 

- modeling continuous dependent variables (ordinary least squares regression)
- modeling binary categorical variables with mixed predictor variables (logistic regression)

I will you show you some of the code used to create our regression models, but, as we will not focus as much on these methods in lab, I will not go into as much detail on these commands, and I'll obscure most of my code for the visualizations, so as not to bog you down in some unfamiliar R functions. However, you will get to work with some of the graphing tools I use here in our next unit.

## Linear Relationships
The GLM is the underlying statistical architecture behind the broader family of **regression modeling** techniques. 

If you haven't had much experience with statistics up to this point, that all may sound a little ethereal, but it really all follows from a simple algebraic principle you likely encountered in high-school math.

I'll show you the general equation that underlies the GLM, but first I'll set set up some context with a refresher on linearity.

$$Y = mX + b$$
This is the basic linear equation that we can use to describe any straight line. Mathematical notation is not always especially intuitive, so I'll clarify the symbols here.

- $Y$ = The value of our y-axis variable (our dependent variable)
- $m$ = The slope of the line, which we often learn as ${\frac{rise}{run}}$
- $X$ = The value of the x-axis variable (our independent variable)
- $b$ = The y-intercept, i.e. the value of $Y$ when $X = 0$. This tells us where the line begins in our coordinate plane

Let's take a very simple example to illustrate the point. I'm going to create a dataframe with an 'x' column and a 'y' column. Each of these variables will be made up of the same vector of values, which will be the sequence of 0:5.

```{r, echo = FALSE, eval = TRUE}
x <- 0:5
y <- 0:5

df <- data.frame(
  x,
  y
)

knitr::kable(df)
```
Now, we'll treat these as coordinates and plot them.
```{r, eval = TRUE, echo = FALSE}
df |>
  ggplot(
    aes(x, y)) +
  geom_point(color = "yellow") +
  geom_line(color = "green") +
  labs(
    title = "Simple line demonstration"
  ) +
  coord_cartesian(
    xlim = c(-5,5),
    ylim = c(-5,5)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_vline(xintercept = 0, color = "gray") +
  scale_x_continuous(breaks = seq(-5,5,1)) +
  scale_y_continuous(breaks = seq(-5,5,1)) +
  annotate(
      "segment",
      x = 2,
      xend = 3,
      y = 2,
  colour = "orange") +
  annotate(
    "segment",
    x = 3,
    y = 2,
    yend = 3,
    color = "orange") +
  annotate(
    "text",
    x = 2.5,
    y = 1.5,
    label = "run"
  ) +
  annotate(
    "text",
    x = 3.5,
    y = 2.5,
    label = "rise"
  ) +
  theme_ft_rc(
    base_size = 24,
      axis_text_size = 24,
      axis_title_size = 24,
      plot_title_size = 30,
      strip_text_size = 24)
```

So, here's the simple line segment that results from the linear combination of our x & y variables. 

The **y-intercept** here is 0. That's the value of $Y$ when $X = 0$

And the **slope coefficient** is 1. To calculate the slope, we focus on the distance between two points. The 'run' is the distance we travel along the x-axis to get from the first point to the second, and the 'rise' is the distance we travel along the y-axis. Here, we travel +1 unit on the x-axis, and +1 unit on the y-axis. Thus, ${\frac{1}{1} = 1}$, and $1$ is then our slope coefficient. 

Now we can plug these values into $Y = mX + b$. This allows us to describe the **conditional distribution** of these two variables. In other words, we can assess the value of $Y$ *given* our slope and any specific value of $X$. 

For example, if we know that $X = 4$, we can plug that into our equation

$$Y = (1)\cdot (4) + 0$$
And naturally we find that

$$Y = 4$$

This is trivial for our case, given that our example line has very few data points and an incredibly simple relationship (where $X = Y$). 

However, this general principle of describing a linear relationship between variables provides the basis of some very powerful tools in statistical inference.

## Ordinary Least Squares
While an example like the one above is helpful as an introduction to the logic of linear relationships between variables, we are very unlikely to ever work with data exhibiting relationships that straightforward. Particularly so in our case, as the social world is messy and multiplex.

Let's take a look at one such combination of variables.

```{r, eval = TRUE, echo = FALSE}
load("C:/Users/selke/Desktop/Scholarship/USAJobs Project/ja_reprocessed.rda")
library(gtsummary)

names(workingjobs.df)[61] <- "Proportion of women"
names(workingjobs.df)[141] <- "Warmth/Competence Scale"

stats <- c("N" = "{length}", 
           "Mean (SD)" = "{mean} ({sd})",
           "Min" = "{min}",
           "Max" = "{max}")

tbl <- 
  purrr::imap(
  stats,
  ~workingjobs.df |>
    tbl_summary(include = c("Warmth/Competence Scale", "Proportion of women"), 
    missing = "no", 
    statistic = ~.x,
    digits = ~ list(
      length = label_style_number(digits = 0)
    ))  |>
    modify_header(all_stat_cols() ~ stringr::str_glue("**{.y}**"))) |>
  tbl_merge(tab_spanner = FALSE) |>
  modify_footnote(~NA) |>
  modify_header(label ~"**Variable**")

tbl

```
These two variables are taken from a dataframe containing information about 5,526 job advertisements scraped from [USAJobs.gov](https://usajobs.gov), the advertising site for federal US jobs. These represent all the job ads available during the summer of 2021. 

The `Proportion of women` variable is fairly straightforward. For each job advertisement, this is the corresponding proportion of women employees, as assessed through nationally representative occupational data.

The 'Warmth/Competence' scale is a measure derived from word embeddings. Without getting too in the weeds, know that this is a measure of how prominently the language of a job ad engages the concepts of 'warmth' vs. 'competence'. 

In social psychology, the Stereotype Content Model has theorized that many of our stereotypical perceptions about people can be characterized along these two dimensions. *Competence* reflects agency, adeptness, leadership, and related qualities. *Warmth* can be thought of in terms of sociability, and is related to interpersonal people-skills, empathy, and deference. 

As this literature has shown, higher status groups tend to be perceived as higher in competence but lower in warmth, whereas lower status groups are often perceived as less competent but more warm. In simpler terms, higher status groups are often thought of us as smart but impersonal, while lower status groups are perceived as less capable but more amiable and sympathetic. 

Negative values on this scale indicate more engagement with warmth as opposed to competence, whereas positive values indicate more engagement with competence as opposed to warmth. Values at or near zero indicate that neither concept is more salient than the other.

Let's consider the association between the gender composition of the US federal workforce and the amount of warmth/competence language in job advertisements. As the US is a patriarchal society, masculinity is associated with higher status than femininity. We'll predict that the language used to describe occupations with a greater proportion of women will contain a greater proportion of 'warm' language.

$$H_0= There \ will \ be \ no \ relationship \ between \ the \ stereotype \ content \ of \ a \ job \ advertisement \ and \ the \ gender \ composition \ of \ the \ occupation$$ 
$$H_1 = Jobs \ with \ a \ greater \ proportion \ of \ women \ will \ produce \ job \ ads \ with \ more \ engagement \ of \ warmth \ language$$
Let's take a look at these two variables plotted against one another. We're conceiving of the proportion of women as our independent variable and the warmth/competence scale as the dependent variable. So, the former will be on the x-axis and the latter will be on the y-axis. 

```{r, eval = TRUE, echo = FALSE}
workingjobs.df |>
  ggplot(
    aes(x = `Proportion of women`, y = `Warmth/Competence Scale`)) +
      geom_point() +
  labs(
    title = "Warmth/Competence Language by Proportion of Women",
    x = "Proportion of women",
    y = "Warmth/Competence Score"
  ) +
  theme_ft_rc(base_size = 24,
      axis_text_size = 24,
      axis_title_size = 24,
      plot_title_size = 30,
      strip_text_size = 24)

```
Now, this is clearly a linear relationship that's quite a bit more complex than our simplified example earlier. Let's go ahead and draw a line that best characterizes the conjoint distribution of these variables.

```{r, eval = TRUE, echo = FALSE}
workingjobs.df |>
  ggplot(
    aes(x = `Proportion of women`, y = `Warmth/Competence Scale`)) +
      geom_point() +
  geom_smooth(method = "lm", color = "yellow") +
  labs(
    title = "Warmth/Competence Language by Proportion of Women",
    x = "Proportion of women",
    y = "Warmth/Competence Score"
  ) +
  theme_ft_rc(base_size = 24,
      axis_text_size = 24,
      axis_title_size = 24,
      plot_title_size = 30,
      strip_text_size = 24)
```
