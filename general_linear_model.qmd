# General Linear Model
```{r, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
library(sysfonts)
sysfonts::font_add_google("Roboto Condensed")
sysfonts::font_add("Arial Narrow", regular = "ARIALN.ttf")
library(showtext)
library(hrbrthemes)
library(tidyverse)
library(scales)
library(wesanderson)
showtext.auto()
```

In the last section, we explored some common techniques for displaying and interpreting the bivariate distribution when all our variables are categorical.

That's a situation you will often find yourself as a data analyst, but the Chi-square test also represents a simple framework for learning and applying the general logic underlying the vast majority of statistical inferences---the comparison of our observed distribution to an estimated null distribution. 

However, we also very often need to work with data that comprise a much wider variety of measurement levels. While any interval-ratio variable can be re-coded into an ordinal or nominal variable, we often want our statistical models to preserve the finer granularity of information captured by interval-ratio variables. Additionally, multivariate analyses quickly become quite cumbersome beyond 3 variables. It will not be long before you are utterly drowning in contingency tables. 

For these situations---when we have a mixture of numeric & categorical variables or multiple control variables---we often call on the **General Linear Model** (GLM).

This is not something that we will work with much in our own projects for this course, only because it's a bit more involved than we have the time cover in the depth it deserves. We could easily spend the entire semester covering the GLM and its various permutations. While it offers us a great deal of statistical power, we have to account for a wider array of underlying assumptions than something like the Chi-square test.

As such, I will use this section to provide a largely conceptual overview of two more commonly encountered use-cases of the GLM: 

- modeling continuous dependent variables (ordinary least squares regression)
- modeling binary categorical variables with mixed predictor variables (logistic regression)

I will you show you some of the code used to create our regression models, but, as we will not focus as much on these methods in lab, I will not go into as much detail on these commands, and I'll obscure most of my code for the visualizations, so as not to bog you down in some unfamiliar R functions. However, you will get to work with some of the graphing tools I use here in our next unit.

## Linear Relationships
The GLM is the underlying statistical architecture behind the broader family of **regression modeling** techniques. 

If you haven't had much experience with statistics up to this point, that all may sound a little ethereal, but it really all follows from a simple algebraic principle you likely encountered in high-school math.

I'll show you the general equation that underlies the GLM, but first I'll set set up some context with a refresher on linearity.

$$Y = mX + b$$
This is the basic linear equation that we can use to describe any straight line. Mathematical notation is not always especially intuitive, so I'll clarify the symbols here.

- $Y$ = The value of our y-axis variable (our dependent variable)
- $m$ = The slope of the line, which we often learn as ${\frac{rise}{run}}$
- $X$ = The value of the x-axis variable (our independent variable)
- $b$ = The y-intercept, i.e. the value of $Y$ when $X = 0$. This tells us where the line begins in our coordinate plane

Let's take a very simple example to illustrate the point. I'm going to create a dataframe with an 'x' column and a 'y' column. Each of these variables will be made up of the same vector of values, which will be the sequence of 0:5.

```{r, echo = FALSE, eval = TRUE}
x <- 0:5
y <- 0:5

df <- data.frame(
  x,
  y
)

knitr::kable(df)
```
Now, we'll treat these as coordinates and plot them.
```{r, eval = TRUE, echo = FALSE}
df |>
  ggplot(
    aes(x, y)) +
  geom_point(color = "yellow") +
  geom_line(color = "green") +
  labs(
    title = "Simple line demonstration"
  ) +
  coord_cartesian(
    xlim = c(-5,5),
    ylim = c(-5,5)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_vline(xintercept = 0, color = "gray") +
  scale_x_continuous(breaks = seq(-5,5,1)) +
  scale_y_continuous(breaks = seq(-5,5,1)) +
  annotate(
      "segment",
      x = 2,
      xend = 3,
      y = 2,
  colour = "orange") +
  annotate(
    "segment",
    x = 3,
    y = 2,
    yend = 3,
    color = "orange") +
  annotate(
    "text",
    x = 2.5,
    y = 1.5,
    label = "run"
  ) +
  annotate(
    "text",
    x = 3.5,
    y = 2.5,
    label = "rise"
  ) +
  theme_ft_rc(
    base_size = 24,
      axis_text_size = 24,
      axis_title_size = 24,
      plot_title_size = 30,
      strip_text_size = 24)
```

So, here's the simple line segment that results from the linear combination of our x & y variables. 

The **y-intercept** here is 0. That's the value of $Y$ when $X = 0$

And the **slope coefficient** is 1. To calculate the slope, we focus on the distance between two points. The 'run' is the distance we travel along the x-axis to get from the first point to the second, and the 'rise' is the distance we travel along the y-axis. Here, we travel +1 unit on the x-axis, and +1 unit on the y-axis. Thus, ${\frac{1}{1} = 1}$, and $1$ is then our slope coefficient. 

Now we can plug these values into $Y = mX + b$. This allows us to describe the **conditional distribution** of these two variables. In other words, we can assess the value of $Y$ *given* our slope and any specific value of $X$. 

For example, if we know that $X = 4$, we can plug that into our equation

$$Y = (1)\cdot (4) + 0$$
And naturally we find that

$$Y = 4$$

This is trivial for our case, given that our example line has very few data points and an incredibly simple relationship (where $X = Y$). 

However, this general principle of describing a linear relationship between variables provides the basis of some very powerful tools in statistical inference.

## Ordinary Least Squares
While an example like the one above is helpful as an introduction to the logic of linear relationships between variables, we are very unlikely to ever work with data exhibiting relationships that straightforward. Particularly so in our case, as the social world is messy and multiplex.

Let's take a look at one such combination of variables.

```{r, eval = TRUE, echo = FALSE}
load("C:/Users/selke/Desktop/Scholarship/USAJobs Project/ja_reprocessed.rda")
library(gtsummary)

names(workingjobs.df)[61] <- "Proportion of women"
names(workingjobs.df)[141] <- "Warmth/Competence Scale"

stats <- c("N" = "{length}", 
           "Mean (SD)" = "{mean} ({sd})",
           "Min" = "{min}",
           "Max" = "{max}")

tbl <- 
  purrr::imap(
  stats,
  ~workingjobs.df |>
    tbl_summary(include = c("Warmth/Competence Scale", "Proportion of women"), 
    missing = "no", 
    statistic = ~.x,
    digits = ~ list(
      length = label_style_number(digits = 0)
    ))  |>
    modify_header(all_stat_cols() ~ stringr::str_glue("**{.y}**"))) |>
  tbl_merge(tab_spanner = FALSE) |>
  modify_footnote(~NA) |>
  modify_header(label ~"**Variable**")

tbl

```
These two variables are taken from a dataframe containing information about 5,526 job advertisements scraped from [USAJobs.gov](https://usajobs.gov), the advertising site for federal US jobs. These represent all the job ads available during the summer of 2021. 

The `Proportion of women` variable is fairly straightforward. For each job advertisement, this is the corresponding proportion of women employees, as assessed through nationally representative occupational data.

The 'Warmth/Competence' scale is a measure derived from word embeddings. Without getting too in the weeds, know that this is a measure of how prominently the language of a job ad engages the concepts of 'warmth' vs. 'competence'. 

In social psychology, the Stereotype Content Model has theorized that many of our stereotypical perceptions about people can be characterized along these two dimensions. *Competence* reflects agency, adeptness, leadership, and related qualities. *Warmth* can be thought of in terms of sociability, and is related to interpersonal people-skills, empathy, and deference. 

As this literature has shown, higher status groups tend to be perceived as higher in competence but lower in warmth, whereas lower status groups are often perceived as less competent but more warm. In simpler terms, higher status groups are often thought of us as smart but impersonal, while lower status groups are perceived as less capable but more amiable and sympathetic. 

Negative values on this scale indicate more engagement with warmth as opposed to competence, whereas positive values indicate more engagement with competence as opposed to warmth. Values at or near zero indicate that neither concept is more salient than the other.

Let's consider the association between the gender composition of the US federal workforce and the amount of warmth/competence language in job advertisements. As the US is a patriarchal society, masculinity is associated with higher status than femininity. We'll predict that the language used to describe occupations with a greater proportion of women will contain a greater proportion of 'warm' language.

$$H_0= There \ will \ be \ no \ relationship \ between \ the \ stereotype \ content \ of \ a \ job \ advertisement \ and \ the \ gender \ composition \ of \ the \ occupation$$ 
$$H_1 = Jobs \ with \ a \ greater \ proportion \ of \ women \ will \ produce \ job \ ads \ with \ more \ engagement \ of \ warmth \ language$$
Let's take a look at these two variables plotted against one another. We're conceiving of the proportion of women as our independent variable and the warmth/competence scale as the dependent variable. So, the former will be on the x-axis and the latter will be on the y-axis. 

```{r, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
workingjobs.df |>
  ggplot(
    aes(x = `Proportion of women`, y = `Warmth/Competence Scale`)) +
      geom_point(size = .5) +
  labs(
    title = "Warmth/Competence Language by Proportion of Women",
    x = "Percentage of women",
    y = "Warmth/Competence Score"
  ) +
  scale_x_continuous(
    labels = percent_format()
  ) +
  theme_ft_rc(base_size = 24,
      axis_text_size = 24,
      axis_title_size = 24,
      plot_title_size = 30,
      strip_text_size = 24)

```
Now, this is clearly a linear relationship that's quite a bit more complex than our simplified example earlier. Let's go ahead and draw a line that best characterizes the conjoint distribution of these variables.

```{r, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
library(scales)


workingjobs.df |>
  ggplot(
    aes(x = `Proportion of women`, y = `Warmth/Competence Scale`)) +
      geom_point(size = .5) +
  geom_smooth(method = "lm", color = "yellow", linewidth = .75) +
  labs(
    title = "Warmth/Competence Language by Percentage of Women",
    x = "Percentage of women",
    y = "Warmth/Competence Score"
  ) +
  scale_x_continuous(
    labels = percent_format()
  ) +
  theme_ft_rc(base_size = 24,
      axis_text_size = 24,
      axis_title_size = 24,
      plot_title_size = 30,
      strip_text_size = 24)
```
What I've plotted here is the **OLS regression line** for these two variables. This line solves for the following equation, which is the basis of many models in the GLM framework.

$${\hat{Y} = {\hat{\beta}{_0}} + {\hat\beta}{_1}X_1 + \epsilon}$$
Now, this may look a bit unfamiliar, but it actually contains much of the same information as the linear equation we reviewed above. We just have some new notation conventions to reckon with.

For starters, we now have $\hat{}$ symbol appearing over several variables. This is called, funnily enough, the 'hat' symbol. This little 'hat' just tells us that the value in question is one we are estimating by solving for the equation rather than a value that appears directly in our data.

But roughly the first $\frac{3}{4}$ of the equation is just the linear equation we saw above.

- $\hat{Y}$ is an estimated value of $Y$
- $\hat{\beta{_0}}$ is an estimated y-intercept
- $\hat{\beta{_1}}$ is an estimated slope coefficient for $X_1$, which is any given value of our independent variable

So this part of the equation:

$$\hat{Y} = {\hat{\beta}{_0}} + {\hat\beta}{_1}X_1$$
Is basically just:

$$Y = mX + b$$
The $\beta$ symbol stands for 'beta coefficient', which you can just think of as a term-of-art for the values that we estimate through linear regression.

Now we can start to think about how these values are estimated, though I'm going to stay conceptual here. In ordinary least squares regression, the calculation takes our observed x and y values as input and then estimates the slope and intercept that produce a line reflecting the minimum possible distance between the line and all data points. 

The math behind this can really get into the weeds, but I'll give a sense of how this line is derived. The least squares method starts by considering a completely horizontal line at the y-intercept of the data. This is likely to be a terrible fit for any data, but regression starts by estimating the collective distance of all points from this line. I won't get too into this value, but know that this distance metric is called **the sum of squared residuals** (SSR). The SSR is taken for this horizontal line at the y-intercept, and then the line is incrementally rotated, with the SSR being calculated many different times for many different rotations of the line. After this iterative cycle, we take the line with the lowest sum of squared residuals as the line best characterizing the linear relationship between $X$ and $Y$. 

I'll note there's a really excellent [StatQuest from Josh Starmer](https://www.youtube.com/watch?v=PaFPbb66DxQ) that gives a great primer on some of these concepts along with highly intuitive visuals.

Now, this raises the question of the last part of the simple linear regression equation: the $\epsilon$ symbol. This is the symbol for what's commonly known as the **error term**. 

To understand this term, we need to think big picture about the relationship we're modeling. We are assuming that the gender composition of the federal workforce influences the representation of these jobs in public-facing advertisements. Now, even if we are right that this influence exists, it's exceedingly unlikely that the proportion of men/women in a given job is the *only* thing that influences the way that job is represented in an advertisement. There are all manner of different possible influences---from the type of position it is, the salary, the racial composition of the workforce, the amount of training required for the position, and so on and so forth. 

But the reality is that, even if we do include indicators for all of these different concepts, social and mental phenomena are incredibly complex. We may still wind up missing indicators that would influence our dependent variable. In addition, it may also be the case that the variables we do have are insufficient in some way or another.

So, we include the error term as a placeholder for all the variation in our dependent variable that is still unexplained after accounting for all of our independent and control variables. This helps give us better information about our model by characterizing the amount of variation in the dependent variable that is explained by the variables we anticipated as influential. Moreover, if we do not keep this error term in here, we will be guaranteed to erroneously estimate the association between $X$ and $Y$.

In R, we can estimate this quite easily using the base R package `lm`. The syntax works as follows:
```{r, eval = TRUE, echo = FALSE}
names(workingjobs.df)[61] <- "femaleu"

workingjobs.df <- workingjobs.df |>
  mutate(
    female_perc = femaleu * 100
  )
```
```{r, eval = FALSE, echo = TRUE}
our_model <- lm(`Warmth/Competence Scale` ~ `Percentage of women`, data = workingjobs.df)
```

```{r, eval = TRUE, echo = FALSE}
our_model <- lm(`Warmth/Competence Scale` ~ female_perc, data = workingjobs.df)
```
```{r, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
library(flextable)
library(huxtable)



c_tab<-huxreg(
  our_model,
  statistics = c("Number of observations" = "nobs", "R-squared" = "r.squared"),
  coefs = c("(Intercept)" = "(Intercept)", "Percentage of women" = "female_perc")) |>
  set_caption("OLS Regression Output")

c_tab <- as_flextable(c_tab)

c_tab <- theme_vader(c_tab)

c_tab

```
Here we can see two of the key estimated values from our linear regression equation. The y-intercept is likely clear enough: 0.846. This is the estimated value of $Y$ when $X = 0$. In other words, this is the Warmth/Competence score predicted by our model for a job with no women at all (i.e. a proportion of 0). Note that this value is positive, indicating that an occupation with no women is estimated to have job-ad language that engages more prominently with competence language.

The value (-0.017) listed for 'Percentage of women' is the estimated slope coefficient of this variable. The interpretation here is that, for every one-unit increase in the percentage of women, the Warmth/Competence score is estimated to *decrease* by 0.017. A one-percentage point increase is fairly small in substantive terms, so we could always multiply this coefficient by 10 to get a broader slope. For example, if the Warmth/Competence scores is estimated to decrease by 0.017 for every additional percentage point, then we would expect the score to decrease by 0.17 for every 10-point increase in percentage.  

So, in the context of:

$${\hat{Y} = {\hat{\beta}{_0}} + {\hat\beta}{_1}X_1 + \epsilon}$$
We now have $\hat{\beta_0} = 0.846$ and ${\hat\beta}{_1}(PercentageWomen) = -0.017$


Now, let's touch on statistical significance. We can also see that the coefficients for both our y-intercept and the slope of 'Percentage of women' are noted as statistically significant using the star notation we have seen elsewhere. These p-values are calculated using what's known as a **t-test**. This is another case where I won't get much into the math, but know that this test also utilizes the logic of hypothesis-testing that we have seen in other cases. 

In the case of associations between a predictor variable and a dependent variable in linear regression, a slope coefficient of 0 would indicate that there is no linear relationship between the variables. The t-test allows us to estimate the likelihood that the slope coefficient we observe is truly different from 0. In our case, this is so.

Let's briefly discuss one of the other values given in our regression output: the R-squared value. 

```{r, eval = TRUE, echo = FALSE}
c_tab
```

I won't get too much into the details on the actual calculation here, but R-squared is one of the valuable bits of information we can gather by accounting for the error term (and thereby modeling the variation influenced by factors not captured in our predictor variables). R-squared reflects the proportion of variance in the dependent variable that is directly explained by the independent variable. So, an R-squared of $0.151$ indicates that roughly 15% of the variation we observe in the Warmth/Competence scores of job ads is explained by the percentage of women in those jobs. 

This is fairly low for a couple of reasons. For one, as I've mentioned throughout, the social world is multiplex, often outside of our direct control, and difficult to model precisely. If you find yourself with an R-squared of even 0.4 - 0.5, that's generally considered fairly high. On the other hand, we would also expect this to be low in our case, as we have an incredibly simple model. We could include any number of different control variables that we would also expect to influence our dependent variable and this would likely boost our R-squared. We can also use this metric to help us choose between different model designs. 

### In Sum
Let's return now to the hypothesis that we started with. 

$$H_0= There \ will \ be \ no \ relationship \ between \ the \ stereotype \ content \ of \ a \ job \ advertisement \ and \ the \ gender \ composition \ of \ the \ occupation$$ 
$$H_1 = Jobs \ with \ a \ greater \ proportion \ of \ women \ will \ produce \ job \ ads \ with \ more \ engagement \ of \ warmth \ language$$
```{r, eval = TRUE, echo = FALSE}
c_tab
```
Given the results of our regression model, we can reject the null hypothesis, as we find evidence supporting our research hypothesis. The negative, statistically significant slope coefficient for 'Percentage of women' aligns with our hypothesis that there will be a positive relationship between the amount of 'warmth' language in a job advertisement and the percentage of women in the occupation being advertised. For each one-percentage-point increase in women at a given occupation, our model estimates a corresponding decrease of 0.017 in the Warmth/Competence score of the job ad. The more a job becomes populated by women, the more predominantly warmth language is engaged by the corresponding job advertisement. 

The slope coefficient that we calculated here is the one that we first saw visualized at the beginning of this section
```{r, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
workingjobs.df |>
  ggplot(
    aes(x = femaleu, y = `Warmth/Competence Scale`)) +
      geom_point(size = .5) +
  geom_smooth(method = "lm", color = "yellow", linewidth = .75) +
  labs(
    title = "Warmth/Competence Language by Percentage of Women",
    x = "Percentage of women",
    y = "Warmth/Competence Score"
  ) +
  scale_x_continuous(
    labels = percent_format()
  ) +
  theme_ft_rc(base_size = 24,
      axis_text_size = 24,
      axis_title_size = 24,
      plot_title_size = 30,
      strip_text_size = 24)
```
What we have just worked through is the simplest case of OLS regression, where there is only one dependent variable and one predictor variable. These principles scale in the case of multiple regression, where we can observe the isolated effects of individual variables when all other predictors are held constant. We won't dig too much into this for sake of time, but you can expect to learn much about multiple regression and all other iterations of the GLM if you continue on in social-science research. There are special cases for modeling count data, modeling proportions, modeling non-linearity or curvilinearity, and so on and so on. While certain details in the model specification or interpretation can vary, the underlying logic will reflect many of the fundamentals we have explored in this unit.
